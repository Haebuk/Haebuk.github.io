[{"content":"What is Airflow? Core Components Web server: UI 담당 웹 서버 Scheduler: 워크플로우 스케줄링 Metastore: 메타데이터가 저장되는 데이터 베이스 Executor: 작업이 어디서 실행될지 정의 Worker: 작업이 실행되는 프로세스 DAG dag_id: 유니크한 dag 이름 start_date: dag가 처음 스케줄되는 시간. datetime 모듈로 정의 schedule_interval: 스케줄되는 간격. cron으로 정의하거나 \u0026quot;@daily\u0026quot;와 같이 정의 default_args owner: str, \u0026quot;airflow\u0026quot; email_on_failure: bool email_on_retry: bool email: str retries: int retry_delay: datetime.timedelta catchup: airflow가 트리거하지 않았던 날짜에 대해 실행 여부, False로 하는 것이 좋음 DAGrun 스케줄러가 DAGrun object 생성 주어진 시간의 dag 정보를 담고 있는 인스턴스 실행될 task들을 가지고 있음 원자성, 멱등성 Operator Operator = Task 실행하고자 하는 작업을 캡슐화했다고 생각하면 된다. task_id: 하나의 dag내에서 유니크한 이름을 가져야 한다. 종류 Action Operator: 실행 (bash) Transfer Operator: 전송 (mysql, postgres) Sensor Operator: 감지 How Airflow works? One Node Architecture Web server가 metastore에서 메타데이터 정보를 가져온다. Schduler가 Metastore와 Executor에서 DAG, 작업을 트리거한다. Executor가 작업 업데이트를 metastore에 완료되었다고 업데이트한다. Executor에는 큐가 있는데, 실행이 정해진 순서대로 되게 한다. Multi Nodes Architecture (Celery) Node1에는 웹서버, 스케줄러, 익스큐터가 있음 Node2에는 메타스토어와 큐가 있음. 큐는 rabbit MQ나 redis 같은 서비스를 사용함 워커 노드들에는 Airflow 워커들이 있음 실행 방식은 1 노드 구조와 유사하나, 익스큐터는 외부 큐에 작업을 푸시한다. 큐 내부에 있는 작업은 워커에 의해 풀 된다. folder dags dags 폴더에 파일이 저장 웹 서버와 스케쥴러가 이를 파싱함 스케쥴러가 메타스토어에 dagrun object를 생성 dag가 실행이 되어야 할 경우 스케줄러가 TaskInstance object를 메타스토어에 스케줄함 TaskInstance를 익스큐터에 보냄 실행 중에 메타스토어의 정보를 실행중으로 업데이트 완료되면 메타스토어의 정보를 완료로 업데이트 dagrun이 종료되었는지 검증 웹서버가 metastore의 정보를 ui에 업데이트 caution dag에 파일이 업데이트 되면 dag_dir_list_interval 주기 후 (기본 5분) UI상에서 확인 가능하다. min_file_process_interval (기본 30초) dag를 파싱하는 시간. dag 코드가 업데이트 되도 해당 시간 후 반영된다. Commands useful commands airflow run: 하나의 task 인스턴스 실행 airflow list_dags: dag 목록 airflow dag_state: dag 상태 airflow task_state: task 상태 airflow test: 테스트 test $ airflow tasks test DAG_ID TASK_ID DATE dependencies task 디펜던시 줄 때 줄바꾸기 1 2 task1 \u0026gt;\u0026gt; task2 \u0026gt;\u0026gt; task3 task3 \u0026gt;\u0026gt; task4 \u0026gt;\u0026gt; task5 DAG Details date options start_date DAG의 task들이 언제부터 트리거되고 스케줄되는지 시간을 정의. ex) 2019-03-01로 정의했다면, 2019년 3월 1일 자정에 스케줄됨 python의 datetime 모듈로 정의 가능 과거나 미래로 설정 가능함 미래로 설정 시, 해당 시간이 될 때까지 기다림 과거로 설정 시, 기다리지 않고 실행 가능. 그러나 catchup=False로 주지 않으면 과거로 설정한 날짜로 부터 schedule_interval마다 task instance가 실행되므로 주의 datetime.now()와 같이 동적으로 할당하지 말 것 schedule_interval start_date의 최소값으로 부터 트리거되는 시간 간격 같은 dag에서도 task 별로 start_date를 따로 줄 수 있기 때문에 최소값으로 정의됨 그러나 같은 dag내 task들은 같은 start_date를 쓰는 것이 좋음 cron expression이나 datetime.timedelta 모듈로 정의 가능 cron을 쓰는 것이 더 정확한 표현을 할 수 있으므로 cron을 사용하는 것이 좋음 execution_date dag가 실행된 시각이 아님 start_date - schedule_interval end_date dag/task가 더이상 스케줄되지 않는 시간 기본값은 None Backfill 실행되지 않았던 dag/task를 실행하는 기능 catchup=True로 설정 시 수행 catchup=False로 설정 시 실행되지 않은 가장 마지막 dag/task만 실행하도록 되어 있음 CLI를 통해 실행 가능. airflow 공식문서 depends_on_past task 레벨에서 정의 default_args에 정의 해서 모든 task에도 적용 가능 이전 dagrun의 특정 task가 실패했다면, 이번 dagrun에서 그 task가 실행되는 것을 막을 수 있음 wait_for_downstream task 레벨에서 정의 default_args에 정의 해서 모든 task에도 적용 가능 wait_for_downtstream이 정의된 task의 downstream task들이 이전 dagrun에서 완료될 때 까지 이번 dagrun 대기 DAGs folder structure 1. Zip dag 파일은 zip 파일 root에 위치해야함 모듈 디펜던시가 필요하면 virtualenv와 pip 사용 2. DAGBag DAG 모음. 폴더 구조로 dag를 다룬다. dev/staging/prod와 같이 환경 분리에 이점이 있다 dagbag이 깨지면 airflow UI상에서 에러가 뜨지 않고 웹서버 로그로만 확인 가능하므로 주의 3. .airflowignore .gitignore와 유사 모든 dags 폴더에 넣는 것이 좋음 Failure Detection DAGs dagrun_timeout: dagrun이 타임아웃되는 시간. 스케줄된 dag만 해당하며(수동 실행은 해당되지 않음) 실행중인 dag의 수가 max_active_runs와 일치하는 경우에만 해당 sla_miss_callback on_failure_callback on_success_callback Tasks email email_on_failure email_on_retry retries retry_delay retry_exponential_backoff max_retry_delay execution_timeout on_failure_callback on_success_callback on_retry_callback Test DAG validation tests 유효한지 cycle이 없는지 default arguments가 잘 설정됐는지 DAG/Pipeline Definition Tests task의 숫자가 맞는지 (로직이 아닌) task가 잘 정의 됐는지 task의 upstream, downstream 디펜던시가 잘 정의됐는지 Unit tests 로직 체크 operator가 잘 동작되는지만 체크 복잡한 로직을 airflow가 하게 두지 마라 (airflow는 오케스트레이션 툴이다) Integration tests task가 데이터를 잘 교환하는지 task의 input 체크 여러 task간 의존성 체크 End to End Pipeline tests 결과가 올바른지 전체 로직 체크 성능 체크 Local Executor 병렬 실행 가능 (개발시 local executor 사용 권장) parallelism = 0: unlimit parallelism \u0026gt; 0: limit 코어수 - 1로 설정하는 것을 권장 dag_cuncurrency와 max_active_runs_per_dag 옵션에 따라 dag간 task 실행 순서를 조정할 수 있다. dag_cuncurrency: dag내에서 동시에 실행 가능한 task의 수 max_active_runs_per_dag: 동시에 실행 시킬 수 있는 dag 수(backfill 일 때 주로 신경쓸 듯) SubDAGs 유사한 DAG를 하나의 그룹으로 묶어 UI상에서 마치 하나의 DAG인 것 처럼 표시할 수 있음 SubDagOperator 사용 기본 Executor는 SequentialExecutor main DAG가 모든 subDAG 들을 task로 관리 Airflow UI는 오직 main DAG만 표시 subDAG는 부모 DAG와 동일한 시각에 스케줄되어야 함. 그렇지 않으면 예상치 못한 결과를 낳을 수 있음 데드락이 발생할 수 있음 Branching DAG가 특정 task의 결과에 따라 경로를 선택할 수 있게끔 하는 것 BranchPythonOperator BranchPythonOperator의 결과가 task_c를 반환하고 BranchPythonOperator의 downstream으로 task_a, task_b, task_c가 있다면 task_c를 실행하고, a와 b는 스킵 depends_on_past=True로 지정시, a와 b는 실패 상태로 뜨기 때문에 다음 DAGrun은 실행되지 않음 BranchPythonOperator에 empty path를 주면 의도하지 않은 결과를 줄 수 있기 때문에 반드시 주는 것이 좋다. path를 스킵하고 싶다면 (task를 끝내고 싶다면) dummy task를 줘서 끝내자 마지막 task도 skipped로 뜨는 것을 막고 싶다면 마지막 task operator에 trigger_rule='one_success'추가 Trigger Rule depends_on_past와 사용 가능 upstream task중 skipped task가 있으면 all_success와 all_failed는 skipped 상태로 표시됨 Kinds all_success: upstream task가 모두 성공하면 run all_failed: upstream task가 모두 실패하면 run all_done: upstream task가 성공 유무와 관련없이 끝나면 run one_failed: upstream task가 하나라도 실패해야 run one_success: upstream task가 하나라도 성공해야 run none_failed: upstream task가 failed가 없어야 run none_skipped: upstream task가 skipped가 없어야 run dummy Variables metadata DB에 저장되는 값 Key, value, Is encrypted로 구성 JSON 형식 가능 Templating placeholder {{}} 를 사용하여 값을 대체 Jinja template Macros https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#macros XCOMs task 간 메세지 공유 key, value, timestamp로 구성 value는 가벼워야 한다. (성능 문제) xcom_push(key, value)로 metadata DB에 푸시 key: returned_value, value: 리턴값 xcom_pull(key)로 받기 key를 명시하지 않으면 returned_value가 기본값 TriggerDagRunOperator 다른 DAG(컨트롤러)의 조건에 따라 특정 DAG(타겟)를 시작하게 함 branch나 subdag로는 너무 복잡해질때 사용 컨트롤러는 타겟이 종료될 때 까지 기다리지 않음 컨트롤러와 타겟은 독립적임 컨트롤러의 히스토리에 관한 시각화는 제공되지 않음 두 dag모두 스케줄되어야 함 타겟 interval은 None이어야 함 두 dag간에 메세지를 주고받을 수 있음(xcoms 대체) ExternalTaskSensor DAG간 종속성 줄 때 사용 예를 들어 DAG1(t1 \u0026gt;\u0026gt; t2 \u0026gt;\u0026gt; t3), DAG2(t3 \u0026gt;\u0026gt; t4 \u0026gt;\u0026gt; t5)가 있을 때 t3가 완료되면 (DAG1이 완료되면) DAG2가 실행되도록 한다. (t4부터) 두 DAG는같은 스케줄이어야 함 (또는 execution_delta 나 execution_date_fn 파라미터 사용) TriggerDagRunOperator와 쓰면 고장남 스케줄 인터벌 None이기 때문 Logging airflow.cfg의 base_log_folder: log 저장 경로 fab_logging_level: flask app builder의 로깅 수준. (flask 기반 웹서버) Metrics Counters: 실패한 task 수 Gauges: queued task 수 Timers: task 완료까지의 밀리초 ","date":"2022-10-14T20:15:22+09:00","image":"https://Haebuk.github.io/p/airflow-%EC%A0%95%EB%A6%AC/AirflowLogo_hubc3c506518d9161c24efab84ad5b3982_59053_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/airflow-%EC%A0%95%EB%A6%AC/","title":"Airflow 정리"},{"content":"\n여러 파이썬 코딩 스타일과 관련된 책을 봤었는데, 가장 잘 정리가 된 책이라고 생각했다.\n최근에 나온 책이라 코드 버전도 최신이다. 데코레이터를 이용한 우아한 재시도 로직은 꽤 멋지다고 생각해 실무에도 적용했다.\n책은 꽤 넓은 범위를 커버한다. 그렇지만 내용이 얕지는 않다고 생각한다. 책을 한 번 읽어서는 모든 내용을 숙지할 수 없을 것 같다.\n읽을 책이 밀려 바로 다시 읽을 순 없지만, 로테이션이 다 돈다면 다시 읽고 싶은 책. 아직 책 속에서 배워야 할 것이 무궁무진하다.\n모든 클린 코드 책이 그렇겠지만, 초보자가 읽으면 별로 도움이 되지 않을 책. 실무에 파이썬을 충분히 사용하고 있으며 평소 클린 코드에 대해 고민을 많이 하는 실무자가 읽으면 좋을 책이다.\n","date":"2022-10-08T11:05:39+09:00","permalink":"https://Haebuk.github.io/p/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%81%B4%EB%A6%B0%EC%BD%94%EB%93%9C-%ED%9B%84%EA%B8%B0/","title":"파이썬 클린코드 후기"},{"content":"주말에 열린 파이콘2022에 다녀왔다. 장소는 명동에 있는 마실 스튜디오.\n컨퍼런스에 참여하는 것은 처음이었는데, 파이썬을 사용하는 유저로서 꼭 참여해보고 싶었기 때문에 큰 마음을 먹고 결제를 했다. 가격은 상당히 비싸다고 생각했는데, 오프라인 참가비 4만원에 굿즈비 2만원이었다. 참가비야 그만한 가치가 있다고 생각했는데, 굿즈는 좀 마음에 들지 않았다. 티셔츠가 있으니 그러려니 하고 넘어갔다.\n토요일과 일요일 이틀동안 진행이 됐고, 각 요일에 열리는 세션 목록이 달랐다. (발표 시간표 링크 바로가기)\n데이터 엔지니어링 항목이 있고, 좀 더 끌리는 주제들이 많은 토요일로 신청하고 싶었으나 일정이 있어 일요일로 했다. 결론부터 말하면 오프라인에서도 발표 세션은 온라인으로 진행됐기 때문에 모든 세션이 녹화되어 상관없었다. (발표 세션 링크는 위 발표 시간표 링크 바로가기 링크 안에 있다.)\n9시 50분쯤 도착했는데, 10시부터 입장이 가능해서 앞에 사람들이 서있었다. 입장 수속을 하고나니 웬걸? 내 양손에 무언가를 바리바리 챙겨주셨다.\n내부는 총 2층으로 이루어져 있었다. 1층에는 데스크와 2개의 세션 룸, 파이썬 도서관, 몇 개의 스탬프 부스가 있었다.\n스탬프를 팜플렛에 찍고 나서 1층에 계신 요기요 직원들과 이벤트 이야기를 하고 (요기요에서 주최하는 이벤트) 2층으로 올라갔다.\n아직 채용부스가 준비되지 않아 분주했다. 부스는 10시 30분부터 운영됐다.\n그전까지 나는 분주히 15개의 스탬프 부스를 돌아 모두 채웠다.\n스탬프를 모두 채우고, 이직 생각이 없는 나는 \u0026lsquo;이제 뭐하지..\u0026rsquo; 생각을 하다 세션 시간표를 보니 파이썬으로 실시간 데이터 처리를 시도해보자. 라는 세션이 있길래 들으러 1층에 내려갔다. (유튜브 링크)\n세션룸은 앞에 빔프로젝터 2개와 의자가 놓여져 있는 다소 열악한 환경이었다. 내가 들은 처음이자 마지막 파이콘2022의 세션이었다.\n어느덧 점심시간이 되고, 12시부터 운영하는 스낵부스를 찾아갔다.\n모든 사람을 포용하는 파이콘의 행동강령때문인지, 비건 메뉴와 글루텐 프리 메뉴가 있었다. 나는 글루텐 프리를 골랐다.\n오후에 시간이 비었다. 자유롭게 이야기 할 수 있는 장소가 있고, 어떤 주제로 이야기할지 적어놓는 화이트보드가 있었다. AI에 대해 얘기하는 시간이 있길래 참가해서 네트워킹을 했다. 다른 분들은 어떻게 일하고 어떤 일을 하는지 알 수 있었던 좋은 시간이었던 것 같다. (파이콘이 끝나고 이분들과 저녁까지 먹었다.)\n3시부터는 토크 콘서트가 열렸다. 채용 담당자분들을 필두로 이야기하는 취업 이야기를 시작으로, \u0026lsquo;알면 쓸모있는 파이썬 잡학 지식(알쓸파잡)\u0026lsquo;에 나도 참가했다.\n파이썬 의존성 관리 툴로 Poetry를 소개했다. 자기소개시 데이터 사이언티스트로 소개한 덕인지, 끝나고 학생분들이 찾아와서 커리어에 대한 질문을 하셨다. 취업전 내가 떠올라 아는 지식 내에서 열심히 상담해드렸다.\n그 후로 o/x 퀴즈랑, 경품 추첨등을 했는데 아쉽게 다 떨어졌다.\n집에 돌아와서 언박싱을 했다.\n티셔츠들도 귀여워서 찍어봤다.\n사실 굿즈만으로도 마음이 훈훈해졌지만 (집에 들고오는게 힘들었다.) 관심사가 일치하는 다양한 사람들과 네트워킹하는 재미도 쏠쏠해서 6만원이 전혀 아깝지 않았다. 내년에도 또 갈 것 같다.\n","date":"2022-10-07T06:42:15+09:00","image":"https://Haebuk.github.io/p/%ED%8C%8C%EC%9D%B4%EC%BD%982022-%ED%9B%84%EA%B8%B0/pycon_hucdfeeccba171252542d11eaec912541f_21276_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%ED%8C%8C%EC%9D%B4%EC%BD%982022-%ED%9B%84%EA%B8%B0/","title":"파이콘2022 후기"},{"content":"들어가며 S3에 저장된 객체를 퍼블릭으로 공개를 할 경우, 자신의 S3 버킷 구조가 그대로 노출되는 문제와 함께, 보안이 우려되는 경우가 있다.\n이 경우 Cloudfront와 원본 액세스 ID, 즉 OAI를 사용해 S3 버킷 구조를 숨기고, 객체를 외부에 공개할 수 있다.\n1. 객체를 공개할 S3 버킷 생성 외부 사용자들에게 공개할 객체를 담는 버킷을 생성한다. 나같은 경우에는 링크드인에 수료증 링크를 걸기 위해 kade-certificate라는 버킷을 생성하고 그 안에 수료증 파일들을 저장했다.\n2. 콘솔을 사용하여 OAI 생성 및 Cloudfront 배포에 추가 Cloudfront 페이지에서 배포 생성을 클릭하고, 생성한 S3 버킷을 원본 도메인으로 선택한다.\n이후 하단의 S3 버킷 액세스에서 Legacy access identities 항목을 선택하고, 새 OAI를 생성한다.\n생성한 OAI를 선택한다.\n그리고 버킷 정책 항목에서 예, 버킷 정책 업데이트를 선택하면 자동으로 S3 정책을 수정해준다.\nGET 요청만 수락하므로 하단의 모든 항목들의 수정 없이 배포 생성 버튼을 클릭한다.\n3. 생성된 Cloudfront url로 접근 배포가 생성되고 나면 다음 화면과 같다.\n도메인 이름에 접근 가능한 URL이 생성된다. (d3sdxwnh25t6pc.cloudfront.net 처럼)\n이 URL 뒤에 버킷내에 저장된 객체명을 붙여 접근하면 다음과 같이 외부에서 접근이 가능함을 알 수 있다.\n이처럼 Cloudfront와 OAI를 사용하여 사용자의 버킷 URL은 숨긴채, 외부에서 접근을 허용할 수 있다.\nReference 원본 액세스 ID(OAI)를 사용하여 Amazon S3 콘텐츠에 대한 액세스 제한: https://docs.aws.amazon.com/ko_kr/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html ","date":"2022-09-09T23:13:41+09:00","permalink":"https://Haebuk.github.io/p/cloudfront-oai%EB%A5%BC-%ED%86%B5%ED%95%9C-s3-url-%EA%B3%B5%EA%B0%9C/","title":"Cloudfront - OAI를 통한 S3 URL 공개"},{"content":"들어가며 최근 흥미로운 프로그램을 발견했다. Udacity에서 제공하는 AWS Machine Learning Engineer Scholarship Program이다.\nAWS 이름을 달고 있는 이유는 이 과정 내에서 AWS 제품인 DeepRacer와 DeepComposer를 실습해 볼 수 있기 때문이다.\n끝까지 수료하면 평가를 치르게 되는데, 기준 점수 이상이 나오면 수료증을 발급해주고, 상위 325등 이내의 성적으로 수료시 후속 프로그램인 AWS Machine Learning Engineer Nanodegree 도 무료로 수강할 수 있는 기회를 제공한다.\nAt the end of the AWS Machine Learning Foundations Course, learners will take an assessment from which top performers will be selected for one of 325 follow-up scholarships to one of Udacity’s most popular and recently refreshed Nanodegree programs: The AWS Machine Learning Engineer Nanodegree program.\nAWS 기계 학습 기초 과정을 마친 325명의 학습자는 Udacity의 가장 인기 있고 최근에 새로워진 Nanodegree 프로그램 중 하나인 AWS 기계 학습 엔지니어 Nanodegree 프로그램을 수강할 수 있게 됩니다.\nUdemy의 nano degree는 평은 좋지만 악랄한 가격으로 유명한데, 무료로 제공한다는 이 기회를 놓칠 수 없어서 냉큼 신청했다.\n강의 구성 강의는 크게 4가지 구성으로 이루어져 있다.\nIntroduction to the Program AWS Machine Learning Foundations Certification Assessment Certificate of Completion Assessment 1. Introduction to the Program 첫번째 챕터는 오리엔테이션으로 이루어져 있다.\nUdacity에 대해 소개하고, 강사진은 누구이며, 선수지식은 무엇이 필요한지, 이 코스에서 무엇을 배울지에 대해 소개한다.\n선수지식은 통계와 파이썬 프로그래밍에 대해 기초수준만 필요하다고 나와있다.\n가볍게 넘어갔다.\n2. AWS Machine Learning Foundations 본격적인 강의 챕터로, 크게 네 가지 구성으로 이루어져 있다.\n첫 번째는 아주 간단한 머신러닝 태스크들의 정의와 문제들을 해결하기 위해 머신러닝을 어떻게 적용하는지 학습한다. 지도, 비지도 및 강화학습에 대한 소개로 봐도 무방한다.\n두 번째는 AWS를 활용한 머신러닝인데, DeepRacer와 DeepComposer 서비스를 이용한다.\nDeepRacer는 강화 학습을 통한 자율 주행 자동차를 훈련시킬 수 있는 서비스, DeepComposer는 작곡 서비스이다. 솔직히 말하면 장난감에 가깝다. 차라리 SageMaker와 같은 서비스를 소개했으면 어땠을까 싶었는데, 워낙 기초 수준의 강의이다 보니 수강생의 흥미를 돋구기 위해 선택한건지, 잘 사용되지 않는 AWS의 서비스를 살리고자(\u0026hellip;)하는 건지는 잘 모르겠다. 여기서 이 강의에 대해 크게 실망을 했다.\n뒤에 두 파트는 소프트웨어 엔지니어링 파트로, 리팩토링, 테스팅, 깃을 통한 버전관리에 대해 소개한다. 모두 기초적인 수준이기 때문에 가볍게 듣고 넘어갔다.\n세 번째 챕터는 평가(시험)을 치루게 된다.\n평가 세 번째 챕터는 두 번째 챕터(AWS Machine Learning Foundations)에서 학습한 내용을 바탕으로 온라인 시험을 치룬다.\n감독은 따로 없고 온라인상으로 60분동안 시험을 치르게 된다.\n60%이상은 수료, 90%이상 부터는 나노 디그리 프로그램에 대한 자격을 얻는다고 한다. 위에 325등과 상이한 내용이라 혼란스러웠다.\n시험 난이도는 생각보다 까다로워서 당황했다. 굉장히 만만히 봤었는데, 당황스러움을 안겨주는 문제들이 많았다. 높은 성적을 받고 싶다면 꼼꼼하게 보는 것을 추천한다. 챕터 2에서 세부 파트가 끝날 때 마다 퀴즈가 나오는데, 틀린 문제들의 오답 노트를 미리 작성했음에도 불구하고 꽤 난항을 겪었다.\n그래도 다행히 한 번에 수료를 하였고, 점수는 따로 공개되지 않았다.\n후기 솔직히 이 강의는 굉장히 별로였고.. 초심자에게도 딱히 추천하지 않는다.\n얻을 게 없다. 머신러닝 기초에 대해 배우고 싶으면 양질의 강의나 책이 매우 많기 때문에 이를 사용하는 것을 추천하며, AWS의 머신러닝 서비스를 사용해보고자 하는 목적으로도 비추천한다.\n오로지 나노디그리를 공짜로 얻기 위한 목적으로 듣는 것이 좋은 것 같다. 11월 6일 나노디그리 대상자를 발표한다고 하니, 그때까지 기다려야 겠다.\n","date":"2022-09-06T13:47:25+09:00","image":"https://Haebuk.github.io/p/udacity-aws-machine-learning-engineer-foundations-2022-%ED%9B%84%EA%B8%B0/udacity-logo-vector_hu025f0b4574d82671ba11d73228d867b5_4100_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/udacity-aws-machine-learning-engineer-foundations-2022-%ED%9B%84%EA%B8%B0/","title":"[Udacity] AWS Machine Learning Engineer Foundations 2022 후기"},{"content":"보안 서비스 어카운트 사용자 어카운트(UserAccount): EKS에서는 IAM과 연결되어 있어 쿠버네티스 관리 대상이 아니고, 네임스페이스의 영향을 받지 않음 서비스 어카운트(ServiceAccount): 쿠버네티스에서만 사용, 파드에서 실행되는 프로세스를 위해 할당. 네임스페이스와 연결된 리소스 파드 기동 시 반드시 서비스 어카운트 한 개를 할당해야 하며, 서비스 어카운트 기반 인증/인가를 하고 있음 지정하지 않을 시 기본 서비스 어카운트가 할당 서비스 어카운트 생성 1 2 # 서비스 어카운트 생성 $ kubectl create serviceaccount sample-serviceaccount 인증이 필요한 개인 저장소에 저장된 이미지를 가져오기 위해 시크릿인 imagePullSecrets를 설정하는 경우 kubectl patch 명령어를 사용하거나 생성할 때 매니페스트를 사용하여 서비스 어카운트 생성 1 2 3 # 생성 후 kubectl patch 명령어로 적용 $ kubectl patch serviceaccount sample-serviceaccount \\ -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;myregistrykey\u0026#34;}]}\u0026#39; 서비스 어카운트와 토큰 생성할 때는 지정하지 않은 시크릿 항목이 존재 kubernetes.io/service-account-token 타입의 시크릿으로 자동 생성 토큰을 변경하고 싶을 떄는 해당 시크릿을 삭제하면 자동으로 재생성됨 1 2 3 4 5 # 서비스 어카운트 정보 확인 $ kubectl get serviceaccounts sample-serviceaccount -o yaml # 연결된 시크릿 $ kubectl get secrets sample-serviceaccount-token-nmmb9 -o yaml 서비스 어카운트는 이 토큰으로 쿠버네티스 API에 대한 인증 정보로 사용가능 파드에 할당된 서비스 어카운트의 할당된 권한이 그대로 파드에 할당되는 권한이 됨 파드의 서비스 어카운트를 명시적으로 지정하려면 spec.serviceAccountName 지정 서비스 어카운트를 지정한 컨테이너를 생성 후 기동된 파드 정보를 확인하면 토큰이 볼륨으로 자동으로 포함되어 있는 것을 알 수 있음 1 2 3 4 5 # 토큰이 볼륨으로 마운트 된 것을 확인 $ kubectl get pods sample-serviceaccount-pod -o yaml # API 서버 인증에 사용되는 토큰과 인증서 등의 파일 확인 $ kubectl exec -it sample-serviceaccount-pod -- ls /var/run/secrets/kubernetes.io/serviceaccount/ 서비스 어카운트에 적절한 RBAC 설정을 한 후 컨테이너에서 다음과 같이 실행하면 API 접근을 위한 인증/인가가 성공하고 기본 네임스페이스로 동작하고 있는 파드 목록 확인 가능 1 2 3 4 5 6 7 8 9 10 11 12 13 # 컨테이너에서 bash 실행 $ kubectl exec -it sample-serviceaccount-pod -- bash # curl 명령어 설치 root@sample-serviceaccount-pod:/# apt update \u0026amp;\u0026amp; apt -y install curl # 토큰을 환경변수로 정의 root@sample-serviceaccount-pod:/# TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token` # 쿠버네티스 API 서버에서 파드 목록 확인 root@sample-serviceaccount-pod:/# curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt \\ https://kubernetes/api/v1/namespaces/default/pods 토큰 자동 마운트 설정을 비활성화 하려면 automountServiceAccountToken을 false로 설정 해당 서비스 어카운트로 기동하는 파드는 토큰을 볼륨으로 마운트 하지 않음 토큰을 사용할 때는 명시적으로 spec.automountServiceAccountToken을 true로 설정하면 마운트 가능 도커 레지스트리 인증 정보 자동 설정 imagePullSecret이 지정된 서비스 어카운트를 할당한 파드가 기동한 경우 자동으로 파드의 서비스 어카운트을 사용하여 도커 이미지를 가져온다. 1 2 # spec.imagePullSecrets가 자동으로 포함된 것을 확인 $ kubectl get pods sample-sa-pullsecret-pod -o yaml spec.imagePullSecrets는 서비스 어카운트에 복수로 지정이 가능하므로 자동으로 여러 인증 정보 설정 가능 RBAC(Role Based Access Control) 어떤 조작을 허용하는지를 결정하는 롤을 생성하고 서비스 어카운트 등의 사용자에게 롤을 연결(롤바인딩)하여 권한을 부여 AggregationRule을 사용해 여러 롤을 집약한 롤을 생성하여 관리성을 향상할 수 있음 롤과 롤바인딩에는 네임스페이스 수준의 리소스와 클러스터 수준의 리소스 두 가지 존재 네임스페이스 수준의 리소스: 롤, 롤바인딩 클러스터 수준의 리소스: 클러스터롤, 클러스터롤바인딩 롤과 클러스터롤 네임스페이스 범위의 리소스를 대상으로 인가 설정 가능 클러스터롤의 경우 노드/네임스페이스/영구볼륨과 같은 클러스터 범위의 리소스나 쿠버네티스 API 정보를 가져오는 nonResourceURL에 대한 권한도 설정 가능 주로 apiGroups, resources, verbs 세 가지 지정 apiGroups와 resources로 지정된 리소스에 대해 verbs 권한 인가 롤에 지정할 수 있는 실행 가능한 조작(verbs)\n종류 개요 * 모두 처리 create 생성 delete 삭제 get 조회 list 목록 조회 patch 일부 업데이트 update 업데이트 watch 변경 감시 주의 사항 디플로이먼트 리소스에 대해 롤을 기술할 때 여러 apiGroups가 존재 extensions/v1beta1, extensions/v1beta2, apps/v1으로 변경되어 왔음 모든 디플로이먼트 리소스를 대상으로 롤을 생성하는 경우에 주의 deployment 리소스와 deployment/scale 서브 리소스는 개별적으로 지정 deployment/scale이 지정되어 있지 않으면 레플리카 수를 변경하는 스케일 처리 불가 롤 생성 rules는 여러 개 설정 가능 네임스페이스 지정 클러스터롤 생성 rules에 noneResourceURLs 지정 가능 헬스 체크용 엔드포인트나 버전 정보 표시용 엔드포인트의 URL metadata.namespace 지정 불가 클러스터롤의 Aggregation 여러 클러스터롤의 정의를 읽는 기능 클러스터롤에 정의된 레이블 기반, 집계되는 쪽 클러스터롤에 정의된 롤은 반영되지 않음 1 2 # 집계된 클러스터롤 확인 $ kubectl get clusterroles sample-aggregated-clusterrole -o yaml 나중에 집계되는 측의 클러스터롤을 변경하는 경우에도 집계되는 측의 클러스터롤에 자동으로 반영 쿠버네티스가 생성하는 클러스터롤 단순한 권한을 사용하는 경우 프리셋 사용 클러스터롤명 내용 cluster-admin 모든 리소스 관리 가능 admin 클러스터롤 편집 + 네임스페이스 수준의 RBAC edit 읽기 쓰기 view 읽기 전용 롤바인딩과 클러스터롤바인딩 roleRef에서 연결하는 롤과 subjects에 연결하는 사용자나 서비스 어카운트 지정 하나의 롤바인딩당 하나의 롤만 가능, subjects에는 여러 사용자나 서비스 어카운트 지정 가능 롤바인딩: 특정 네임스페이스에 롤 또는 클러스터 롤에서 정의된 권한 부여 네임스페이스 새로 생성시 그 네임스페이스에도 같은 롤바인딩 추가해야 함 클러스터롤바인딩: 모든 네임스페이스에 클러스터롤에서 정의한 권한 부여 네임스페이스 새로 생성해도 네임스페이스 간에 같은 권한 부여 가능 롤바인딩 생성 사용자에 대해 특정 네임스페이스에서 롤 또는 클러스터롤에 정의한 권한 부여 클러스터롤바인딩 생성 사용자에 대해 모든 네임스페이스에서 클러스터롤로 정의된 권한 부여 보안 컨텍스트 각 컨테이너에 대한 보안 설정 설정 가능 항목\n종류 개요 privileged 특수 권한을 가진 컨테이너로 실행 capabilities Capabilities 추가와 삭제 allopPrivilegeEscalation 컨테이너 실행 시 상위 프로세스보다 많은 권한을 부여할지 여부 readOnlyRootFileSystem root 파일 시스템을 읽기 전용으로 할지 여부 runAsUser 실행 사용자 runAsGroup 실행 그룹 runAsNonRoot root에서 실행 거부 seLinuxOptions SELinux 옵션 특수 권한 컨테이너 생성 spec.containers[].securityContext.privileged를 true로 설정 컨테이너 내부에서 기동하는 프로세스의 리눅스 Capabilities가 호스트와 동등한 권한을 가짐 Capabilities 부여 더 세분화된 특정 Capabilities 부여/제거 가능 1 2 # Capabilities 확인 $ kubectl exec -it sample-capabilities -- capsh --print | grep Current root 파일 시스템의 읽기 전용 설정 컨테이너 이미지에 포함된 파일 등을 읽기 전용으로 설정 가능 root 파일 시스템을 읽기 전용으로 설정하면 커널 관련 파일 등을 변경할 수 없어 보안성 향상 1 2 # 파일 시스템을 읽기 전용으로 마운트 $ kubectl exec -it sample-rootfile-readonly -- touch /etc/os-release 네트워크 정책 클러스터 내부에서 파드 간에 통신할 경우 트래픽 룰을 규정 네트워크 정책을 사용하지 않을 경우 모든 파드는 서로 통신이 가능 네트워크 정책을 사용한다면 네임스페이스별로 트래픽을 전송하지 못하게 하거나 모든 파드 간 통신을 차단하고 특정 파드 간 통신을 허용하는 화이트리스트 방식 사용 가능 네트워크 정책 생성 네트워크 정책은 인그레스(수신)과 이그레스(송신)로 구성 인그레스: 인바운드 방향의 트래픽 룰을 설정 이그레스: 아웃바운드 방향의 트래픽 룰 설정 설정 범위를 podSelector로 지정 네트워크 정책은 네임스페이스별로 생성해야 함 네트워크 정책 종류와 통신 제한 범위\n정책 종류 인그레스 룰의 경우 이그레스 룰의 경우 podSelector 특정 파드에서 들어오는 통신 허가 특정 파드로 나가는 통신 허가 namespaceSelector 특정 네임스페이스상에 있는 파드에서 들어오는 통신 허가 특정 네임스페이스상에 있는 파드로 나가는 통신 허가 ipBlock 특정 CIDR(IP 주소)에서 들어오는 통신 허용 특정 CIDR(IP 주소)로 나가는 통신 허용 화이트리스트 방식과 블랙리스트 방식 화이트리스트 방식: 모든 트래픽을 차단해 두고 특정 트래픽만 허가 블랙리스트 방식: 모든 트래픽 허가해 두고 특정 트래픽만 차단 클라우드의 네트워크 기본 설정에서 인바운드는 전체를 차단하고 아웃바운드는 전체를 허용하는 것이 일반적 시크릿 리소스 암호화 시크릿 리소스는 base64로 인코드되어 있을 뿐으로 깃 저장소에 업로드하는 것은 보안상 위험할 수 있음 별도의 암호화 필요 kubesec 오픈 소스 소프트웨어 KMS를 사용해 암호화 가능 파일 전체를 암호화하지 않고 시크릿 구조를 유지한 채 값만 암호화하므로 가독성 높음 시크릿 매니페스트를 암호화하기 때문에 깃 저장소에도 업로드 가능 SealedSecret 오픈 소스 소프트웨어 암호화된 SealedSecret이라는 커스텀 리소스 생성 후 클러스터에 등록하면 클러스터 내부에서 SealedSecret에서 시크릿 리소스로 변환되는 구조 생성된 SealedSecret 리소스는 암호화되어 있기 때문에 깃 저장소에 배치 가능 ExternalSecret 오픈 소스 소프트웨어 SealedSecret과 유사한 구조 ","date":"2022-07-10T17:12:39+09:00","image":"https://Haebuk.github.io/p/%EB%B3%B4%EC%95%88/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EB%B3%B4%EC%95%88/","title":"보안"},{"content":"유연한 고급 스케줄링 필터링과 스코어링 필터링: 파드 스케줄시 충분한 리소스가 있는지, 필수 조건으로 지정한 레이블을 가진 노드인지 등을 체크 스코어링: 필터링 후 노드 목록에 순위를 매겨 가장 적합한 노드를 계산 필터링과 스코어링 이후 우선순위가 같은 스케줄링 대상 노드가 여러개 존재 시 무작위 선택 매니페스트에서 지정하는 스케줄링 사용자가 배치하고 싶은 노드를 선택하는 방법과 관리자가 배치하고 싶지 않은 노드를 지정하는 방법이 있음 쿠버네티스 사용자가 배치하고 싶은 노드를 선택하는 방법 종류 개요 nodeSelector(가장 단순한 노드 어피니티) 단순한 노드 어피니티 기능 노드 어피니티 특정 노드상에서만 실행 노드 안티어피니티 특정 노드 이외에서 실행 인터파트 어피니티 특정 파드가 존재하는 도메인(노드, 존)에서 실행 인터파트 안티어피니티 특정 파드가 존재하지 않는 도메인에서 실행 빌트인 노드 레이블과 레이블 추가 빌트인 노드 레이블: 노드에 미리 지정되어 있는 레이블 1 2 # 노드에 할당된 레이블 정보 $ kubectl get nodes -o json | jq \u0026#34;.items[] | .metadata.labels\u0026#34; 1 2 3 4 5 6 # kubectl get nodes에서 확인한 노드명을 지정하여 레이블 부여 $ kubectl label node NODE_NAME disktype=hdd cpuspec=low cpugen=3 $ kubectl label node NODE_NAME disktype=ssd cpuspec=low cpugen=2 $ kubectl label node NODE_NAME disktype=hdd cpuspec=high cpugen=4 1 2 # 노드 목록과 disktype 레이블 표시 $ kubectl -L=disktype,cpuspec,cpugen get node nodeSelector(가장 단순한 노드 어피니티) 단순한 노드 어피니티만을 실행하는 경우 equality-based는 단일 조건만 지정할 수 있음 1 2 # 반드시 disktype=ssd인 레이블을 가진 노드에서 기동 $ kubectl get pods sample-nodeselector -o wide 노드 어피니티 파드를 특정 노드에 스케줄링하는 정책 spec.affinity.nodeAffinity에 작성 노드 어피티니의 필수 스케줄링 정책과 우선 스케줄링 정책\n설정 항목 개요 requiredDuringSchedulingIgnoredDuringExecution 필수 스케줄링 정책 preferredDuringSchedulingIgnoredDuringExecution 우선적으로 고려되는 스케줄링 정책 필수 조건 requiredDuringSchedulingIgnoredDuringExecution이 만족되지 않은 스케줄링을 수행시 스케줄링에 실패\npreferredDuringSchedulingIgnoredDuringExecution는 어디까지나 우선적으로 스케줄링을 하는 것 필수 조건의 스케줄링 정책\nnodeSelectorTerms: 어떤 조건의 노드가 스케줄링 가능한 노드인지 정의 복수 지정 가능, OR 조건 matchExpressions: AND 조건이 되도록 여러 조건 지정 1 2 3 4 5 6 7 8 # (A and B) or (C and D) nodeSelectorTerms: - matchExpressions: - A - B - matchExpressions: - C - D 우선 조건의 스케줄링 정책에서도 복수 조건 지정 가능 우선순위의 가중치(weight)와 조건 쌍 여러개 1 2 3 4 5 6 7 8 9 10 11 12 # (A and B)가 가중치 X, (C and D)가 가중치 Y의 우선순위로 스케줄링 실시 preferredDuringSchedulingIgnoredDuringExecution: - weight: X preference: matchExpressions: - A - B - weight: Y preference: matchExpressions: - C - D matchExpressions 오퍼레이터와 집합성 기준 조건 matchExpressions는 key 레이블, 오퍼레이터, values 레이블이라는 세 가지 요소로 구성 values 레이블은 배열로 취급 레플리카셋/디플로이먼트/데몬셋/스테이트풀셀/잡의 셀렉터에서 사용 가능 1 2 3 4 5 6 7 8 9 10 - matchExpressions: - key: disktype operator: In values: - ssd - hdd # 한 줄로 정의한 경우 - matchExpressions: - {key: disktype, operator: In, values: [ssd, hdd]} matchExpressions에서 사용 가능한 오퍼레이터\n오퍼레이터 종류 사용 방법 의미 In A In [B, \u0026hellip;] 레이블 A의 값이 [B, \u0026hellip;] 중 어느 하나 이상과 일치 NotIn A NotIn [B, \u0026hellip;] 레이블 A의 값이 [B, \u0026hellip;] 중 어느 것에도 일치하지 않음 Exists A Exists [] 레이블 A가 존재 DoesNotExist A DoesNotExist [] 레이블 A가 존재하지 않음 Gt A Gt [B] 레이블 A의 값이 B보다 큼 Lt A Lt [B] 레이블 A의 값이 B보다 작음 Exists는 values 지정 불가 (key가 존재하는지 여부만이 조건) DoesNotExist는 key가 존재하지 않는 것이 조건 Gt/Lt는 values에 하나의 정수 값만 지정 노드 안티어피니티 파드를 특정 노드 이외의 다른 노드에 스케줄링하는 정책 spec.affinity.nodeAffinity 조건에 부정형 오퍼레이터를 지정(엄밀히 말하면 노드 안티어피니티는 존재하지 않음) 인터파드 어피니티 특정 파드가 실행되는 도메인에 파드를 스케줄링 하는 정책 spec.affinity.podAffinity에 조건 지정 가까이 있는 파드끼리는 통신 레이턴시가 적음 노드들간 기능 차이가 적을 경우 노드를 의식하지 않고 파드 기반의 스케줄링만으로 제어 가능 topology는 어느 범위(도메인)를 스케줄링 대상으로 할지를 지정 requiredDuringSchedulingIgnoredDuringExecution은 배열로 여러 조건 지정 가능 1 2 3 4 5 6 7 8 9 10 11 12 # (A and B)의 파드와 같은 X에 있는 노드 or (C and D)의 파드와 같은 Y에 있는 노드 requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - A - B topologyKey: X - labelSelector: matchExpressions: - C - D topologyKey: Y 인터파드 안티어피니티 특정 파드가 없는 도메인에서 동작시키는 정책 spec.affinity.PodAntiAffinity 지정 가능 여러 조건을 조합한 파드 스케줄링 노드 어피니티/노드 안티어피니티/인터파드 어피니티/인터파드 안티어피니티 조합하여 사용 가능 테인트와 톨러레이션 쿠버네티스 관리자가 배치하고 싶지 않은 노드를 지정하는 방법 노드에 대한 테인트를 설정해 두고 그것을 허용할 수 있는 파드만 스케줄링 허가 조건에 맞지 않는 파드를 노드에서 축출 가 테인트 부여 세 가지 파라미터를 사용한 Key=Value:Effect 형식으로 구성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 특정한 한 대의 노드에 env=prd:Noschedule 테인트 부여 $ kubectl taint node NODE_NAME env:prd=NoSchedule # 특정 레이블을 가진 모든 노드에 env=prd:NoSchedule 테인트 부여 $ kubectl taint node -l kubernetes.io/os=linux env=prd:NoSchedule # env를 키로 하는 테인트 삭제 $ kubectl taint node NODE_NAME env- # env를 키로 하는 NoSchedule 테인트 삭제 $ kubectl taint node NODE_NAME env:NoSchedule- # 부여된 테인트 확인 $ kubectl describe node NODE_NAME Key, Value는 임의의 값으로 지정하고 노드 어피니티와 동일하게 일치 여부를 조건으로 사용 Effect는 테인트와 톨러레이션이 일치하지 않을 경우의 동작 Effect 종류\nEffect 종류 개요 PreferNoSchedule 가능한 한 스케줄링하지 않음 NoSchedule 스케줄링하지 않음(이미 스케줄링된 파드는 유지) NoExecute 실행을 허가하지 않음(이미 스케줄링된 파드는 정지 PreferNoSchedule 파드에 톨러레이션 설정이 없는 경우나 테인트에 일치하지 않아도 스케줄링 대상 노드가 됨 NoSchedule 파드에 톨러레이션 설정이 없는 경우나 테인트에 일치하지 않으면 스케줄링 허가하지 않음 NoExecute 일반적인 파드 정지처럼 terminationGracePeriodSeconds로 유예 기간 설정 톨러레이션을 지정한 파드 기동 Key/Value/Effect 지정 후 테인트에서 부여된 Key/Value/Effect가 같은 경우 허용 완전 일치뿐만 아니라 Key/Value/Effect중 하나를 미지정한 경우 와일드카드로 처리 spec.tolerations로 지정 톨러레이션에서 지정 가능한 오퍼레이터\n오퍼레이터 개요 Equal Key = Value Exists Key 존재 NoSchedule과 NoExecute의 경우 조건과 Effect 모두 일치할 경우 스케줄링 PreferNoSchedule은 조건이 일치하지 않아도 스케줄링 되지만 우선순위 하락 어떤 테인트가 부여되든 관계엾이 스케줄링하는 경우 Exists 오퍼레이터만 지정하면 모든 조건에 일치 가능 1 2 tolerations: - operator: Exists NoExecute 일정 시간 허용 조건이 일치하는 파드는 일정 기간만 가동을 허용하는 스케줄링을 할 때 사용 여러 개의 테인트와 톨러레이션 여러 테인트가 부여된 경우 파드의 톨러레이션은 모든 테인트 조건을 만족해야 해당 노드에 스케줄링 가능 장애 시 부여되는 테인트와 축출 노드 장애 발생 시 자동으로 NoExecute의 테인트 부여하여 자동으로 파드 축출 노드 장애시 부여되는 테인트\nEffect Key 개요 NoSchedule node.kubernetes.io/not-ready 노드 상태가 NotReady NoSchedule node.kubernetes.io/unreachable 노드와의 네트워크 접속이 되지 않는 경우 (Unknown) 장애가 발생해도 계속 기동하고 싶은 경우 위 테인트에 대응하는 톨러레이션을 설정해야 함 기본적으로 tolerationsSecond=300으로 부여되며 300초 이내에 노드 문제를 해결하지 않으면 파드가 축출됨 1 2 3 4 5 6 7 8 9 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operators: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operators: Exists tolerationSeconds: 300 쿠버네티스가 부여하는 그 외 테인트 Effect Key 개요 NoSchedule node.kubernetes.io/memory-pressure 노드에 메모리 부족 NoSchedule node.kubernetes.io/disk-pressure 노드에 디스크 뷰족 NoSchedule node.kubernetes.io/pid-pressure 노드에 PID 고갈 NoSchedule node.kubernetes.io/network-unavailable 노드의 네트워크가 연결되지 않음 NoSchedule node.kubernetes.io/unschedulable kubectl cordon에 의해 스케줄링에서 제외됨 문제가 발생해도 스케줄링하고 실행해야 하는 중요도가 높은 파드는 톨러레이션 설정 클라우드 환경에서 노드가 생성되고 삭제될 때 부여되는 테인트도 있음 PriorityClass를 이용한 파드 우선순위와 축출 파드가 이미 리소스의 한계까지 스케줄링된 상태에서 우선순위가 높은 파드를 생성하는 경우 기존 파드 축출 가능 여러 파드가 스케줄링 대기 상태일 경우 스케줄링 순서는 수시로 정렬되어 우선순위가 높은 파드부터 스케줄링 PriorityClass 생성 system-으로 시작하는 이름은 예약어로 등록되어 있으 붙일 수 없음 우선순위(value)와 설명(description)으로 구성됨 globalDefault 옵션을 true로 설정시 PriorityClass가 지정되지 않은 파드에 기본 우선순위 설정으로 PriorityClass 부여 globalDafault: true의 PriorityClass가 여러 개 존재할 경우 가장 우선순위가 낮은 PriorityClass가 기본 우선순위 설정으로 부여 PriorityClass가 전혀 연결되지 않는 경우 priority는 0 PriorityClass를 파드의 spec.podPriorityClass에 지정 해당 podPriorityClass를 가지고 spec.Priority를 자동 업데이트 수동으로 spec.Priority 지정한 경우 파드 생성 불가 1 2 # 파드에 지정된 우선순위 확인 $ kubectl get pods sample-high-priority -o jsonpath=\u0026#39;{.spec.priority}\u0026#39; 파드 축출시 노드상에 원순위가 낮은 파드를 제외한 상태에서 어피니티 등의 조건을 만족하는지 판단 우선순위가 낮은 파드에 인터파드 어피니티를 설정하면 스케줄링이 수행되지 않는 경우 있음 인터파드 어피니티는 우선순위가 높거나 같은 파드에 스케줄링 해야함 우선순위 축출 비활성화 우선순위를 설정하고 싶지만 다른 파드를 축출하고 싶지 않은 경우 사용 스케줄링시 우선순위가 높은 파드부터 스케줄링됨 preemtionPolicy를 Never로 설정 PriorityClass와 PodDisruptionBudget의 경합 PriorityClass에 의한 축출은 podDisruptionBudget을 고려하여 스케줄링 기타 스케줄링 커스텀 스케줄러 생성 가능 기존 쿠버네티스 스케줄러에 할당되지 않도록 파드에 spec.schedulerName 지정해야 함 스캐줄러를 구현하지 않은 경우 Pending 상태 유지 ","date":"2022-06-25T17:12:36+09:00","image":"https://Haebuk.github.io/p/%EC%9C%A0%EC%97%B0%ED%95%9C-%EA%B3%A0%EA%B8%89-%EC%8A%A4%EC%BC%80%EC%A4%84%EB%A7%81/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EC%9C%A0%EC%97%B0%ED%95%9C-%EA%B3%A0%EA%B8%89-%EC%8A%A4%EC%BC%80%EC%A4%84%EB%A7%81/","title":"유연한 고급 스케줄링"},{"content":"메인터넌스와 노드 정지 스케줄링 대상에서 제외와 북귀(cordon/uncordon) 쿠버네티스 노드는 두 상태를 가짐 SchedulingDisabled: 노드가 스케줄링 대상에서 제외 (파드 신규 생성 x), 이미 실행 중인 파드에는 영향 없음 SchedulingEnabled: 기본 상태 1 2 3 4 5 # 노드 중 하나를 SchedulingDisabled로 변경 $ kubectl cordon NODE_NAME # 노드 중 하나를 SchedulingEnabled로 변경 $ kubectl uncordon NODE_NAME 노드 배출 처리로 인한 파드 축출(drain) 실행 중인 파드를 축출해야 할 경우 사용 SchedulingDisabled 상태로 바꾼 후 각 파드에 SIGTERM 신호를 보내므로 cordon을 실행할 필요 없음 1 2 # 실행 중인 파드를 모두 축출(데몬셋 이외) $ kubectl drain NODE_NAME --force --ignore-daemonsets drain시 다음과 같은 케이스에서는 에러가 발생 디플로이먼트등으로 관리되지 않는 파드 삭제(단일 파드는 파드 삭제 후 재성성이 불가하기 때문) \u0026ndash;force옵션으로 해결 가능 로컬 스토리지 사용하는 파드 삭제(로컬 스토리지 데이터가 삭제되므로) \u0026ndash;delete-local-data로 해결 가능 데몬셋이 관리하는 파드 삭제 \u0026ndash;ignore-daemonset 옵션을 사용해 해결 가능 PodDisruptionBudget(PDB)을 사용한 안전한 축출 파드 축출 시 특정 디플로이먼트하에 있는 레플리카가 동시에 정지되면 다운타임이 발생 여러 노드에서 동시 배출 처리를 한 경우 해당 현상이 발생할 확률 증가 노드가 배출 처리를 할 때 파드를 정지할 수 있는 최대 수를 제한하는 리소스 최소 기동 개수와 최대 정지 개수를 보면서 노드상의 파드 축출 HPA에 의해 파드 수가 변화하는 환경에서는 백분율로 지정하는 것이 좋음 동시에 여러 노드를 배출 처리하는 경우에도 효과적 minAvailable, minUnavailable은 둘 중 하나만 설정 가능 파드에 여러 PDB가 연결되면 축출 실패 ","date":"2022-06-22T17:12:29+09:00","image":"https://Haebuk.github.io/images/Kubernetes-Logo.wine.png","permalink":"https://Haebuk.github.io/p/%EB%A9%94%EC%9D%B8%ED%84%B0%EB%84%8C%EC%8A%A4%EC%99%80-%EB%85%B8%EB%93%9C-%EC%A0%95%EC%A7%80/","title":"메인터넌스와 노드 정지"},{"content":"헬스 체크와 컨테이너 라이프 사이클 헬스 체크 파드가 정상인지 판단하는 기능 이상 종료된 경우 spec.restartPolicy에 따라 파드 재시작 헬스체크 방법 Probe 종류 역할 실패 시 동작 Liveness Probe 파드 내부의 컨테이너가 정상 동작 중인지 확인 컨테이너 재가동 Readiness Probe 파드가 요청을 받아들일 수 있는지 확인 트래픽 차단(파드 재기동x) Startup Probe 파드의 첫 번째 기동이 완료되었는지 확인 다른 Probe 실행 시작 x Liveness Probe: 헬스 체크에 한 번 실패하면 재시작 없이는 복구가 어려울 때 사용 Readiness Probe: db에 정상적으로 접속되는지, 캐시에 로드가 끝났는지, 기동 시간이 오래 걸리는 프로세스가 끝났는지 등등 체크 실패한 경우 트래픽이 파드에 전송되지 않도록 함 Startup Probe: 처음 기동하는데 시간이 오래 걸릴 경우 사용, 완료까지 다른 Probe나 서비스가 시작되지 않음 헬스체크 방식 헬스 체크는 컨테이너별로 이루어짐 하나의 컨테이너라도 실패하면 전체 파드가 실패한 것으로 간주 헬스 체크 방식 내용 exec 명령어를 실행하고 종료 코드가 0이 아니면 실패 httpGet HTTP GET 요청 실행 후 Status Code가 200~399가 아니면 실패 tcpSocket TCP 세션이 연결되지 않으면 실패 명령어 기반의 체크(exec) 명령어로 실행하고 종료 코드로 확인 가장 유연성이 높은 체크 명령어는 컨테이너별로 실행 1 2 3 livenessProbe: exec: command: [\u0026#34;test\u0026#34;, \u0026#34;-e\u0026#34;, \u0026#34;/ok.txt\u0026#34;] HTTP 기반의 체크(httpGet) HTTP GET 요청의 Status Code로 확인 HTTP GET 요청은 kubelet에서 이루어짐 1 2 3 4 5 6 7 8 9 livenessProbe: httpGet: path: /health port: 80 scheme: HTTP host: web.example.com httpHeaders: - name: Authorization value: Bearer Token TCP 기반의 체크(tcpSocket) TCP 세션 활성화를 검증하여 확인 1 2 3 livenessProbe: tcpSocket: port: 80 헬스 체크 간격 Liveness Probe는 실패시 파드가 재시작하므로 실패까지의 체크 횟수를 설정 successThreshold는 1이상, Liveness Probe와 Startup Probe의 경우 반드시 1 첫 번째 체크까지의 지연은 가급적 사용 x, Startup Probe 사용 설정 항목 내용 initialDelaySeconds 첫 헬스 체크까지의 지연(최대 failureThreshold까지 연장) periodSeconds 헬스 체크 간격 시간(초) timeoutSeconds 타임아웃까지의 시간(초) successThreshold 성공이라고 판단하기까지 체크 횟수 failureThreshold 실패라고 판단하기까지 체크 횟수 1 2 3 4 5 6 livenessProbe: initialDelaySeconds: 5 periodSeconds: 5 timeoutSecods: 1 successThreshold: 1 failureThreshold: 1 헬스 체크 생성 헬스 체크 방법 세 가지와 헬스 체크 방식 세가지로 총 9가지 패턴의 헬스 체크 가능 Liveness, Readiness, Startup Probe는 하나 이상 지정 가능 1 2 # 파드에 생성된 Probe 확인 $ kubectl describe pod sample-healthcheck | egrep -E \u0026#34;Liveness|Readiness\u0026#34; 추가 Ready 조건을 추가하는 파드 ready++(ReadinessGate) 파드가 정말 Ready 상태인지 추가 체크 클라우드 외부 로드밸런서와의 연계에 시간이 걸리는 경우 사용 일반적인 파드의 Ready 판단만으로는 롤링 업데이트시 기존 파드가 한 번에 사라져 안전하게 업데이트 할 수 없는 경우 사용 ReadinessGate를 통과할 때까지 서비스 전송 대상 x, 롤링 업데이트 시 다음 파드 기동으로 이동 x 여러개 사용 가능. 모든 상태가 Ready가 되지 않으면 파드는 Ready 상태가 되지 않음 Readiness Probe를 무시한 서비스 생성 스테이트풀셋에서 Headless 서비스 사용 시 파드가 Ready가 되지 않아도 클러스터 구성을 위해 파드의 이름 해석이 필요한 경우가 존재 Readiness Probe가 실패한 경우에도 서비스에 연결되게 하려면 spec.publishNotReadyAddresses를 true로 설정 Startup Probe를 사용한 지연 체크와 실패 Startup Probe의 경우 failureThreshold의 값을 충분히 크게 두는 것이 좋음 1 2 3 4 5 # Startup Probe에서 감시하고 있는 파일 생성 $ kubectl exec -it sample-startup -- touch /root/startup # Startup Probe가 성공한 후 Liveness와 Readiness Probe가 실행된 것을 알 수 있음 $ kubectl exec -it sample-startup -- head /root/log 컨테이너 라이플사이클과 재기동(restartPolicy) 컨테이너 프로세스 정지 또는 헬스 체크 실패시 컨테이너 재기동 여부, 방식 설정 잡은 Always 선택 불가 restartPolicy 내용 Always 파드 정지시 항상 파드 재기동 OnFailure 예상치 못하게 파드가 정지한 경우(종료 코드 0 이외) 파드 재기동 Never 파드 재기동 x 초기화 컨테이너 파드 내에서 메인이 되는 컨테이너 기동 전 별도의 컨테이너 기동 설정에 필요한 스크립트 등을 메인 컨테이너에 보관하지 않는 상태 유지 가능(보안, 이미지 용량 감소) spec.initContainers 복수 지정 가능, 위에서부터 순차적 커맨드 실행 저장소에서 파일 가져오기, 컨테이너 기동 지연, 설정 파일 동적 생성, 서비스 생성 확인, 메인 컨테이너 기동 전 체크 작업 등 1 2 # 초기화 컨테이너에서 추가된 파일 순서 확인 $ kubectl exec -it sample-initcontainer -- cat /usr/share/nginx/html/index.html 기동 직후와 종료 직전에 임의의 명령어 실행(postStart/preStop) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 기동 후 바로 확인한 상태 $ kubectl exec -it sample-lifecycle-exec -- ls /tmp started # 기동 20초 경과 후 확인한 상태 $ kubectl exec -it sample-lifecycle-exec -- ls /tmp poststart started # 파드 정지 $ kubectl delete -f sample-lifecycle-exec.yaml # 삭제 요청 후 바로 확인한 상태 $ kubectl exec -it sample-lifecycle-exec -- ls /tmp poststart prestop started postStart는 비동기 실행이므로 기동시 파일을 배치하는 작업은 초기화 컨테이너를 사용하거나 Entrypoint 안에서 실행 postStart, preStop은 여러 번 실행될 가능성도 있음 postStart에 타임아웃 설정 불가 postStart 실행 중에는 Probe 실행되지 않음 파드의 안전한 정지와 타이밍 기동 중인 파드의 삭제 요청이 쿠버네티스 API 서버에 도착시 비동기로 \u0026lsquo;preStop 처리 + SIGTERM 처리\u0026rsquo;와 \u0026lsquo;서비스에서 제외 설정\u0026rsquo;이 이루어짐 서비스 제외 처리가 끝나기 전 몇 초간 \u0026lsquo;preStop 처리 + SIGTERM 처리\u0026rsquo;에서 대기하거나 요청을 받으면서 정지 처리하는 것이 효과적 전자는 애플리케이션 수정 필요 x 파드에는 spec.terminationGracePeriodSeconds(기본 30초) 설정값이 있음 해당 시간 안에 \u0026lsquo;preStop 처리 + SIGTERM 처리\u0026rsquo; 끝내야 함 끝나지 않는 다면 SIGKILL 신호가 컨테이너에 전달되어 강제 종료 강제 종료를 막기 위해 \u0026lsquo;preStop 처리 + SIGTERM 처리\u0026rsquo;완료 시간을 충분히 확보해야 함 preStop 처리만으로 terminationGracePeriodSeconds 시간을 모두 사용한 경우에느 추가로 2초가 SIGTERM 시간으로 확보됨 1 2 3 4 5 # terminationGracePeriodSeconds=3으로 삭제 $ kubectl delete pod sample-termination --grace-period 3 # 강제로 즉시 삭제 $ kubectl delete pod sample-termination --grace-period 0 --force 리소스를 삭제했을 때의 동작 레플리카셋등의 상위 리소스가 삭제되면 하위 리소스가 되는 파드 등을 삭제하기 위해 gc수행 생성된 파드에는 어떤 레플리카셋으로 생성됐는지 판별하기 위해 metadata.ownerReferences 아래에 자동으로 정보 저장 1 2 # 파드 정의에 포함돈 상위 리소스 정보 확인 $ kubectl get pods sample-rs-l23kds -o json | jq .metadata.ownerReferences 삭제됐을 때 동작 Background(기본값) 레플리카셋 즉시 삭제 후 파드는 gc가 백그라운드에서 비동기로 삭제 Foreground 레플리카셋을 즉시 삭제하지 않고 deletionTimestamp, metadata.finalizers = foregroundDeletion으로 설정 gc가 각 파드에서 blockOwnerDeletion = true 인것을 삭제 (false인 것은 Foreground 삭제라도 Background로 삭제) 모든 삭제가 끝나면 레플리카셋 삭제 Orphan 레플리카셋 삭제시 파드 삭제를 하지 않음 Foreground 사용하려면 API 조작해여 함 ","date":"2022-06-19T17:12:20+09:00","image":"https://Haebuk.github.io/p/%ED%97%AC%EC%8A%A4%EC%B2%B4%ED%81%AC%EC%99%80-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EB%9D%BC%EC%9D%B4%ED%94%84%EC%82%AC%EC%9D%B4%ED%81%B4/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%ED%97%AC%EC%8A%A4%EC%B2%B4%ED%81%AC%EC%99%80-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EB%9D%BC%EC%9D%B4%ED%94%84%EC%82%AC%EC%9D%B4%ED%81%B4/","title":"헬스체크와 컨테이너 라이프사이클"},{"content":"리소스 관리와 오토 스케일링 리소스 제한 CPU/메모리 리소스 제한 CPU는 클럭 수가 아닌 1vCPU를 1,000m 단위로 지정 단위 리소스 유형 단위 CPU 1 = 1000m = 1 vCPU 메모리 1G = 1000M (1Gi = 1024Mi) Requests: 사용하는 리소스 최솟값 지정한 양의 리소스가 노드에 존재하지 않으면 스케줄링 되지 않음 Limits: 사용할 리소스의 최댓값 노드에 Limits로 지정한 리소스가 없어도 스케줄링 됨 Requests만 설정한 경우 Limits는 자동 설정되지 않고 부하가 최대로 상승할 때까지 리소스 계속 소비 파드가 많이 가동하는 노드에서 리소스 뻇기 발생, OOM 발생 Limits만 설정한 경우 은 값이 Requests에 설정 시스템에 할당된 리소스와 Eviction 매니저 일반 리소스는 고갈 시 쿠버네티스 자체가 동작하지 않거나 그 노드 전체에 영향 가능성 각 노드에는 kube-reserved, system-reserved 두 가지 리소스가 시스템용으로 확보 실제 파드 할당 가능 리소스는 리소스 총량 - (kube-reserved, system-reserved) Eviction 매니저가 시스템 전체가 과부하되지 않도록 관리 Allocatable, system-reserved, kube-reserved 실제 사용되는 리소스 합계가 Eviction Threshold 넘지 않는지 정기적으로 확인, 초과한 경우 파드 Evict Eviction Threshold는 soft, hard 두 가지 존재 soft: SIGTERM 신호를 보내 지정한 시간 후 파드 정지 hard: SIGKILL 신호를 보내 바로 파드 정지 Evict 우선 순위 Requests에 할당된 양보다 초과하여 리소스를 소비하고 있는 것 PodPriority가 낮은 것 Requests에 할당된 양보다 초과하여 소비하고 있는 리소스 양이 더 많은 것 GPU 등의 리소스 제한 엔비디아 GPU 1 2 3 4 5 resources: requests: nvidia.com/gpu: 2 limits: nvidia.com/gpu: 2 오버커밋과 리소스 부족 스케일 아웃을 해도 리소스가 없으면 추가되는 파드 중 리소스를 할당할 수 없는 파드는 Pending 상태가 됨 생성된 파드에 부하가 증가시 리소스 사용량이 100%를 초과하더라도 오버커밋하여 실행 Cluster Autoscaler와 리소스 부족 Cluster Autoscaler는 수요에 따라 노드를 자동으로 추가하는 기능 pending 상태의 파드 발생시 처음으로 Cluster Autoscaler 동작 기본적으로 리소스에 의한 스케줄링은 Requests(최소) 기준으로 동작 Requests와 Limits에 너무 큰 차이를 주지 않을 것 Requests를 너무 크게 설정하지 않을 것 실제 값을 정할 때 Requests, Limits를 낮게 설정하고 성능 테스트를 통해 올려가는 것이 좋음 메모리의 경우 OOM이 발생하지 않을 정도의 리소스 할당 LimitRange를 사용한 리소스 제한 파드, 컨테이너, 영구볼륨 대해 리소스의 최솟값과 최댓값, 기본값 등을 설정 가능 신규 파드가 생성될 때 사용, 기존 파드에는 영향 X 설정 가능 항목 설정 항목 개요 default 기본 Limits defaultRequest 기본 Requests max 최대 리소스 min 최소 리소스 maxLimitRequestRatio Limits/Requests의 비율 LimitRange를 설정할 수 있는 리소스와 설정 항목 타입 사용 가능한 설정 항목 컨테이너 default/defaultRequest/max/min/maxLimitRequetsRatio 파드 max/min/maxLimitRequestRatio PVC max/min 컨테이너에 대한 LimitRange type: Container의 LimitRange로 설정 파드에 대한 LimitRange type: Pod의 LimitRange로 설정 컨테이너에서 사용하는 리소스 합계로 최대/최소 리소스 제한 PVC에 대한 LimitRange type: PersistentVolumeClaim의 LimitRange로 설정 일정 용량 이상으로 볼륨을 생성하지 못하게 할 수 있음 QoS Class 사용자가 직접 설정하지 않고 파드의 Requests/Limits 설정에 따라 자동으로 설정 Qos Class 조건 우선순위 BestEffort Requests/Limits 모두 미지정 3 Guaranteed Requests/Limits 같고 CPU, 메모리 모두 지정 1 Burstable Guranteed 충족하지 못하고 한 개 이상의 Requests/Limits 설정 2 쿠버네티스가 컨테이너에 oom score 설정할 때 사용 oom score: -1000(최고 순위) ~ 1000(최저 순위) Guaranteed의 경우 쿠버네티스 시스템 구성 요소(oom score=-999) 외에 우선순위가 높은 컨테이너가 없어 좀 더 안정적 실행 가능 QoS Class 조건 BestEffort 1000 Guaranteed -998 Burstable min(max(2, 1000 - (1000 * 메모리의 Requests) / 머신 메모리 용량), 999) 1 2 # 파드 목록과 QoS Class 표시 $ kubectl get pods -o custom-columns=\u0026#34;NAME:{.metadata.name},QOS Class:{.status.qosClass}\u0026#34; BestEffort 리소스 제한이 전혀 없음 LimitRange가 설정된 환경에서는 지정되지 않은 경우에도 자동으로 리소스 제한이 설정되므로 되지 않음 Guaranteed 최소한으로 사용하는 리소스와 최대한으로 사용하는 리소스에 차이가 없는 상태 모든 파드를 Guaranteed로 한다면, 부하 증가에 따른 다른 파드로의 영향을 피할 수 있지만, 집약률 낮아짐 Burstable 특정 리소스만 제한 설정 Requests보다 Limits가 큰 경우 최악의 경우 노드가 과부하를 받을 가능성 리소스 쿼터를 사용한 네임스페이스 리소스 쿼터 제한 각 네임스페이스마다 사용 가능한 리소스를 제한 이미 생성된 리소스에는 영향 X 생성 가능한 리소스 수 제한과 리소스 사용량 제한으로 나눌 수 있음 리소스 쿼터가 설정된 경우 제한된 항목 설정은 필수 생성 가능한 리소스 수 제한 count/RESOURCE.GROUP 구문을 사용 리소스 사용량 제한 CPU/메모리에 대해 컨테이너에 할당 가능한 리소스 양 제한 스토리지는 Limits는 존재하지 않고 Requests만 지정 가능 스토리지클래스마다 제한을 둘 수 있음(SSD, HDD, \u0026hellip;) HorizontalPodAutoscaler 디플로이먼트/레플리카셋/레플리케이션 컨트롤러의 레플리카 수를 CPU 부하 등에 따라 자동으로 스케일링하는 리소스 부하가 높아지면 스케일 아웃, 낮아지면 스케일 인 파드에 Resource Requests가 설정되어 있지 않은 경우 동작하지 않음 30초에 한번씩 오토 스케일링 여부 확인 필요한 레플리카 수 = ceil(sum(파드의 현재 CPU 사용률) / targetAverageUtilization) CPU 사용률은 metrics-server에서 가져온 각 파드의 1분간 평균값 사용 최대 3분에 1회 스케일 아웃, 최대 5분에 1회 스케일 인 실행 스케일 아웃 조건 식: avg(파드의 현재 CPU 사용률) / targetAverageUtilization \u0026gt; 1.1 스케일 인 조건 식: avg(파드의 현재 CPU 사용률) / targetAverageUtilization \u0026lt; 0.9 1 2 # CLI로 HPA 생성 $ kubectl autoscale deployment sample-deployment --cpu-percent=50 --min=1 --max=10 CPU 이외의 리소스를 사용하여 오토 스케일링 하는 경우 프로메테우스나 그 외의 메트릭 서버와 연계하기 위한 설정 별도 필요 VerticalPodAutoscaler 파드에 할당하는 CPU/메모리의 Requests는 실제 성능을 측정하려면 서비스 환경에 배포해야 하므로 조절이 어려움 VPA는 컨테이너에 할당하는 리소스를 자동으로 스케일해주는 기능 스케일 아웃이 아닌 스케일 업 대상이 되는 워크로드 리소스, 파드 내부 컨테이너 제한, 업데이트 정책 세 부분으로 구성 Requests를 변경하려면 파드 재기동 필요 - 성능에 악영향 가능성 추천값 계산만 하고 참고로만 확인할 수 있는 옵션도 존재 쿠버네티스의 핵심 기능이 아니기 때문에 별도 구성 요소 설치해야 함 updateMode 내용 Off Requests의 추천값을 계산만 하고 실제 변경은 없음 Initial 파드가 재생성된 시점에만 추천값을 Requests로 변경 Recreate 추천값이 변경될 때 파드가 재생성되고 Requests를 변경 Inplace(미구현) 추천값이 변경될 때 파드를 기동한 상태로 Requests 변경 Auto 추천값이 변경될 때 Inplace 또는 Recreate로 Requests 변경 1 2 # vpa 상태 확인 $ kubectl describe vpa sample-vpa 모드 내용 Lower Bound Requests의 최소 추천값, 밑도는 경우 성능에 큰 영향 Upper Bound Requests의 최대 추천값, 웃도는 경우 낭비 Target Requests의 추천값, 가장 효율적 Uncapped Target 리소스 제약을 고려하지 않은 Requests 추천 값 Requests 변경으로 파드 재작성이 빈번하게 실행되는 것을 막기 위해 Target이 변경된 후 Requests를 Lower Bound보다 작거나 Upper Bound 보다 크게 설정하면 파드 변경됨 PodDisruptionBudget을 고려해 점차적으로 파드 변경 설정되지 않은 경우 절반씩 변경 ","date":"2022-06-18T17:12:04+09:00","image":"https://Haebuk.github.io/images/Kubernetes-Logo.wine.png","permalink":"https://Haebuk.github.io/p/%EB%A6%AC%EC%86%8C%EC%8A%A4%EA%B4%80%EB%A6%AC%EC%99%80-%EC%98%A4%ED%86%A0%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81/","title":"리소스관리와 오토스케일링"},{"content":"컨피그 \u0026amp; 스토리지 API 카테고리 컨테이너 설정 파일, 패스워드 같은 기밀 정보 추가 영구 볼륨 제공 시크릿 컨피그맵 영구 볼륨 클레임 환경 변수 사용 개별 컨테이너 설정 내용은 환경 변수나 파일이 저장되어 있는 영역을 마운트하여 전달하는 것이 일반적 파드 템플릿에 env 또는 envFrom 지정 다음과 같은 정보를 환경 변수에 포함 가능 정적 설정 파드 정보 컨테이너 정보 시크릿 리소스 기밀 정보 컨피그맵 리소스 설정값 정적 설정 spec.containers[].env에 정적인 값 설정 1 2 # sample-env 파드의 환경변수 확인 $ kubectl exec -it sample-env -- env | grep MAX_CONNECTION 컨테이너 기본 타임존: UTC -\u0026gt; 환경 변수 지정하여 변경 가능 1 2 3 4 # 타임존 설정 env: - name: TZ value: Asia/Seoul 파드 정보 fieldRef를 통해 참조 가능 1 2 3 4 5 # 파드가 기동 중인 노드 확인 $ kubectl get pods -o wide sample-env-pod # sample-env-pod 파드 환경 변수 \u0026#39;K8S_NODE\u0026#39; 확인 $ kubectl exec -it sample-env-pod -- env | grep K8S_NODE 컨테이너 정보 resourceFieldRef를 통해 참조 가능 1 $ kubectl exec -it sample-env-container -- env | grep CPU 환경 변수 사용시 주의 사항 command나 args로 실행할 명령어를 지정할 때는 ${}가 아닌 $()로 지정 매니페스트 내부에 정의된 환경 변수만 참조 가능 시크릿 범용 시크릿(Opaque) 스키마리스 시크릿 kubectl로 파일에서 값을 참조하여 생성(\u0026ndash;from-file) kubectl로 envfile에서 값을 참조하여 생성(\u0026ndash;from-env-file) kubectl로 직접 값을 전달하여 생성(\u0026ndash;from-literal) 매니페스트에서 생성(-f) 하나의 시크릿당 저장 가능한 데이터 사이즈는 총 1MB kubectl로 파일에서 값을 참조하여 생성(\u0026ndash;from-file) 일반적으로 파일명이 그대로 키가 되므로 확장자는 붙이지 않는 것이 좋음 파일 생성시 개행 코드 없도록 주의 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 시크릿에 포함된 값을 파일로 내보내기 $ echo -n \u0026#34;root\u0026#34; \u0026gt; ./username $ echo -n \u0026#34;rootpassword\u0026#34; \u0026gt; ./password # 파일에서 값 참조하여 시크릿 생성 $ kubectl create secret generic --save-config sample-db-auth \\ --from-file=./username --from-file=./password # 시크릿 확인 $ kubectl get secrets sample-db-auth -o json | jq .data # base64 인코딩되어 있음 $ kubectl get secrets sample-db-auth -o json | jq -r .data.username # base64 디코드 $ kubectl get secrets sample-db-auth -o json | jq -r .data.username | base64 --decode kubectl로 envfile에서 값을 참조하여 생성(\u0026ndash;from-env-file) 하나의 파일에서 일괄적으로 생성하는 경우 1 2 $ kubectl create secret generic --save-config sample-db-auth \\ --from-env-file ./env-secret.txt kubectl로 값을 직접 전달하여 생성(\u0026ndash;from-literal) 1 2 $ kubectl create secret generic --save-config sample-db-auth \\ --from-literal=username=root --from-literal=password=rootpassword 매니페스트에서 생성(-f) base64로 제대로 인코드되었는지 확인 data가 아닌 stringData 필드를 사용하면 일반 텍스트로 작성 가능 TLS 타입 시크릿 인증서로 사용할 시크릿을 사용하는 경우 인그레스 리소스 등에서 사용하는 것이 일반적 매니페스트로 생성할 수 있지만 기본적으로 비밀키와 인증서 파일로 생성하는 것이 좋음 도커 레지스트리 타입 시크릿 컨테이너 레지스트리가 프라이빗 저장소인 경우에 인증 정보를 시크릿으로 정의하여 사용 가능 kubectl로 직접 생성하는 것이 편리 ~/.docker/config.json 파일 대체용으로 사용 1 2 3 4 5 6 7 8 9 10 11 12 # 도커 레지스트리 인증 정보의 시크릿 생성 $ kubectl create secret docker-registry --save-config sample-registry-auth \\ --docker-server=REGISTRY_SERVER \\ --docker-username=REGISTRY_USER \\ --docker-password=REGISTRY_USER_PASSWORD \\ --docker-email=REGISTRY_USER_EMAIL # base64로 인코드된 dockercfg 형식의 JSON 데이터 $ kubectl get secrets -o json sample-registry-auth | jq .data # base64로 디코드한 dockercfg 형식의 JSON 데이터 $ kubectl get secrets sample-registry-auth -o yaml | grep \u0026#34;\\.dockerconfigjson\u0026#34; | awk -F \u0026#39; \u0026#39; \u0026#39;{print $2}\u0026#39; | base64 --decode 이미지 다운로드 시 시크릿 사용 인증이 필요한 도커 레지스트리의 프라이빗 저장소에 저장된 이미지를 다운로드할 때, 시크릿을 사전에 생성한 후 파드 정의 spec.imagePullSecrets에 docker-registry 타입의 시크릿 지정 imagePullSecrets는 복수 설정 가능 기본 인증 타입의 시크릿 사용자명과 패스워드로 인증하는 시스템을 사용하는 경우 kubectl로 직접 값을 전달하여 생성(\u0026ndash;from-literal) 리소스를 생성하지 않고 매니페스트를 출력하는 경우 \u0026ndash;dry-run, -o yaml 옵션 사용 1 2 3 4 # 직접 옵션에서 type과 값을 지정하여 시크릿 생성 $ kubectl create secret generic --save-config sample-basic-auth \\ --type kubernetes.io/basic-auth \\ --from-literal=username=root --from-literal=password=rootpassword 매니페스트에서 생성(-f) type에 kubernetes.io/basic-auth 지정 데이터 스키마로 username과 password 지정 SSH 인증 타입의 시크릿 비밀키로 인증하는 시스템을 사용하는 경우 kubectl로 파일에서 값을 참조하여 생성(\u0026ndash;from-file) 1 2 3 4 5 6 7 # SSH 비밀키 생성 $ ssh-keygen -t rsa -b 2048 -f sample-key -C \u0026#34;sample\u0026#34; # 파일에서 type과 값을 참조하여 시크릿 생성 $ kubectl create secret generic --save-config sample-ssh-auth \\ --type kubernetes.io/ssh-auth \\ --from-file=ssh-privatekey=./sample-key 매니페스트에서 생성(-f) type에 kubernetes.io/ssh-auth 지정 데이터 스키마로 ssh-privatekey 지정 시크릿 사용 컨테이너에서 사용할 경우 두 가지 패턴이 존재 환경 변수로 전달 시크릿의 특정 키만 시크릿의 전체 키 볼륨으로 마운트 시크릿의 특정 키만 시크릿의 전체 키 환경 변수로 전달 특정 키를 전달할 경우 spec.containers[].env의 valueFrom.secretKeyRef 사용 env로 하나씩 정의하기 때문에 환경 변수명 지정 가능 전체를 전달할 경우 매니페스트가 길어지지는 않지만 시크릿에 어떤 값이 있는지 매니페스트 정의에서 알기 힘듦 여러 시크릿을 가져오면 키가 충돌할 수 있으므로 접두사를 붙여 충돌 방지 1 2 3 4 5 6 7 8 # sample-secret-single-env 파드의 DB_USERNAME 확인 $ kubectl exec -it sample-secret-single-env -- env | grep DB_USERNAME # sample-secret-multi-env 파드의 환경 변수 확인 $ kubectl exec -it sample-secret-multi-env -- env # sample-secret-prefix-env 파드의 접두사가 DB인 환경 변수 확안 $ kubectl exec -it sample-secret-prefix-env -- env | egrep ^DB 볼륨으로 마운트 특정 키를 마운트하는 경우 spec.volumes[]의 secret.items[]를 사용 마찬가지로 시크릿 전체를 마운트 가능, 매니페스트가 길어지지 않지만 어떤 값이 있는지 알기 힘듦 1 2 3 4 5 # sample-secret-single-volume 파드의 /config/username.txt 확인 $ kubectl exec -it sample-secret-single-volume -- cat /config/username.txt # 파드 내부의 /config 디렉터리 내용 확인 $ kubectl exec -it sample-secret-multi-volume -- ls /config 동적 시크릿 업데이트 일정 기간마다(kubelet의 Sync Loop 타이밍) kube-apiserver로 변경 확인 변경이 있을경우 파일 교체(기본 60초) 주기를 조정하려면 kubelet의 \u0026ndash;sync-frequency 옵션 지정 환경 변수를 사용한 시크릿의 경우 파드를 기동할 때 환경 변수가 정해지므로 동적 변경 불가 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # 시크릿에 마운트된 디렉터리 확인 $ kubectl exec -it sample-secret-multi-volume -- ls -la /config drwxrwxrwt 3 root root 120 Jun 11 08:47 . drwxr-xr-x 1 root root 42 Jun 11 08:47 .. drwxr-xr-x 2 root root 80 Jun 11 08:47 ..2022_06_11_08_47_42.732252402 lrwxrwxrwx 1 root root 31 Jun 11 08:47 ..data -\u0026gt; ..2022_06_11_08_47_42.732252402 lrwxrwxrwx 1 root root 15 Jun 11 08:47 password -\u0026gt; ..data/password lrwxrwxrwx 1 root root 15 Jun 11 08:47 username -\u0026gt; ..data/username # 파드의 /config/username 파일 내용 확인 $ kubectl exec -it sample-secret-multi-volume -- cat /config/username # 시크릿 변경 전 경과 시간 확인 $ kubectl get pods sample-secret-multi-volume # 시크릿 내용 업데이트 $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: sample-db-auth type: Opaque data: # root \u0026gt; admin으로 변경 username: YWRtaW4= EOF # 시크릿에 마운트된 디렉터리 확인 $ kubectl exec -it sample-secret-multi-volume -- ls -la /config total 0 drwxrwxrwt 3 root root 100 Jun 11 08:54 . drwxr-xr-x 1 root root 42 Jun 11 08:47 .. drwxr-xr-x 2 root root 60 Jun 11 08:54 ..2022_06_11_08_54_21.841196102 lrwxrwxrwx 1 root root 31 Jun 11 08:54 ..data -\u0026gt; ..2022_06_11_08_54_21.841196102 lrwxrwxrwx 1 root root 15 Jun 11 08:47 username -\u0026gt; ..data/username # root에서 admin으로 변경됨 $ kubectl exec -it sample-secret-multi-volume -- cat /config/username # 동적으로 파일이 변경된 후의 경과시간 확인 -\u0026gt; 파드가 재생성되지 않음 $ kubectl get pods sample-secret-multi-volume 시크릿 내용을 username으로만 하고 kubectl apply를 실행하여 그 외의 파일(password) 삭제됨 처음 시크릿 생성시 kubectl apply나 kubectl create \u0026ndash;save-config를 사용하지 않은 경우 매니페스트 병합 처리가 불완전하여 결과가 달라짐 1 2 # password 파일은 삭제됨 $ kubectl exec -it sample-secret-multi-volume -- ls /config 컨피그맵 설정 정보 등을 key-value로 저장할 수 있는 데이터 저장 리소스 하나의 컨피그맵마다 저장할 수 있는 사이즈 총 1MB Generic 타입의 시크릿과 거의 동일한 방법으로 생성 kubectl로 파일에서 값을 직접 참조 kubectl로 직접 값을 전달 매니페스트로 생성 매니페스트로 생성시 시크릿과 다르게 base64로 인코드되지 않고 추가됨 value를 여러 행으로 전달할 경우 YAML 문법에 맞게 Key: |등과 같이 다음 행부터 정의 숫자는 큰 따옴표로 둘러싸기 1 2 3 4 5 6 7 8 # 파일로 컨피그맵 생성 $ kubectl create configmap --save-config sample-configmap --from-file=./nginx.conf # 컨피그맵에 등록된 데이터 확인 1 $ kubectl get configmaps sample-configmap -o json | jq .data # 컨피그맵에 등록된 데이터 확인 2 $ kubectl describe configmap sample-configmap binaryData 필드를 사용하여 UTF-8이외의 데이터를 포함하는 바이너리 데이터도 저장 가능(시크릿도 동일) 매니페스트 파일로 저장하려면 \u0026ndash;dry-run=client -o yaml 옵션 사용 1 2 3 4 5 6 7 8 9 10 11 $ kubectl create configmap sample-configmap-binary \\ --from-file image.jpg \\ --from-literal=index.html=\u0026#34;Hello, Kubernetes\u0026#34; \\ --dry-run=client -o yaml \\ \u0026gt; sample-configmap-binary.yaml # 로컬 8080 포트에서 파드의 80 포트로 포트 포워딩 $ kubectl port-forward sample-configmap-binary-webserver 8080:80 # 브라우저로 표시 $ open http://localhost:8080/image.jpg 1 2 3 # 인수에 각 직접 전달 $ kubectl create configmap --save-config web-config \\ --from-literal=connection.max=100 --from-literal=connection.min=10 컨피그맵 사용 두 가지 방법 환경 변수로 전달 특정 키만 spec.containers[].env의 valueFrom.configMapKeyRef 사용 전체 키 변수 명에 \u0026lsquo;.\u0026rsquo;, \u0026lsquo;-\u0026rsquo; 사용하지 않는 것이 좋음 여러 행의 경우 볼륨으로 마운트하는 것을 권장 볼륨으로 마운트 특정 키만 spec.volumes[]의 configMap.items[] 사용 전체 키 1 2 3 4 5 6 7 8 9 10 11 # 파드의 CONNECTION_MAX 환경 변수 내용 확인 $ kubectl exec -it sample-configmap-single-env -- env | grep CONNECTION_MAX # 파드의 여러 환경 변수 확인 $ kubectl exec -it sample-configmap-multi-env -- env # 파일로 저장된 컨피그맵 확인 $ kubectl exec -it sample-configmap-single-volume -- cat /config/nginx-sample.conf # 파드에 마운트된 /config 아래 파일 확인 $ kubectl exec -it sample-configmap-multi-volume -- ls /config 시크릿과 컨피그맵의 공통 주제 사용 구분 시크릿 데이터는 etcd에 저장됨 쿠버네티스 노드상에 영구적으로 데이터가 남지 않게 tmpfs 영역에 저장 base64로 인코드되어 있어 화면에서 판단하기 어려우나 단순 base64 인코드 이므로 깃 저장소 업로드는 금지 시크릿을 암호화하는 OSS나 Vault와 같은 서드 파티 솔류션 사용 마운트 시 퍼미션 변경 파드에서 실행하는 경우 볼륨을 생성할 때 실행 권한 부여 가능 기본값 0644(rw-r\u0026ndash;r\u0026ndash;)로 마운트 퍼미션은 8진수 표기에서 10진수 표기로 변환한 형태를 사용 동적 컨피그맵 업데이트 볼륨 마운트 사용시 일정 기간 마다 파일 교체 (기본값 60초) 환경 변수를 사용한 컨피그맵은 동적 업데이트 불가 데이터 변경 거부 immutable 설정 변경하면 데이터 변경 방지 가능 변경하려면 리소스를 삭제하고 다시 생성 볼륨 마운트의 경우 파드 재생성 필요 볼륨, 영구 볼륨, 영구 볼륨 클레임의 차이 볼륨 미리 준비된 사용 가능한 볼륨을 매니페스트에 직접 지정하여 사용 사용자가 설정된 볼륨을 사용할 수 있지만 신규 볼륨 생성 또는 기존 볼륨 삭제 불가 매니페스트에서 볼륨 리소스 생성 불가 영구 볼륨 외부 영구 볼륨을 제공하는 시스템과 연계하여 신규 볼륨 생성 또는 기존 볼륨 삭제 가능 매니페스트에서 영구 볼륨 리소스를 별도로 생성하는 형태 영구 볼륨 클레임 영구 볼륨 리소스를 할당하는 리소스 영구 볼륨은 클러스터에 볼륨을 등록만 함 -\u0026gt; 실제 사용하려면 영구 볼륨 클레임 정의 동적 프로비저닝 기능 사용시 영구 볼륨 클레임이 사용된 시점에 영구 볼륨 동적으로 생성 가능 볼륨 추상화하여 파드와 느슨하게 결합된 리소스 emptyDir hostPath downwardAPI projected nfs iscsi cephfs 파드에 정적으로 볼륨을 지정 -\u0026gt; 플러그인에 따라 충돌 가능성 emptyDir 파드용 임시 디스크 영역으로 사용 가능 파드 종료시 삭제 호스트의 임의 영역 마운트 불가 호스트의 파일 참조 불가 1 2 # 기동 중인 쿠버네티스 노드의 디스크 영역 할당 확인 $ kubectl exec -it sample-emptydir -- df -h | grep /cache emptyDir.sizeLimit으로 리소스 제한 가능 용량 초과시 Evict(축출) 1 2 3 4 5 # 150MB 파일을 /cache/dummy에 생성 $ kubectl exec -it sample-emptydir-limit -- dd if=/dev/zero of=/cache/dummy bs=1M count=150 # 파드 상태 모니터링 $ kubectl describe pods sample-emptydir-limit 고속 tmpfs 메모리 영역 사용 가능 emptyDir.medium에 Memory 지정 컨테이너에 대한 메모리 사용 상한 설정에도 영향을 줌 1 2 # tmpfs 영역 할당 확인 $ kubectl exec -it sample-emptydir-memory -- df -h | grep /cache hostPath 쿠버네티스 노드상의 영역을 컨테이너에 매핑하는 플러그인 호스트의 임의 영역 마운트 가능 어떤 영역 사용할지 지정 type: Directory/DirectoryOrCreate/File/Socket/BlockDevice 등 Directory/DirectoryOrCreate 차이: 디렉터리가 존재하지 않을 때 생성 후 기동 유무 보안상의 이유로 안전하지 않은 컨테이너가 업로드될 수 있으므로 사용하지 않는 것이 좋음 1 2 3 4 5 # 호스트 OS 이미지 확인 $ kubectl exec -it sample-hostpath -- cat /srv/os-release | grep PRETTY_NAME # 컨테이너 OS 이미지 확인 $ kubectl exec -it sample-hostpath -- cat /etc/os-release | grep PRETTY_NAME downwardAPI 파드 정보 등을 파일로 배치하기 위한 플러그인 환경 변수 fieldRef와 ResourceFieldRef 사용 방법과 동일 1 2 # 파드 정보등이 파일로 배치 $ kubectl exec -it sample-downward-api -- ls /srv projectd 시크릿/컨피그맵/downwardAPI/serviceAccountToken의 볼륨 마운트롤 하나의 디렉터리에 통합하는 플러그인 1 2 3 4 5 6 7 8 # /srv 디렉터리 확인 $ kubectl exec -it sample-projectd -- ls /srv # /srv/configmap 디렉터리 확인 $ kubectl exec -it sample-projectd -- ls /srv/configmap # /srv/secret 디렉터리 확인 $ kubectl exec -it sample-projectd -- ls /srv/secret 영구볼륨(PV) 기본적으로 네트워크를 통해 디스크를 attach하는 디스크 타입 개별 리소스로 생성 후 사용 pluggable한 구조로 되어 있음 생성 레이블 동적 프로비저닝을 사용하지 않고 영구 볼륨을 생성하는 경우 영구 볼륨 종류를 알 수 없으므로, 레이블을 사용하는 것이 좋음 용량 동적 브로비저닝을 사용할 수 없는 상황에서는 작은 용량의 영구 볼륨도 준비해야함 (가장 비슷한 용량이 할당되므로) 접근 모드 ReadWriteOnce: 단일 노드에서 Read/Write 가능 ReadOnlyMany: 여러 노드에서 Read 가능 하나라도 쓰기 요청이 있는 파드가 있으면 다른 노드에서 마운트 불가능 파드에서 영구 볼륨 지정할 때 ReadOnly 지정 ReadWriteMany: 여러 노드에서 Read/Write 가능 Reclaim Policy 영구 볼륨 사용 후 처리 방법을 제어하는 정책 영구 볼륨 클레임에서 사용된 후 영구 볼륨 클레임이 삭제되었을 때 영구 볼륨 자체의 동작 설정 세가지 방법 존재 Delete: 영구 볼륨 자체가 삭제 GCP/AWS 등에서 확보되는 외부 볼륨의 동적 프로비저닝 때 사용되는 경우가 많음 Retain: 영구 볼륨 삭제하지 않고 유지 또다른 PVC에 의해 다시 마운트 되지는 않음 Recycle: 영구 볼륨 데이터 삭제 후 재사용 가능 상태로 만듦 다른 영구 볼륨 클레임에서 마운트 가능 동적 프로비저닝을 사용하는 것이 좋음 스토리지클래스 동적으로 영구 볼륨을 프로비저닝하는 구조 영구 볼륨 클레임(PVC) 영구 볼륨을 요청하는 리소스 PVC에서 지정된 조건(용량, 레이블)을 기반으로 PV에 대한 요청이 들어오면 스케줄러는 현재 가지고 있는 PV에서 적당한 볼륨을 할당 설정 다음과 같은 항목 설정 가능 레이블 셀렉터 용량 접근 모드 스토리지클래스 PVC 용량이 PV 보다 작아야 할당(PV보다 큰 용량이 할당됨) 파드에서 사용하려면 spec.volumes에 persistentVolumeClaim.claimName 지정 동적 프로비저닝 PVC가 생성되는 타이밍에 동적으로 영구 볼륨 생성 사전에 영구 볼륨을 생성할 필요 없으며, 용량 낭비 발생하지 않음 사전에 어떤 PV를 생성할지 정의한 스토리지클래스 생성 영구 볼륨 할당 타이밍 제어 동적 프로닝 사용 -\u0026gt; PVC 생성시 파드에 PVC가 붙어있지 않아도 PV가 생성됨 실제 파드에 붙기전에 PV가 생성되고 연결할 수 있음 volumeBindingMode 설정값 설정값 개요 Immediate(기본값) 즉시 PV가 생성되고 연결할 수 있게 됨 WaitForFirstConsumer 처음으로 파드에 사용될 때 PV가 생성되고 연결할 수 있게 됨 PVC 조정을 사용한 볼륨 확장 동적 프로비저닝을 사용하고 크기 조정을 지원하는 볼륨 플러그인을 사용할 땐 PVC 확장 가능 사전에 스토리지클래스에 allowVolumeExpantion:true 설정 축소는 불가 1 2 3 4 5 # 마운트된 PV 크기 확인 $ kubectl exec -it sample-pvc-resize-pod --df -h | grep /usr/share/nginx/html # PVC에서 요청하는 용량 변경 $ kubectl patch pvc sample-pvc-resize --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;resources\u0026#34;: {\u0026#34;requests\u0026#34;: {\u0026#34;storage\u0026#34;: \u0026#34;16Gi\u0026#34;}}}}\u0026#39; 스테이트풀셋에서 PVC spec.volumeClaimTemplate 항목을 사용하면 PVC를 별도 정의하지 않아도 자동으로 생성 가능 volumeMounts에서 사용 가능한 옵션 읽기 전용(ReadOnly) 마운트 여러 볼륨을 컨테이너에 마운트할 때 readonly 옵션 지정 가능 hostPath는 컨테이너에 호스트 영역을 보여주므로 보안상 좋지 않음 최소한 ReadOnly로 마운트하자 subPath 볼륨 마운트시 특정 디렉터리를 루트로 마운트하는 기능 각 컨테이너가 하나의 볼륨을 사용하면서도 서로 영향이 없도록 디렉터리 나눌 수 있음 1 2 3 4 5 # subPath /path1을 지정한 컨테이너 $ kubectl exec -it sample-subpath -c container-b -- find /data # subPath를 지정하지 않은 컨테이너 $ kubectl exec -it sample-subpath -c container-a -- find /data ","date":"2022-06-16T17:11:40+09:00","image":"https://Haebuk.github.io/p/%EC%BB%A8%ED%94%BC%EA%B7%B8%EC%99%80%EC%8A%A4%ED%86%A0%EB%A6%AC%EC%A7%80-api/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EC%BB%A8%ED%94%BC%EA%B7%B8%EC%99%80%EC%8A%A4%ED%86%A0%EB%A6%AC%EC%A7%80-api/","title":"컨피그와스토리지 API"},{"content":"서비스 API 카테고리 클러스터 컨테이너에 대한 엔드포인트를 제공하거나 레이블과 일치하는 컨테이너의 디스커버리에 사용되는 리소스\n서비스 ClusterIP ExternalIP(ClusterIP의 한 종류) NodePort LoadBalancer Headless(None) ExternalName Node-Selector 인그레스 파드는 서비스를 사용하지 않고도 파드간 통신이 가능하나, 서비스를 사용하면 두 가지 큰 장점이 있음\n파드에 트래픽 로드 밸런싱 서비스 디스커버리와 클러스터 내부 DNS 파드에 트래픽 로드 밸런싱 수신한 트래픽을 여러 파드에 로드 밸런싱 ClusterIP 클러스터 내부에서만 사용 가능한 가상 IP를 가진 엔드포인트 제공하는 로드 밸런서 구성 spec.selector에 정의한 조건에 따라 트래픽 전송 1 2 3 4 5 6 7 8 # 서비스 생성 $ kubectl apply -f sample-clusterip.yaml # 지정한 레이블을 가진 파드 중 특정 JSON Path를 컬럼으로 출력 $ kubectl get pods -l app=sample-app -o custom-columns=\u0026#34;NAME:{metadata.name},IP:{status.podIP}\u0026#34; # 서비스 상세 정보 확인 $ kubectl describe service sample-clusterip Endpoints에 app=sample-app 라벨을 가진 파드의 IP 정보가 있음 Endpoints 항목에 아무것도 없을 경우 셀렉터 조건이 맞지 않을 가능성 있음 1 2 3 4 5 # 일시적으로 파드를 시작하여 서비스 엔드포인트로 요청 ## (여러 번 실행 시 비슷한 빈도로 세 개의 파드명 표시) $ kubectl run --image=amsy810/tools:v2.0 --restart=Never --rm -i testpod --command -- curl -s http://{ClusterIP}:8080 Host=10.100.50.111 Path=/ From=sample-deployment-687d589688-8nbcs ClientIP=172.31.9.8 XFF= pod \u0026#34;testpod\u0026#34; deleted 여러 포트 할당 하나의 서비스에 여러 포트 할당 가능 (바람직) 이름을 사용한 포트 참조 파드의 포트 정의에 이름을 지정하면 이름을 사용하여 참조 가능 1 2 3 4 5 # 서비스 목적지 엔드포인트 확인 $ kubectl describe service sample-named-port-service # 파드 IP 주소 확인 $ kubectl get pods -o wide 클러스터 내부 DNS와 서비스 디스커버리 서비스에 속한 파드를 보여주거나 서비스명에서 엔드포인트를 반환하는 것 서비스 디스커버리 방법 환경 변수 사용 DNS A 레코드 사용 DNS SRV 레코드 사용 환경 변수를 사용한 서비스 디스커버리 파드 내부에서는 환경 변수에서도 같은 네임스페이스 서비스 확인 가능 -이 포함된 서비스명은 _로 변경 후 대문자 변환됨 파드 생성 후 서비스 생성 또는 삭제에 따라 변경된 환경 변수가 기존 파드에 자동 등록되지 않음 먼저 생성한 파드 재생성 필요 1 2 # 환경 변수에 등록된 서비스 정보 확인 $ kubectl exec -it sample-deployment-{}-{} -- env | grep -i kubernetes spec.enableServiceLinks를 false로 지정시 환경 변수 추가 비활성화 (기본값 true) DNS A 레코드를 사용한 서비스 디스커버리 IP 주소를 편하게 관리하기 위해 기본적으로 자동 할당된 IP 주소에 연결된 DNS 명을 사용하는 것이 좋음 서비스명의 이름 해석이 수행되고 해당 ip로 요청이 발송 다른 네임스페이스의 경우 sample-cluster.default와 같이 네임스페이스명을 붙여 이름 해석해야 함 1 2 3 # 일시적으로 파드 기동하여 컨테이너 내부에서 sample-clusterip:8080으로 HTTP 요청 $ kubectl run --image=amsy810/tools:v2.0 --restart=Never --rm -i testpod \\ --command -- curl -s http://sample-clusterip:8080 DNS SRV 레코드를 사용한 서비스 디스커버리 포트명과 프로토콜을 사용해 서비스를 제공하는 포트 번호를 포함한 엔드포인트를 DNS로 해석 _서비스 포트명._프로토콜.서비스명.네임스페이스명.svc.cluster.local 1 2 3 4 5 6 7 8 9 10 11 # 일시적으로 파드 기동하여 SRV 레코드가 다른 파드에서 해석이 가능한지 확인 $ kubectl run --image=amsy810/tools:v2.0 --restart=Never --rm -i testpod \\ --command -- dig _http-port._tcp.sample-clusterip.default.svc.cluster.local SRV ;; QUESTION SECTION: ;_http-port._tcp.sample-clusterip.default.svc.cluster.local. IN SRV ;; ANSWER SECTION: _http-port._tcp.sample-clusterip.default.svc.cluster.local. 5 IN SRV 0 100 8080 sample-clusterip.default.svc.cluster.local. ;; ADDITIONAL SECTION: sample-clusterip.default.svc.cluster.local. 5 IN A 10.100.50.111 목적지 호스트명 sample-clusterip.default.svc.cluster.local과 포트번호 8080 해석 가능 클러스터 내부 DNS와 클러스터 외부 DNS 파드의 DNS 서버 설정을 명시적으로 하지 않으면 클러스터 내부 DNS 사용하여 이름 해석 수행 내부 이외의 레코드는 외부 DNS에 재귀 질의 해야함 노드 로컬 DNS 캐시 대규모 클러스터에서 성능 향상을 위해 각 노드의 로컬에 DNS 캐시 서버를 포함하는 구조도 존재 활성화한 환경에서 파드의 질의 대상은 같은 노드에 있는 로컬 DNS 캐시 서버 ClusterIP 서비스 클러스터 내부에서만 통신 가능한 가상 IP 할당 클러스터 외부에서 트래픽을 수신할 필요가 없는 환경에서 내부 로드 밸런서로 활용 ExternalIP 서비스 지정한 쿠버네티스 노드 IP 주소:포트에서 수신한 트래픽을 컨테이너로 전달하는 형태로 외부와 통신 특별한 이유가 없다면 NodePort 서비스를 사용하는것이 좋음 type: ExternalIP를 지정하는 것이 아님 (ClusterIP에 해당함) NodePort 서비스 모든 쿠버네티스 노드 IP 주소:포트에서 수신한 트래픽을 컨테이너로 전달하는 형태로 외부와 통신 모든 쿠버네티스 노드의 IP 주소에서 해당 포트를 listen 하기 때문에 충돌 주의 할당된 포트 번호를 지정할 필요가 없을 경우, 포트를 지정하지 않으면 빈 포트 번호가 자동으로 선택됨 설정 항목 개요 spec.ports[].port ClusterIP에서 수신할 포트 번호 spec.ports[].targetPort 목적지 컨테이너 포트 번호 spec.ports[].nodePort 모든 쿠버네티스 노드 IP 주소에서 수신할 포트 번호 NodePort 주의점 가용한 포트 범위는 대부분 30000 ~ 32767 여러 NodePort 서비스에서 같은 포트 사용 불가 LoadBalancer 서비스 클러스터 외부로부터 트래픽을 수신할 때 가장 실용적인 서비스 쿠버네티스 노드와 별도로 외부 로드 밸런스를 사용 -\u0026gt; 노드 장애에도 크게 문제가 되지 않음 (장애 발생한 노드에는 트래픽을 전송하지 않음) 장애 감지 까지 일시적으로 서비스 중단 현상이 발생할 수 있음 생성시 EXTERNAL-IP가 \u0026lt;pending\u0026gt; 상태임 로드밸런서를 백그라운드에서 생성중이기 때문에 아직 미할당된 상태 정적으로 외부 LB에서 사용하는 IP 주소 지정 가능 방화벽을 지정하여 접속 제한 가능 1 2 3 # 외부에서 통신 # 여러번 실행 시 비슷한 빈도로 세 개의 파드명 표시 $ curl -s http://{lb EXTERNAL-IP}:8080 그 외 서비스 세션 어피니티 트래픽을 서비스에 연결된 어느 하나의 파트에 전송되면, 그 파드에 계속 보내고 싶을 때 사용 최대 세션 고정 시간 설정 가능 노드 간 통신 제외와 발신 측 IP 주소 유지 NodePort, LoadBalancer 서비스에서 노드에 도착한 요청은 2단계 로드 밸런싱이 이루어짐(레이턴시 오버헤드 발생) 발신 측 IP 주소가 유실되는 특징 데몬셋은 하나의 노드에 하나의 파드만 배치되므로 같은 노드에만 통신하고 싶은 경우 NodePort 서비스의 spec.externalTrafficPolicy를 local로 설정 해당 노드의 요청은 그 노드상에 있는 파드에만 전송 두 개 이상의 파드 존재시 균등 전송 파드가 없다면 요청에 응답할 수 없으므로 가능하면 사용 X LoadBalancer 서비스의 spec.externalTrafficPolicy를 local로 설정 별도의 헬스 체크용 NodePort가 있어 파드가 존재하지 않는 노드에는 요청이 전송되지 않음 헤드리스 서비스(None) 대상이 되는 개별 파드의 IP 주소가 직접 반환되는 서비스 로드 밸런싱을 위한 IP 주소는 제공되지 않음 DNS 라운드 로빈을 사용한 엔드포인트 제공 스테이트풀셋이 헤드리스 서비스를 사용하는 경우에만 파드명으로 IP주소를 찾을 수 있음 생성 조건 서비스의 spec.type이 ClusterIP일 것 서비스의 spec.ClusterIP가 None일 것 [옵션] 서비스의 metadata.name이 스테이트풀셋의 spec.serviceName과 같을 것 파드명으로 디스커버리하는 경우 ExternalName 서비스 외부도메인으로 CNAME 반환 다른 이름을 설정하고 싶거나, 클러스터 내부에서의 엔드포인트를 쉽게 변경하고 싶을 때 사용 외부 서비스와 느슨한 결합 확보 목적지가 변경되었을 때 ExternalName 서비스를 변경하는 것만으로 가능 ClusterIP 서비스에서 ExternalName 서비스로 전환할 경우 spec.clusterIP를 명시적으로 공란으로 두어야함 None-Selector 서비스 서비스명으로 이름 해석시 자신이 설정한 멤버에 대해 로드 밸런싱 수행 externalName을 지정하지 않고 셀렉터가 존재하지 않는 서비스 생성 + 엔드포인트 리소스 수동 생성하면 유연한 서비스 생성 가능 쿠버네티스 외부에 애플리케이션 서버에 대한 요청을 분산하는 경우에도 쿠버네티스 서비스 사용 가능 서비스 환경과 스테이징 환경에서 클러스터 내외부를 분리해도 애플리케이션에서 같은 ClusterIP로 보여줄 수 있음 인그레스 서비스들을 묶는 서비스들의 상위 객체 리소스와 컨트롤러 매니페스트를 쿠버네티스에 등록하는 것만으로는 아무 처리도 일어나지 않음 실제 처리를 하는 컨트롤러라는 시스템 구성 요소 필요 인그레스 리소스: 매니페스트에 등록된 API 리소스 인그레스 컨트롤러: 인그레스 리소스가 쿠버네티스에 등록되었을 때 어떠한 처리 수행 인그레스 종류 클러스터 외부 로드 밸런서를 사용한 인그레스 GKE 인그레스 클러스터 내부 로드 밸런서를 사용한 인그레스 Nginx 인그레스 클러스터 외부 로드 밸런서를 사용한 인그레스 인그레스 리소스 생성만으로 로드 밸런서의 가상 IP가 할당되어 사용 가능 순서(단계) 클라이언트 -\u0026gt; L7 로드 밸런서(NodePort 경유) -\u0026gt; 목적지 파드 클러스터 내부에 인그레스용 파드를 배포하는 인그레스 인그레스 리소스에서 정의한 L7 수준의 로드 밸런싱 처리를 하기 위해 인그레스용 파드를 클러스터 내부에 생성해야 함 클러스터 외부에서 접속할 수 있도록 별도로 인그레스용 파드에 LoadBalancer 서비스를 생성하는 등의 준비 필요 SSL 터미네이션이나 경로 기반 라우팅 등과 같은 L7 수준의 처리를 위해 부하에 따른 레플리카 수의 오토 스케일링 고려해야 함 순서(단계) 클라이언트 -\u0026gt; L4 로드 밸런서(type: LoadBalancer) -\u0026gt; Nginx 파드(Nginx 인그레스 컨트롤러) -\u0026gt; 목적지 파드 정리 파드 서비스 디스커버리나 L4 로드 밸런싱 기능을 제공하기 위한 서비스 리소스 L7 로드 밸런싱 기능을 제공하는 인그레스 서비스 L4 로드 밸런싱 클러스터 내부 DNS를 사용한 이름 해석 레이블을 사용한 파드의 디스커버리 인그레스 L7 로드 밸런싱 SSL 터미네이션 경로 기반 라우팅 서비스 종류 IP 엔드포인트 내용 ClusterIP 쿠버네티스 클러스터 내부에서만 통신 가능한 가상 IP ExternalIP 특정 쿠버네티스 노드의 IP 주소 NodePort 모든 쿠버네티스 노드의 모든 IP 주소(0.0.0.0) LoadBalancer 클러스터 외부에서 제공되는 로드 밸런서의 가상 IP Headless(None) 파드의 IP 주소를 사용한 DNS 라운드 로빈 ExternalName CNAME을 사용한 느슨한 연결 확보 None-Selector 원하는 목적지 멤버를 설정할 수 있는 다양한 엔드 포인트 인그레스 종류 구현 예제 클러스터 외부 로드 밸런서를 사용한 인그레스 GKE 클러스터 내부에 인그레스용 파드를 배포하는 인그레스 Nginx 인그레스 ","date":"2022-06-11T23:00:00+09:00","image":"https://Haebuk.github.io/p/%EC%84%9C%EB%B9%84%EC%8A%A4-api/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EC%84%9C%EB%B9%84%EC%8A%A4-api/","title":"서비스 API"},{"content":"워크로드 API 카테고리 클러스터에 컨테이너를 기동시키기 위해 사용되는 리소스 Pod Replication Controller(Deprecated) ReplicaSet Deployment DaemonSet StatefulSet Job CronJob 파드 워크로드 리소스의 최소 단위 파드 디자인 패턴 종류 개요 사이드카 패턴 메인 컨테이너에 기능 추가 앰배서더 패턴 외부 시스템과의 통신 중계 어댑터 패턴 외부 접속을 위한 인터페이스 제공 파드 명령어 1 2 # 파드 생성 $ kubectl apply -f sample-pod.yaml 1 2 # 파드 목록 표시 $ kubectl get pods 1 2 # 파드 상세 정보 표시 $ kubectl get pods --output wide 1 2 3 4 5 6 7 8 9 # 컨테이너에서 /bin/bash 실행 $ kubectl exec -it sample-pod -- /bin/bash root@sample-pod:/# (이후 컨테이너 내부에서 명령어 실행 가능) # 컨테이너에서 ls 명령어 실행 $ kubectl exec -it sample-pod -- /bin/ls # 다수의 컨테이너 포함한 파드의 경우 특정 컨테이너 지정 가능 $ kubectl exec -it sample-2pod -c nginx-container -- /bin/ls 파드 주의 사항 파드 내 컨테이너가 같은 포트로 바인드되면 안됨 쿠버네티스는 ENTRYPOINT를 command, CMD를 args라고 부름 파드명 제한 영어 소문자 또는 숫자 기호는 \u0026lsquo;-\u0026rsquo; 또는 \u0026lsquo;.\u0026rsquo; 시작과 끝은 영어 소문자 레플리카셋/레플레케이션 컨트롤러 파드의 레플리카를 생성하고 지정한 파드 수를 유지하는 리소스 노드나 파드에 장애가 발생했을 때도 지정한 파드 수를 유지하기 위해 다른 노드에서 파드를 기동하므로 장애 시에 많은 영향 받지 않음 모니터링은 특정 레이블을 가진 파드 수를 계산하는 형태로 이루어짐 레플리카셋 명령어 1 2 3 4 5 # 레플리카셋 생성 $ kubectl apply -f sample-rs.yaml # 레플리카셋 확인 $ kubectl get replicasets -o wide 1 2 3 # 레이블 지정하여 파드 목록 표시 # 레플리카셋 이름-임의의 문자열로 명명 ex. sample-rs-cnvm5 kubectl get pods -l app=sample-app -o wide 1 2 # 레플리카셋 상세 정보 표시 (증감 이력 등) # kubectl describe replicaset sample-rs 레플리카셋 주의점 spec.selector와 spec.template.metadata.labels의 레이블의 일치하지 않으면 에러 외부에서 같은 레이블을 가진 파드가 있으면 레플리카 수에 충족되는 만큼 파드 삭제 레플리카셋 스케일링 두 가지 방법 매니페스트 수정하여 kubectl apply -f 명령어 실행 IaC(Infrastructure as Code)를 구현하기 위해서라도 해당 방법 권장 1 2 $ sed -i -e \u0026#39;s|replicas: 3|replicas: 4|\u0026#39; sample-rs.yaml $ kubectl apply -f sample-rs.yaml kubectl scale 명령어를 사용하여 스케일 처리 레플리카셋 이외에도 디플로이먼트/스테이트풀셋/잡/크론잡에서 사용 가능 1 2 # 레플리카 수를 5로 변경 $ kubectl scale replicaset sample-rs --replicas 5 디플로이먼트 여러 레플리카셋을 관리하며 롤링 업데이트나 롤백 등을 구현하는 리소스 디플로이먼트 -\u0026gt; 레플리카셋 -\u0026gt; 파드 (관리 순서) 롤링 업데이트 과정 신규 레플리카셋 생성 신규 레플리카셋 레플리카 수(파드 수)를 단계적으로 늘림 이전 레플리카셋의 레플리카 수(파드 수)를 단계적으로 줄임 2,3 반복 이전 레플리카셋은 레플리카 수를 0으로 유지 파드를 하나만 기동해도 디플로이먼트 사용을 권장(파드 장애시 자동 재생성, 롤링 업데이트 등) 디플로이먼트 명령어 1 2 # --record를 사용하여 업데이트 이력을 저장. 디플로이먼트 가동 (deprecated option) $ kubectl apply -f sample-deployment.yaml --record 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 디플로이먼트 확인 $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE sample-deployment 3/3 3 3 2m30s # 레플리카셋 확인 $ kubectl get replicasets NAME DESIRED CURRENT READY AGE sample-deployment-77c7b569f6 3 3 3 2m39s # 파드 확인 $ kubectl get pods NAME READY STATUS RESTARTS AGE sample-deployment-77c7b569f6-bnbcw 1/1 Running 0 2m44s sample-deployment-77c7b569f6-fzh2m 1/1 Running 0 2m44s sample-deployment-77c7b569f6-wjvbn 1/1 Running 0 2m44s 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # 컨테이너 이미지 업데이트 $ kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record # 디플로이먼트 업데이트 상태 확인 $ kubectl rollout status deployment sample-deployment # 변경 이력 확인 $ kubectl rollout history deployment sample-deployment # 초기 상태의 디플로이먼트 $ kubectl rollout history deployment sample-deployment --revision 1 # 한 번 업데이트된 후의 디플로이먼트 $ kubectl rollout history deployment sample-deployment --revision 2 # 버전 지정하여 롤백 $ kubectl rollout undo deployment sample-deployment --to-revision 1 # 직전 버전으로 롤백 $ kubectl rollout undo deployment sample-deployment 1 2 3 4 5 # 업데이트 일시 정지 $ kubectl rollout pause deployment sample-deployment # 업데이트 일시 정지 해제 $ kubectl rollout resume deployment sample-deployment 1 2 3 4 5 6 # 레플리카 수를 3에서 4로 변경한 매니페스트를 apply - 디플로이먼트 스케일링 $ sed -i -e \u0026#39;s|replicas: 3|replicas: 4|\u0026#39; sample-deployment.yaml $ kubectl apply -f sample-deployment.yaml # kubectl scale 명령어 $ kubectl scale deployment sample-deployment --replicas=5 1 2 # 매니페스트를 사용하지 않고 명령어로 디플로이먼트 생성 $ kubectl create deployment sample-deployment-by-cli --image nginx:1.16 디플로이먼트 업데이트 전략 Recreate 모든 파드 삭제하고 다시 파드 생성 다운타임 발생하지만 추가 리소스를 사용하지 않으며 전환이 빠름 기존 레플리카셋의 레플리카 수를 0으로 하고 리소스 반환 신규 레플리카셋 생성하고 레플리카 수 늘림 RollingUpdate 업데이트 중 정지 가능 최대 파드 수(maxUnavailable)와 생성 가능한 최대 파드 수(maxSurge)를 설정 가능 추가 리소스를 사용하지 않도록 하거나 많은 리소스를 소비하지 않고 빠르게 전환하는 등의 동작 제어 가능 maxUnavailable과 maxSurge 모두 0은 불가능 상세 업데이트 파라미터 minReadySeconds(최소 대기 시간(초)) 파드가 Ready 상태가 된 후 디플로이먼트 리소스에서 파드 기동이 완료되었다고 판단(다음 파드의 교체가 가능하다고 판단)하기까지의 최소 시간 revisionHistoryLimit(수정 버전 기록 제한) 디플로이먼트가 유지할 레플리카셋 수 롤백이 가능한 이력 수 progressDeadlineSeconds(진행 기한 시간(초)) Recreate/RollingUpdate 처리 타임아웃 시간 타임아웃 시간 경과시 자동 롤백 디플로이먼트 주의사항 실제 환경에서는 롤백 보다는 이전 매니테스트를 kubectl apply를 실행하여 적용하는 것이 호환성 면에서 좋음 pause 상태에서는 업데이트가 즉시 반영되지 않고, 롤백도 되지 않음 데몬셋 레플리카셋의 특수 형태 (각 노드에 파드를 하나씩 배치) 노드를 늘렸을 때도 데몬셋의 파드는 자동으로 늘어난 노드에서 기동 로그 수집또는 모니터링 프로세스를 위해 사용 데몬셋 명령어 1 2 # 데몬셋 생성 $ kubectl apply -f sample-ds.yaml 데몬셋 업데이트 전략 OnDelete 데몬셋 매니페스트를 수정해도 기존 파드는 업데이트 X 일반적으로 모니터링, 로그 전송에 사용되므로 업데이트는 다음에 다시 생성하거나 수동으로 임의의 시점에 하도록 운영상 정지하면 안되는 파드, 업데이트가 급히 필요하지 않은 경우 사용 (이전 버전이 계속 사용되는 점 주의) 1 2 # 파드 업데이트 시 수동 정지 후 자동화된 복구 기능으로 새 파드 생성 $ kubectl delete pod sample-ds-ondelete-xxxxx RollingUpdate 데몬셋에서는 하나의 노드에 동일 파드를 여러개 생성할 수 없음 maxSurge 설정 불가 maxUnavailable만 지정하여 RollingUpdate maxUnavailable 0 불가 스테이트풀셋 DB와 같은 stateful한 워크로드에 사용하기 위한 리소스 레플리카셋과의 주된 차이점 생성되는 파드의 접미사는 숫자 인덱스 (sample-statefulset-0, sample-statefulset-1, \u0026hellip;) 파드명 불변 데이터를 영구적으로 저장하는 구조 (영구 볼륨을 사용하는 경우 파드 재기동시 같은 디스크 사용) 동시에 여러 파드가 생성되지 않고 하나씩 생성하며, Ready 상태가 되면 다음 파드를 생성 podManagemetPolicy를 Parallel로 설정하여 병렬로 동시에 기동 가능 스테이트풀셋 명령어 1 2 3 4 5 # 스테이트풀셋 생성 $ kubectl apply -f sample-statefulset.yaml # 스테이트풀셋 확인 $ kubectl get statefulsets 1 2 3 4 5 # 영구 볼륨 클레임 확인 $ kubectl get persistentvolumeclaims # 영구 볼륨 확인 $ kubectl get persistentvolumes 스테이트풀셋 스케일링 1 2 3 4 5 6 # 레플리카 수를 3에서 4로 변경한 매니페스트를 apply $ sed -i -e \u0026#39;s|replicas: 3|replicas: 4|\u0026#39; sample-statefulset.yaml $ kubectl apply -f sample-statefulset.yaml # kubectl scale을 사용한 스케일링 $ kubectl scale statefulset sample-statefulset --replicas=5 기본적으로 파드를 동시에 하나씩만 생성하고 삭제하므로 시간이 더 걸림 스케일 아웃일 때는 인덱스가 가장 작은 것 부터 파드를 하나씩 생성하고, 이전에 생성된 파드가 Ready 상태가 되고 나서 다음 파드 생성 스케일 인일 때는 인덱스가 가장 큰(가장 최근) 파드 부터 삭제 항상 0번째 파드가 먼저 생성되고 나중에 삭제되므로 마스터 노드로 사용하는 이중화 구조 애플리케이션에 적합 스테이트풀셋 업데이트 전략 Ondelete 데몬셋과 동일하게 매니페스트 변경해도 기존 파드는 그대로임 영속성 영역을 가진 DB나 클러스터등에서 많이 사용하므로 임의의 시점이나 다음에 재기동할 때 업데이트 RollingUpdate maxSurge, maxUnavailable 둘 다 사용 불가능 파드마다 Ready 상태인지 확인하고 업데이트 Parallel로 설정되어 있는 경우에도 하나씩 업데이트가 이루어짐 partition 설정시 전체 파드 중 어떤 파드까지 업데이트할지 지정 가능 전체에 영향을 주지 않고 부분적으로 업데이트를 확인할 수 있어 보다 안전 수동으로 재기동해도 Ondelete와 달리 partition보다 작은 인덱스를 가진 파드는 업데이트 X 예) partition이 3이면 0,1,2 인덱스 파드는 업데이트 X partition 값 수정하면 해당 값에 맞는 인덱스를 가진 파드들 업데이트됨 영구 볼륨 데이터 저장 확인 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 컨테이너 내부에 영구 볼륨 마운트 확인 $ kubectl exec -it sample-statefulset-0 -- df -h | grep /dev/sd # 영구 볼륨에 sample.html 이 없는지 확인 $ kubectl exec -it sample-statefulset-0 -- ls /usr/share/nginx/html/sample.html ls: cannot access \u0026#39;/usr/share/nginx/html/sample.html\u0026#39;: No such file or directory # 영구 볼륨에 sample.html 생성 kubectl exec -it sample-statefulset-0 -- touch /usr/share/nginx/html/sample.html # 영구 볼륨에 sample.html 이 있는지 확인 $ kubectl exec -it sample-statefulset-0 -- ls /usr/share/nginx/html/sample.html /usr/share/nginx/html/sample.html # 예상치 못한 파드 정지 1(파드 삭제) $ kubectl delete pod sample-statefulset-0 # 예상치 못한 파드 정지 2(nginx 프로세스 정지) $ kubectl exec -it sample-statefulset-0 -- /bin/bash -c \u0026#39;kill 1\u0026#39; # 파드 정지, 복구 후에도 파일 유실 없음 $ kubectl exec -it sample-statefulset-0 -- ls /usr/share/nginx/html/sample.html /usr/share/nginx/html/sample.html 스테이트풀셋을 삭제해도 영구 볼륨은 해제되지 않음 영구 볼륨 해제하지 않고 스테이트풀셋 재기동시 그대로 파드가 기동 1 2 # 스테이트풀셋이 확보한 영구 볼륨 해제 $ kubectl delete persistentvolumeclaims www-sample-statefulset-{0..4} 잡 컨테이너를 사용하여 한 번만 실행되는 리소스 N개의 병렬로 실행하면서 지정한 횟수의 컨테이너 실행(정상 종료)를 보장하는 리소스 파드의 정지가 정상 종료되는 작업에 적합 (레플리카셋의 경우 파드의 정지는 예상치 못한 에러임) 잡에서는 정상 종료한 파드 수(COMPLETION)을 표기함 잡 명령어 1 2 3 4 5 6 7 8 # 잡 생성 $ kubectl apply -f sample-job.yaml # 잡 목록 표시 $ kubectl get jobs # 잡이 생성한 파드 확인 $ kubectl get pods --watch 1 2 # 잡 상태 모니터링 $ kubectl get job sample-job-ttl --watch --output-watch-events 1 2 3 4 # 매니페스트를 사용하지 않고 명령어로 잡 생성 $ kubectl create job sample-job-by-cli \\ --image=amsy810/tools:v2.0 \\ -- sleep 30 1 2 # 크론잡 기반 잡 생성 $ kubectl create job sample-job-from-cronjob --from cronjob/sample-cronjob restartPolicy Never 파드에 장애가 발생하면 신규 파드가 생성 OnFailure 파드 장애 발생시 다시 동일한 파드를 사용하여 잡을 다시 시작 RESTART 카운터가 증가 파드 IP 주소는 변경 X, 영구 볼륨이나 hostPath 마운트하지 않은 경우 데이터 유실 태스크와 작업 큐 병렬 실행 completion: 몇 회 완료시 잡을 성공으로 표기할 것인지(기본값 1), 변경 불가 parallelism: 병렬성 지정(기본값 1), 변경 가능 backoffLimit: 실패 허용 횟수, 변경 가능 워크로드 completions parallelism backoffLimit 목적 1회만 실행하는 태스크 1 1 0 성공 유무에 관계 없이 반드시 1회 실행 N개 병렬로 실행시키는 태스크 M N P 병렬 태스크 수행 한 개씩 실행하는 작업 큐 미지정 1 P 한 번 정상 종료할 때까지 한 개씩 실행 N개 병렬로 실행하는 작업 큐 미지정 N P 큰 처리 전체가 정상 종료할 때까지 몇 개의 병렬 수로 계속 실행하고픈 경우 1 2 # 병렬성을 2에서 3으로 변경한 매니페스트 apply $ sed -e \u0026#39;s|parallelism: 1|parallelism: 2|\u0026#39; sample-job.yaml | kubectl apply -f - 크론잡 스케줄링된 시간에 잡 생성 크론잡 -\u0026gt; 잡 -\u0026gt; 파드 (3계층 관리 구조) 크론잡 일시정지 점검이나 어떤 이유로 잡 생성 원하지 않을 경우 suspend(일시정지) 매니페스트 수정 후 kubectl apply 실행 kubectl patch 1 $ kubectl patch cronjob sample-cronjob -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;suspend\u0026#34;:true}}\u0026#39; 동시 실행 제어 잡이 의도한 시간 간격에서 정상 종료시 동시 실행되지 않고 알아서 새로운 잡 실행 기존 잡이 실행되고 있을 때를 제어하고 싶은 경우 정책 개요 Allow(기본값) 동시 실행에 대한 제한 X Forbid 이전 잡이 종료되지 않은 경우 다음 잡 실행 X (동시 실행 X) Replace 이전 잡 취소 후 잡 시작(이전 잡의 레플리카 수를 0으로 변경) 실행 시작 기한 제어 쿠버네티스 마스터가 일시적으로 정지되는 경우와 같이 시작 시간 지연시, 이 지연 시간을 허용하는 시간 기본값은 아무리 늦어져도 잡을 생성 예) 매시 00분 시작 잡을 \u0026lsquo;매시 00~05분에만 실행 가능\u0026rsquo; 설정 시 300초로 설정 크론잡 이력 설정 항목 개요 spec.successfulJobsHistoryLimit 성공한 잡을 저장하는 개수 spec.failedJobsHistoryLimit 실패한 잡을 저장하는 개수 실제 운영 환경에서는 컨테이너 로그를 외부 로그 시스템을 통해 운영하는 것이 좋음 둘 다 0으로 설정시 잡은 종료시 즉시 삭제 매니페스트 사용하지 않고 크론잡 생성 1 2 3 4 $ kubectl create cronjob sample-cronjob-by-cli \\ --image amsy810/random-exit:v2.0 \\ --schedule \u0026#34;*/1 * * * *\u0026#34; \\ --restart Never 정리 파드나 레플리카셋 생성 시 처음부터 디플로이먼트로 생성하는 것이 좋음 리소스 종류 사용 방법 파드 디버깅이나 확인 용도 레플리카셋 파드 스케일링, 관리 디플로이먼트 스케일링할 워크로드에 사용 데몬셋 각 노드에 파드 하나씩 배포할 때 스테이트풀셋 영속성 데이터 등의 상태를 가진 워크로드에 사용 잡 작업 큐나 태스크 등 컨테이너 종료가 필요한 워크로드에 사용 크론잡 정기적으로 잡을 생성하고 싶은 경우 ","date":"2022-06-10T16:51:33+09:00","image":"https://Haebuk.github.io/p/%EC%9B%8C%ED%81%AC%EB%A1%9C%EB%93%9C-api/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EC%9B%8C%ED%81%AC%EB%A1%9C%EB%93%9C-api/","title":"워크로드 API"},{"content":"Docker 내용 정리 도커 컨테이너 설계 도커 컨테이너 생성 시 주의해야 할 점 네 가지\n1 컨테이너당 1 프로세스 변경 불가능한 인프라(Immutable Infrastructure) 이미지로 생성 경량의 도커 이미지로 생성 실행 계정은 root 이외의 사용자로 설정 1. 1 컨테이너당 1 프로세스 기존 VM처럼 하나의 이미지 안에 여러 프로세스는 비추 여러 프로세스 기동 시 주변 에코 시스템과 맞지 않거나 관리가 힘들어짐 2. 변경 불가능한 인프라를 구현하는 이미지로 생성 변경 불가능한 인프라: \u0026ldquo;환경 변경 시 오래된 환경은 없애고 새로운 환경 생성\u0026rdquo; 또는 \u0026ldquo;한번 만든 환경은 절대 불변하게\u0026rdquo; 전자의 경우 쿠버네티스는 자동으로 만들어주지만 후자는 컨테이너 이미지 관리자가 고려해야 함 도커 컨테이너는 버전 관리 가능하므로, 컨테이너 이미지 내에 애플리케이션 실행 바이너리 또는 관련 리소스를 가능한 포함시켜야 함 3. 도커 이미지 경량화 컨테이너 실행 시 최초 1회는 이미지를 외부에서 pull해야 함 dnf, yum, apt로 패키지 설치 후 저장소 패키지 목록 등의 캐시파일 삭제 멀티 스테이지 빌드 활용하여 이미지에 필요한 파일만 추가 기본 이미지가 경량인 배포판 이미지 사용 (ex. alpine linux, distorless 등) 도커 파일 최적화에 따라 레이어 줄이기 도커 이미지 생성시 squash 사용 4. 실행 계정 권한 최소화 root 사용자는 최대한 사용하지 않도록 한다. ENTRYPOINT와 CMD 컨테이너가 기동할 때 실행하는 명령어를 지정할 때 사용 아주 간단히 설명하면 $ENTRYOINT $CMD가 실행된다고 볼 수 있음 ENTRYPOINT에 바꿀 필요가 없는 부분을 정의하고 CMD에 기본값 인수 등을 정의하는 것이 일반적 예) ENTRYPOINT에 /bin/sleep 지정, CMD에 sleep 시간 지정 ","date":"2022-06-03T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/docker-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC/","title":"Docker 내용 정리"},{"content":"들어가며 현업에서 간단하게 로컬에서 데이터를 뽑아보려해도 수 GB는 훌쩍 넘어가는 경우가 다반사기 때문에, Pandas로는 한계가 있음을 느꼈습니다.\nDask를 사용하여 기초적인 병렬 계산, 데이터프레임 다루기, 간단한 신경망을 통해 학습하는 과정을 살펴보겠습니다.\nhttps://www.youtube.com/watch?v=Alwgx_1qsj4를 참고했습니다.\n예전에 촬영되어서 그대로 코드를 작성하면 작동하지 않는 코드가 여럿 있습니다. 2022년 1월 10일 기준으로 작동하도록 수정했습니다.\nPre-required dask와 함께 진행에는 영향이 없지만 아래에서 제공하는 시각화를 위해서는 graphviz 라이브러리를 설치해야합니다.\n또한 Machine Learning 파트에서 Tensorflow를 사용합니다. M1 맥북에서 실행했기 때문에 출력문에 약간의 차이가 발생할 수 있습니다.\nBasic 첫번째로 dask가 제공하는 병렬 계산에 대해 살펴보도록 하겠습니다.\n아래와 같이 함수가 작동할 때 마다 1초씩 대기하는 코드가 있습니다.\n1 2 3 4 5 6 7 8 9 from time import sleep def inc(x): sleep(1) return x + 1 def add(x, y): sleep(1) return x + y 한번 실행시켜 보겠습니다. x와 y에 1과 2를 할당하고 x와 y를 더합니다.\n1 2 3 4 5 %%time x = inc(1) y = inc(2) z = add(x, y) CPU times: user 451 µs, sys: 697 µs, total: 1.15 ms\rWall time: 3.01 s\r1초, 1초, 1초 3번을 대기 했기때문에 총 실행시간이 약 3초가 나왔음을 알 수 있습니다.\n이를 Dask를 이용하여 기다리지 않고 계산하게 만들 수 있습니다.\n1 from dask import delayed 이를 위해서 Dask의 delayed 모듈을 임포트합니다. delayed 모듈은 병렬로 계산하고자 하는 것이 있을 때 매우 효과적입니다.\n1 2 3 4 5 6 7 8 9 @delayed def delayed_inc(x): sleep(1) return x + 1 @delayed def delayed_add(x, y): sleep(1) return x + y 위에서 정의했던 inc와 add의 함수와 동일합니다. 단지 함수 위에 @delayed 데코레이터를 붙여주기만 하면 끝입니다.\n한 번 시간을 측정해보겠습니다.\n1 2 3 4 %%time x = delayed(delayed_inc)(1) y = delayed(delayed_inc)(2) z = delayed(delayed_add)(x, y) delayed메서드 안에 위에서 정의한 함수를 넣고 바깥에 함수 값을 할당합니다.\nCPU times: user 116 µs, sys: 20 µs, total: 136 µs\rWall time: 130 µs\r놀랍게도 1초도 안걸려 모든 계산이 끝났습니다. (정확한 결론은 아래를 참조해주세요.)\ndask가 어떤 병렬 계산을 수행했는지 시각적으로 확인할 수 있습니다.\n1 z.visualize() 위에서부터 차례로 코드를 실행하는 것이 아닌 병렬로 계산한다는 것을 알 수 있습니다.\ndask의 메서드로 정의한 값을 알아보려면 평소와는 다른 방법을 써야하는데요, 아래와 같습니다.\nz값 (2+3=5)가 나오길 기대했지만, 엉뚱한 값이 나옵니다.\n1 z Delayed('delayed_add-0e54f9e1-941d-49e9-903f-34e96b0dba54')\r이는 실제 계산이 수행된 것이 아닌 어떤 메타데이터를 가르키고 있다고 볼 수 있습니다.\n우리가 원하는 계산을 수행하려면 compute()를 사용해야 합니다.\n1 2 %%time z.compute() CPU times: user 1.76 ms, sys: 1.56 ms, total: 3.32 ms\rWall time: 2.01 s\r5\rz의 값은 5가 나왔고, 실행 시간은 2초가 나왔습니다. 각 inc(x) inc(y)가 병렬로 수행되는 데 1초, add(x+y)에서 1초가 소요되었기 때문입니다.\nFor loop 조금 더 오래걸리는 예제를 살펴보겠습니다.\n파이썬의 for문은 악명이 자자한데요, 데이터 수를 무자비하게 늘리기보다는 앞에서 사용했던 함수를 사용해 시간을 늘려보겠습니다.\n1부터 8까지 담겨져 있는 파이썬 리스트를 선언합니다.\n1 data = [1, 2, 3, 4, 5, 6, 7, 8] 리스트에서 값을 뽑아 inc함수에 삽입하고 결과를 빈 리스트에 담아 최종 합을 산출하는 코드입니다.\n1 2 3 4 5 6 7 8 %%time results = [] for x in data: y = inc(x) results.append(y) total = sum(results) CPU times: user 1.19 ms, sys: 1.2 ms, total: 2.38 ms\rWall time: 8.03 s\rinc가 총 8번 호출됐기 때문에 실행시간이 8초가 나왔습니다.\n이를 dask의 delayed 메서드로 병렬화시켜보겠습니다. 과연 모든 inc가 병렬로 계산되어 1초 남짓한 시간이 걸릴까요?\n1 2 3 4 5 6 7 8 9 10 11 %%time results = [] for x in data: y = delayed(delayed_inc)(x) results.append(y) total = delayed(sum)(results) total.compute() CPU times: user 3.28 ms, sys: 2.15 ms, total: 5.43 ms\rWall time: 1.01 s\r44\r1부터 8까지 모두 더한 44가 결과값으로 나왔고, 실행시간은 예상한대로 1초가 나왔습니다. 모든 inc 함수가 병렬로 수행됐음이 분명합니다.\n검증을 위해 시각화해보겠습니다.\n1 total.visualize() 처음에 이 그래프를 보고 감탄했던 기억이(\u0026hellip;) 아름답게 병렬 계산을 하는 것을 알 수 있습니다.\nDataFrame dask하면 생각나는 것이 바로 대용량 dataframe입니다. dask를 이용해 빠르게 로드하고 집계하는 방법에 대해 살펴보겠습니다.\n먼저 데이터는 약 200MB의 데이터로 뉴욕 공항의 항공기 이착륙 관련 데이터입니다.\n아래와 같이 데이터를 다운로드 하고 로드합니다.\n1 2 3 4 5 import urllib print(\u0026#34;- Downloading NYC Flights dataset... \u0026#34;, end=\u0026#39;\u0026#39;, flush=True) url = \u0026#34;https://storage.googleapis.com/dask-tutorial-data/nycflights.tar.gz\u0026#34; filename, headers = urllib.request.urlretrieve(url, \u0026#39;nycflights.tar.gz\u0026#39;) - Downloading NYC Flights dataset... 1 2 3 4 import tarfile with tarfile.open(filename, mode=\u0026#39;r:gz\u0026#39;) as flights: flights.extractall(\u0026#39;data/\u0026#39;) 여기까지 했다면 data폴더 안에 10개의 csv파일이 생성된 것을 확인할 수 있습니다.\npandas를 사용할 경우 이를 반복문을 통해 pd.concat으로 순차적으로 데이터프레임을 합치는 방법으로 접근하는데요, dask는 아래와 같이 분할된 파일을 한번에 로드할 수 있는 기능을 제공합니다.\n1 2 3 4 5 6 import os import dask.dataframe as dd df = dd.read_csv(os.path.join(\u0026#39;data\u0026#39;, \u0026#39;nycflights\u0026#39;, \u0026#39;*.csv\u0026#39;), parse_dates={\u0026#39;Date\u0026#39;: [0, 1, 2]}) df .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rpandas와는 다르게 모든 값이 감춰져있습니다. 이를 살펴보는 방법은 조금 후에 살펴보도록 하고, dask 데이터프레임 로드시 주의해야할 점에 대해 먼저 설명하겠습니다.\n1 df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rhead()메서드는 pandas와 동일한 기능을 제공합니다. tail()도 한 번 살펴볼까요?\n1 df.tail() ---------------------------------------------------------------------------\rValueError Traceback (most recent call last)\r/var/folders/xm/8mvqw44j1md_q70lrkm9_wh00000gn/T/ipykernel_47466/281403043.py in \u0026lt;module\u0026gt;\r----\u0026gt; 1 df.tail()\r/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/dask/dataframe/core.py in tail(self, n, compute)\r1143 1144 if compute:\r-\u0026gt; 1145 result = result.compute()\r1146 return result\r1147 ... 중략 ...\rValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\r+----------------+---------+----------+\r| Column | Found | Expected |\r+----------------+---------+----------+\r| CRSElapsedTime | float64 | int64 |\r| TailNum | object | float64 |\r+----------------+---------+----------+\rThe following columns also raised exceptions on conversion:\r- TailNum\rValueError(\u0026quot;could not convert string to float: 'N54711'\u0026quot;)\rUsually this is due to dask's dtype inference failing, and\r*may* be fixed by specifying dtypes manually by adding:\rdtype={'CRSElapsedTime': 'float64',\r'TailNum': 'object'}\rto the call to `read_csv`/`read_table`.\rtail()은 head()와 다르게 오류가 발생합니다. 이는 dask가 dataframe을 생성할 때 데이터타입을 데이터의 초반 행을 통해 추론하기 때문입니다.\n오류문을 살펴보면 CRSElapsedTime은 int64를 기대했는데 float64였고, TailNum은 float64를 기대했는데 object가 나타났다고 합니다.\n이를 해결하기 위해서는 dask dataframe을 정의할 때 데이터타입을 명시해줘야 합니다.\n1 2 3 4 5 6 7 df = dd.read_csv( os.path.join(\u0026#39;data\u0026#39;, \u0026#39;nycflights\u0026#39;, \u0026#39;*.csv\u0026#39;), parse_dates={\u0026#39;Date\u0026#39;: [0, 1, 2]}, dtype={\u0026#39;TailNum\u0026#39;: str, \u0026#39;CRSElapsedTime\u0026#39;: float, \u0026#39;Cancelled\u0026#39;: bool} ) CRSElapsedTime과 TailNum의 데이터 타입을 명시하고, 이따가 사용할 Cancelled열도 미리 데이터 타입을 선언해주겠습니다.\n다시 한 번 마지막 값을 살펴보겠습니다.\n1 df.tail() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r정상적으로 값이 출력됨을 알 수 있습니다.\n이번에는 간단한 집계함수를 사용해보겠습니다.\n1 %time df.DepDelay.max().compute() CPU times: user 3.18 s, sys: 526 ms, total: 3.71 s\rWall time: 1.61 s\r1435.0\r매우 빠르게 최대값을 산출해냄을 알 수 있습니다. dask는 이를 어떻게 계산했을까요?\n시각적으로 살펴보겠습니다.\n1 df.DepDelay.max().visualize(rankdir=\u0026#39;LR\u0026#39;, size=\u0026#39;12, 12!\u0026#39;) 각각의 파티션(총 10개)에서 최대값 후보를 선정한다음에 최종 최대값을 선출해냄을 알 수 있습니다.\n단순하게 생각하면 pandas 집계보다 10배 빠르다고 볼 수도 있겠습니다.\nML with Dask 마지막으로 간단한 신경망을 통해 학습하는 방법을 살펴보고 마치겠습니다.\n먼저 학습에 사용할 데이터를 정의하고 정보를 확인합니다.\n1 df_train = df[[\u0026#39;CRSDepTime\u0026#39;, \u0026#39;CRSArrTime\u0026#39;, \u0026#39;Cancelled\u0026#39;]] 1 df_train.iloc[:, :-1].compute().values array([[1540, 1701],\r[1540, 1701],\r[1540, 1701],\r...,\r[1645, 1901],\r[1645, 1901],\r[1645, 1901]])\r1 df_train.iloc[:, -1].compute().values array([False, False, False, ..., False, False, False])\r1 df_train.shape (Delayed('int-94ab9ac8-9432-4a95-b40f-abdaca09c41e'), 3)\r0번째 값은 dask delayed객체로 나오고, 1번째 값은 총 열 개수인 3이 나오는 것이 특징입니다.\n1 df_train.isnull().sum().compute() CRSDepTime 0\rCRSArrTime 0\rCancelled 0\rdtype: int64\r결측치는 없습니다. 아주 간단한 신경망을 정의하고 학습시켜보겠습니다.\n1 2 3 4 5 6 7 8 9 import tensorflow as tf from keras.models import Sequential from keras.layers import Dense model = Sequential() model.add(Dense(20, input_dim=df_train.shape[1]-1, activation=\u0026#39;relu\u0026#39;)) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;sgd\u0026#39;) from_tensor_slices를 사용해 데이터프레임을 변환합니다.\n1 2 3 dataset = tf.data.Dataset.from_tensor_slices( (df_train.iloc[:, :-1].compute().values, df_train.iloc[:, -1].compute().values) ).batch(512) 1 model.fit(dataset, epochs=5) Epoch 1/5\r10203/10203 [==============================] - 38s 4ms/step - loss: 239.6750\rEpoch 2/5\r10203/10203 [==============================] - 38s 4ms/step - loss: 0.1011\rEpoch 3/5\r10203/10203 [==============================] - 38s 4ms/step - loss: 0.1006\rEpoch 4/5\r10203/10203 [==============================] - 39s 4ms/step - loss: 0.1006\rEpoch 5/5\r10203/10203 [==============================] - 39s 4ms/step - loss: 0.1006\r\u0026lt;keras.callbacks.History at 0x298c5f040\u0026gt;\r여기서는 학습을 할 수 있다에 초점을 맞췄기 때문에, 성능 검증은 다루지 않습니다.\n마치며 대용량 데이터에 적합한 라이브러리인 dask에 대해 기초를 다뤄봤습니다. 심화된 기능은 공식 홈페이지에 상세히 나와있습니다.\ndask의 기능을 좀 더 숙지한다면 매우 많은 부분에서 pandas를 대체할 수 있을것이라 기대합니다.\n더 소개할만한 기능을 수집해서 다음 포스팅에 공유하도록 하겠습니다. 감사합니다.\n","date":"2022-01-10T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/dask-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC/","title":"Dask 튜토리얼"},{"content":"엘라스틱 서치를 파이썬으로 쿼리하는 방법에 대해 알아보겠습니다.\n현업에서 엘라스틱 서치를 통해 정보를 받고 Kibana를 통해 시각화를 많이 하고 있는데요, 이러한 로그 정보들을 파이썬을 통해 분석하고 싶은 경우가 있습니다. 예를 든다면 dau(daily activate user)의 정보를 수집하고 있는데 몇시에 가장 많이 접속하는지, 이런 정보를 가공해 다른 곳에 사용한다던지 말이죠.\n이러한 정보를 키바나를 통해 확인할 수 있지만, 별도의 레포트를 만들 경우 seaborn이나 matplotlib을 통해 시각화를 진행할때가 많습니다. 그렇다면 어떻게 엘라스틱 서치로 저장된 정보를 파이썬으로 쿼리할 수 있을 지 알아보겠습니다.\n여기서는 scroll 메서드를 사용해 순차적으로 저장된 모든 정보를 불러오고 json 파일로 저장하는 방법을 살펴봅니다.\n예제 아래 예제에서는 dau 라는 이름을 가진 인덱스에서 product_id=2021 인 정보를 모두 쿼리합니다.\n1 2 3 4 5 6 7 import json from datetime import datetime from elasticsearch import Elasticsearch host = \u0026#34;받아올 엘라스틱 서치 URI\u0026#34; es = Elasticsearch(host) indices = [\u0026#39;dau\u0026#39;] # 인덱스 명 size = 10000 # 한 번에 받아올 데이터 갯수 여기서 host 는 본인이 사용하고 있는 엘라스틱 서치 서비스의 URI입니다. size=10000 은 한 번에 1만개씩 받아오겠다는 의미입니다.\n1 2 3 4 5 6 7 8 doc = { \u0026#39;size\u0026#39;: size, \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;product_id\u0026#39;: 2021, } } } doc에 쿼리 정보를 담아줍니다.\n1 2 3 4 # 이전 스크롤 1초 저장 response = es.search(index=indices, body=doc, scroll=\u0026#39;1s\u0026#39;) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] result = [] response 에 엘라스틱 서치 search 메서드를 정의합니다. old_scroll_id 에 초기 스크롤 id를 정의하고 결과를 받을 빈 리스트를 정의합니다.(result)\n1 2 for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) 가장 처음 쿼리를 저장합니다.\n1 2 3 4 5 6 while len(response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]): response = es.scroll(scroll_id=old_scroll_id, scroll=\u0026#39;1s\u0026#39;) for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] # scroll id 초기화 print(f\u0026#39;Result Length: {len(result)}\u0026#39;) 이후 반복문을 통해 더 이상 받아올 정보가 없을 때 까지 탐색하여 정보를 저장합니다.\n1 2 3 # save file with open(\u0026#39;es_dau.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(result, f) 마지막으로 result 를 json 형식의 파일로 저장하면 이후 자유롭게 어디서나 쿼리한 정보를 받아올 수 있습니다.\n전체코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import json from datetime import datetime from elasticsearch import Elasticsearch host = \u0026#34;사용할 엘라스틱 서치 URL\u0026#34; es = Elasticsearch(host) indices = [\u0026#39;dau\u0026#39;] size = 10000 doc = { \u0026#39;size\u0026#39;: size, \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;product_id\u0026#39;: 2021, } } } # 이전 스크롤 1초 저장 response = es.search(index=indices, body=doc, scroll=\u0026#39;1s\u0026#39;) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] result = [] for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) while len(response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]): response = es.scroll(scroll_id=old_scroll_id, scroll=\u0026#39;1s\u0026#39;) for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] # scroll id 초기화 print(f\u0026#39;Result Length: {len(result)}\u0026#39;) # save file with open(\u0026#39;es_dau.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(result, f) ","date":"2021-12-19T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%EC%97%98%EB%9D%BC%EC%8A%A4%ED%8B%B1-%EC%84%9C%EC%B9%98-%EC%BF%BC%EB%A6%AC%ED%95%98%EA%B8%B0/","title":"파이썬으로 엘라스틱 서치 쿼리하기"},{"content":"지난번 포스팅에서 data analytics learning plan을 수강한다고 언급했었는데요,(참조: [AWS] Data Analytics Learning Plan을 시작하며) 아무래도 영어로 된 강의다보니 첫 수강에 부담이 있었습니다.\n한국어로 된 유사한 강의가 있나 찾아보던 중 발견하게 되어 먼저 수강하기로 결정했습니다. (링크: Data Analytics Fundamentals (Korean), 수강신청을 하지 않았을 경우 접속이 되지 않을 수 있습니다. 먼저 aws skill builder에서 등록을 진행해주세요.)\n강의 구성 강의는 약 3시간 30분으로 이루어져 있습니다. 강의 구성은 5V에 대해 소개하는데요, 볼륨(Volume), 속도(Velocity), 다양성(Variety), 정확성(Veracity), 가치(Value)의 5V입니다.\n데이터 분석 과정에서 직면한 5V의 문제를 어떤식으로 접근해야하는지, AWS의 어떤 서비스를 이용하면 되는지에 대해 소개합니다.\n후기 솔직히 별로..라고 생각했습니다.\n기초강의다보니 실무적으로 사용하는 방법보다는, 이런 문제는 우리의 어떤 서비스를 통해 해결할 수 있어~ 에서 마치는 느낌입니다.\nAWS에 어느정도 관심있으신 분들은 다 아시는 내용일거라고 생각합니다.\n아무튼 수료 ","date":"2021-01-09T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/aws-data-analysis-fundamentals-%ED%9B%84%EA%B8%B0/","title":"AWS Data Analysis Fundamentals 후기"}]