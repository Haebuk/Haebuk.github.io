[{"content":"\n아이디어 불패의 법칙 - 시장의 나오는 무수한 아이디어 중 살아남는 아이디어는 극소수에 불과하다.\n어떤 아이디어가 불패하는, 실패하지 않는 ‘될 놈’일까.\n이 책은 지난번 읽었던 ‘진화된 마케팅, 그로스해킹’을 읽었던 것만큼 재밌게 읽었다. 다음과 같은 띵-했던 순간들이 있었다.\n‘생각랜드’라는 단어는 나조차 한 번은 빠졌었던 같은 착각의 장소가 구체화한 것 같은 느낌을 받았다. ‘프리토타이핑’이라는 최소한의 검증 모델은 그로스해킹에서 수행하는 실험과 비슷한 냄새가 났고, XYZ - xyz가설은 이러한 검증 모델이 막연하게 수행되지 않도록 바로 잡아주는 도구와 같았다고 생각했다. 중간에 나오는 수많은 프리토타이핑 방법론과 점수 평가 척도는 이 책에서 말하는 불패가 허황된 것이 아님을 느끼게 했다. 책 내용 중 저자는 이 방법론을 알게 된 후로 굉장히 다양한 곳에서 이 방법론을 적용할 기회가 있었다고 말한다.\n나 역시도 최근에 이러한 상황을 겪게 되어서 속으로 미소를 지었던 경험이 있다.\n토스에서 발간한 ‘유난한 도전’에는 초창기 비바리퍼블리카의 실패 사례가 있다. 대충 요약하면 다음과 같다.\n“울라블라는 오프라인 SNS 앱인데, 싱가포르에서 디자인상도 받았어. 알려지기만 하면 대박일 거야. 누가 훔쳐갈까 봐 특허 등록도 해놨어. ㅡ 1년 4개월이 지난 후 ㅡ 울라블라는 실패했다는 것을 인정할 수밖에 없었다. 이외에도 8번의 실패한 서비스가 있었다.”\n이 사례를 보며 나는 초창기 비바리퍼블리카가 이 방법론을 알았다면 더 빨리 ‘안 될 놈’을 포기하거나 더 나은 아이디어를 찾을 수 있지 않았을까 생각한다. (물론 토스는 지금 내가 왈가왈부할 수 있는 위치에 있지 않다. 나 역시 토스 앱을 사랑하고 책도 샀을 정도로 토스의 팬이다. 사랑해요 토스) 책을 더 읽다 보면 실제로 이 방법론을 실패로 체득하고 엄청나게 빨리 성장하는 모습이 나온다.\n또한 책을 읽으면서 계속 떠나지 않았던 생각은 ‘이 아이디어가 정말 제품이 시장에 나오기 전에만 먹히는 것일까?’라는 생각을 했다. 한 제품 안에는 수많은 기능이 존재하고, 어떤 기능들은 수많은 인력과 시간 비용이 들어가서 세상에 나온다. 이러한 기능들은 반드시 ‘될 놈’이 아닐 테니, 초기에 이 방법론을 적용해 위험을 줄일 수 있겠다는 생각을 했다. 내 생각은 위에서도 언급했지만 그로스해킹의 방법론과 연계가 되었다. 역시 성공하는 무언가에는 공통점이 있나 보다.\n다음은 책을 읽으면서 인상 깊게 느꼈던 구절들.\n실패와 관련된 구절 여러 결과 중에서 확률이 가장 높은 것은 ‘실패’다. 약 80퍼센트의 신제품이 처음의 기대에 미치지 못하고 ‘실패’나 ‘실망’, ‘취소’ 등으로 분류된다. 유능한 실행력은 실패에 대한 해결책이 되지 못한다. 인상 깊었던 ‘생각랜드’ ‘생각랜드’에 빠지지 않기 - 누구나 자신의 아이디어는 시장에서 먹힐 것이라 확신하고 팀의 역량이 강할수록 이러한 확신은 더 강해진다. ‘생각랜드’에 빠지지 않도록 조심해야 한다. 자신의 아이디어를 테스트할 최소한의 모형을 만들어 시장에서 먹히는지 확인해야 한다. 구글의 데이터 지향적 의사결정 과정 신선함: 데이터는 새로 나온 것일수록 좋다. 몇 년 전의 진실은 지금 거짓일 수 있다. 90년대 말의 8초 법칙(웹사이트 로딩이 8초 이내에 되어야 한다.)은 이제 2초 법칙이 됐다. 조만간 0.5초 법칙이 될 것 확실한 관련성: 지금 평가하려는 특정 제품이나 의사결정에 직접 적용할 수 있어야 한다. (당연하다고 여기지 말자. 항상 냉정하게 판단하자.) 알려진 출처: 다른 곳에서 수집한 데이터에 의존하면 안 된다. 이해관계 때문에 어떤 편향이나 영향력, 동기가 작용했을 수 있다. 통계적 유의성: 데이터는 통계적으로 유의미해야 한다. 충분히 큰 샘플을 사용해야 한다. 개인적인 경험과 일회성 이야기를 데이터로 제시하지 말자. 숫자로 이야기하기: 모호한 용어를 피하고 가능하다면 숫자를 사용하자. 데이터를 표현하는 최고의 방법은 숫자로 이야기하는 것이다. XYZ 가설 적어도 X 퍼센트의 Y는 Z 할 것이다. 최초 값은 그냥 출발점에 불구 하다. 근사치조차 되지 못할 수 있다. 그러나 시장도 그렇게 생각하는지 검증할 수 있는 강력한 도구가 된다. (모호함을 제거하는 데 뛰어난 효과를 보인다.) 범위 축소: xyz 가설 - 범위를 더 축소한다. 테스트는 로컬 하게 할 수 있도록 하자. 전 세계나 대한민국은 여전히 큰 범위이다. 적극적 투자 지표 모든 사람의 의견은 0점이다. 프리토타이핑의 될 놈 척도를 평가할 수 있는 구체적인 투자 지표를 만들어야 한다. 비용과 시간을 많이 쓸수록 높은 점수를 준다. 북클럽에서 제목이 너무 K-경영서 제목같다는 비평이 많았다. 나도 웃으면서 공감한다. 출판사에서도 제목에 대한 프리토타이핑을 했으면 이런 제목이 나왔을까 싶다.\n","date":"2022-12-12T15:30:39+09:00","image":"https://Haebuk.github.io/p/%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4-%EB%B6%88%ED%8C%A8%EC%9D%98-%EB%B2%95%EC%B9%99-%ED%9B%84%EA%B8%B0/idea_hu5136b3058c11f31409e30fefa5f0d8ae_141624_120x120_fill_q75_box_smart1.jpeg","permalink":"https://Haebuk.github.io/p/%EC%95%84%EC%9D%B4%EB%94%94%EC%96%B4-%EB%B6%88%ED%8C%A8%EC%9D%98-%EB%B2%95%EC%B9%99-%ED%9B%84%EA%B8%B0/","title":"아이디어 불패의 법칙 후기"},{"content":"Introduction Local Runner(개발 환경)에서 개발한 dag(코드)를 MWAA(프로덕션 환경)에 업로드하는 경우, 개발 과정에서 프로덕션 환경의 Connections(연결) 및 Variables(변수)를 개발 환경에도 적용시키고 싶은 경우가 있다.\n가장 단순하게는 Web UI나 CLI를 사용해 Airflow의 메타 저장소에 등록하여 사용해도 된다. 그러나 같은 두 환경 간 차이가 발생할 수 있으며 특히 개발 환경의 경우 reset-db를 하는 경우가 빈번하므로 그때마다 휘발된 변수 및 연결을 다시 등록하는 것은 매우 번거로울 것이다.\n이 글에서는 Web UI나 CLI를 사용하여 Airflow의 연결과 변수를 등록하지 않고 Secrets Backend, 그 중에서도 AWS Secrets Manager를 사용해 개발과 프로덕션 환경의 연결과 변수를 통일하는 방법을 소개한다.\nSetting Config 먼저 이 글에서는 Local Runner와 MWAA는 이미 실행이 가능한 상태로 가정한다.\nsecrets backend Airflow의 secrets backend를 AWS Secret Manager로 사용하기 위해서는 아래와 같은 두 줄을 추가해야한다.\n1 2 3 [secrets] backend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend backend_kwargs = {\u0026#34;connections_prefix\u0026#34;: \u0026#34;airflow/connections\u0026#34;, \u0026#34;variables_prefix\u0026#34;: \u0026#34;airflow/variables\u0026#34;} 여기서 주목해야할 것은 *_prefix인데, Airflow가 secrets backend에서 연결 및 변수를 가져오기 위해서는 각각의 id에 정확한 prefix를 붙여야 가져올 수 있다.\n연결과 변수 구성에 대한 예를 들어보자.\nsecrets backend에 airflow/connections/my_conn_id를 정의한 경우 operator에서 Connection(conn_id=\u0026quot;my_conn_id\u0026quot;)로 불러올 수 있다. airflow/variables/my_var를 정의한 경우 Variable.get(\u0026quot;my_var\u0026quot;)로 내부에 저장된 값을 가져올 수 있다. 오타가 나면 airflow.exceptions.AirflowNotFoundException: The conn_id isn't defined 에러가 발생하므로 주의하자.\n만약 프로덕션에 사용되는 환경이 여러개고 각 환경마다 config를 다르게 구성하고 싶으면 다음과 같은 방식으로 prefix를 변경하여 사용 가능하다.\n1 2 3 [secrets] backend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend backend_kwargs = {\u0026#34;connections_prefix\u0026#34;: \u0026#34;my_airflow_env/connections\u0026#34;, \u0026#34;variables_prefix\u0026#34;: \u0026#34;my_airflow_env/variables\u0026#34;} MWAA와 Local Runner에서 위 config을 추가하는 방식은 약간 상이하기 때문에 나눠서 살펴보자.\nMWAA 본인의 MWAA 환경에서 편집버튼을 누르면 두 번째 페이지에 “Airflow 구성 옵션 - 선택 사항” 항목이 있다. 여기서 두 가지 옵션을 입력한다.\n이 구성 옵션 항목에 입력하는 파라미터들이 airflow.cfg 에 들어간다고 이해하면 편하다. 이후 저장하며 환경 구성을 업데이트한다. (약 10여분 소요된다.)\n추가로 MWAA 환경의 실행 역할에 Secrets Manager 권한을 추가해야 한다. 여기서는 SecretsManagerReadWrite 정책을 추가했다.\n만약 SecretsManagerReadWrite가 아닌 최소 정책을 추가하고 싶다면 아래와 같은 커스텀 정책을 추가하도록 한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:GetResourcePolicy\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:us-west-2:012345678910:secret:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Local Runner Local Runner의 경우 local-runner/docker/config/airflow.cfg에서 [secret]항목의 다음 두 줄을 수정한다.\n1 2 3 [secret] backend = airflow.providers.amazon.aws.secrets.secrets_manager.SecretsManagerBackend backend_kwargs = {\u0026#34;connections_prefix\u0026#34;: \u0026#34;airflow/connections\u0026#34;, \u0026#34;variables_prefix\u0026#34;: \u0026#34;airflow/variables\u0026#34;, \u0026#34;region_name\u0026#34;: \u0026#34;ap-northeast-2\u0026#34;} 2-1 MWAA 항목과 거의 유사하지만 \u0026quot;region_name\u0026quot;: ap-northeast-2 항목이 추가되었다. local runner의 s3 sync를 사용하는 경우 local-runner/docker/config/.env.localrunner에 AWS_ACCESS_KEY_ID 와 AWS_SECERT_ACCESS_KEY 가 입력되어 있을 것이므로 backend_kwargs 에서 생략이 가능하다. 만약 .env.localrunner 에 상기 항목을 등록하지 않은 경우 backend_kwargs 에 추가해야한다.\nCreate Secrets 이제 Secrets Manager에 샘플 변수와 연결을 만들고 개발 및 프로덕션 환경에서 모두 적용되는지 확인해보자.\nVariables AWS Secrets Manager - 암호 생성 - 다른 유형의 보안 암호 선택 - 일반 텍스트에 hello i'm variable 을 입력했다.\n하단의 암호화 키 - 사용하고 있거나 새로 생성한다. 다음을 누른다.\n보안 암호 이름을 2에서 주었던 prefix와 동일하게 작성한다. 나는 airflow/variables/sample_var_1 로 등록하였다. 설명은 옵션이므로 생략 가능하다. 다음을 선택한다.\n교체 구성은 따로 건드리지 않고 다음을 누르고 저장한다. 완료 후 새로고침 시 생성이 된 것을 확인할 수 있다.\nConnections 연결은 변수와 거의 비슷하지만 다른 점이 있다. 시크릿 값을 URI string으로 줄 것인가 key-value pair로 줄 것인지를 정해야 한다.\n이 글을 따라왔다면 URI string으로 구성해야한다. key-value pair로 구성하기 위해서는 backend-kwargs 구성에 \u0026quot;full_url\u0026quot;: \u0026quot;true\u0026quot; 옵션을 추가로 줘야 한다. 자세한 설명은 Storing and Retrieving Connection 을 참고한다.\n이해를 돕기 위해 sample connection은 mysqlOperator에 connection을 연결한다 가정하고 생성해보겠다.(데이터베이스는 따로 구성하지 않겠다.)\nMySqlOperator는 다음과 같은 파라미터를 받는다.\n위 파라미터를 URI string으로 구성한다. Airflow의 URI string은 다음과 같이 구성된다. (URI format example)\n1 my-conn-type://login:password@host:port/schema?param1=val1\u0026amp;param2=val2 이를 참조해 sample connection URI string을 다음과 같이 작성할 수 있다.\n1 mysql://ID:PW@HostUrl:3306/dbName?charset=utf8 3-1. Variable과 동일하게 AWS Secrets Manager에 새 보안 암호를 생성한다. 키/값 페어 항목에서 일반 텍스트에 연결 URI string을 입력한다.\n보안 암호 이름을 airflow/connections/sample_connection_1 과 같이 작성하고 저장한다.\n연결과 변수를 모두 생성하였다.\nRun Examples 이제 개발 환경과 프로덕션 환경에서 두 시크릿을 가져올 수 있는지 확인해보자.\n연결과 변수를 가져오는 두 task를 정의하는 dag(sample_read_secret_manager_dag)를 작성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from airflow.decorators import dag, task from airflow.models import Variable from airflow.models.connection import Connection from airflow.utils.dates import days_ago default_args = { \u0026#34;owner\u0026#34;: \u0026#34;airflow\u0026#34;, \u0026#34;start_date\u0026#34;: days_ago(1), \u0026#34;depends_on_past\u0026#34;: False, \u0026#34;catchup\u0026#34;: False, } @dag( default_args=default_args, schedule_interval=\u0026#34;@once\u0026#34;, tags=[\u0026#34;read_secrets_manager\u0026#34;], ) def sample_read_secret_manager_dag(): @task() def print_variable_from_sm(): var = Variable.get(\u0026#34;sample_var_1\u0026#34;) print(var) @task() def print_connection_from_sm(): c = Connection(conn_id=\u0026#34;sample_connection_1\u0026#34;) uri = c.get_uri() print(uri) print_variable_from_sm() print_connection_from_sm() dag = sample_read_secret_manager_dag() dag를 작성 후 실행시켜보면 개발 환경과 프로덕션 환경 모두 성공한 것을 확인할 수 있다.\n개발 환경과 프로덕션 환경 모두 두 task 로그를 확인해봤을 때 AWS Secrets Manager에 저장한 값을 잘 받아오는 것을 확인할 수 있다.\n마치며 Airflow secrets backend, 그 중에서도 AWS Secrets Manager를 사용하여 Airflow의 연결 및 변수를 통일하는 작업을 살펴봤다. 연결 및 변수 정보를 외부에 한 번 저장하는 것으로 두 환경에서 받아올 수 있었다. 이를 통해 좀 더 빠른 개발을 가능하게 할 수 있을 것으로 기대 된다.\nReference Configuring an Apache Airflow connection using a Secrets Manager secret AWS Secrets Manager Backend ","date":"2022-11-10T19:03:39+09:00","image":"https://Haebuk.github.io/p/aws-secrets-manager%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%B4-local-runner%EC%99%80-mwaa-secrets-backend-%ED%86%B5%EC%9D%BC%ED%95%98%EA%B8%B0/AirflowLogo_hubc3c506518d9161c24efab84ad5b3982_59053_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/aws-secrets-manager%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%B4-local-runner%EC%99%80-mwaa-secrets-backend-%ED%86%B5%EC%9D%BC%ED%95%98%EA%B8%B0/","title":"AWS Secrets Manager를 사용해 Local Runner와 MWAA Secrets backend 통일하기"},{"content":"\n총평: 그로스 해킹을 적용하고 있는 사람들에게 내비게이션이 될 수 있는 책\n사내에서 그로스 해킹을 접목한 지 1년이 되어간다. 유의미한 결과도 있었지만 무의미한 결과도 있었다. 우리는 작은 회사니 우리 인원으로 한 달에 한 번 실험이면 충분하다는 생각을 가지고, 이 정도면 잘하고 있다고 생각하며 어느 정도 안일하게 지내온 것 같다.\n그러나 이 책을 읽으며 반성을 많이 했다. 책에서는 우선순위와 함께 다양한 상황에 맞는 가이드를 제공한다. 수도 없이 ‘어, 이거 우리 서비스에서도 적용할 수 있는 이야기인데, 왜 못했지?’라는 생각을 했던 것 같다. 어쩌면 잘못 나아가고 있는 방향에 대해 짚어주고, ‘경로가 잘못되었으니 돌아가라’라는 조언을 방법과 함께 들은 것 같다.\n이 책을 입사 당시 한 번, 지금, 총 두 번 읽었다. 부끄럽지만 처음 읽었을 때는 조금 지루한 책이라고 생각했다. 뭔가 당연한 얘기를 늘어뜨려 하는 것 같았다. 그래서 다른 그로스 해킹 책이 낫다고 생각했었다.\n그러나 서비스에 좀 더 몰두하고 그로스 해킹에 대해 고민을 많이 하고 나니, 이 책에는 정말 많은 서비스의 성장에 대한 고민과 해결 과정의 노하우가 담겨 있다고 생각했다. 내년에 한 번 더 읽으면 또 어떤 충격을 가져올지 기대가 되었다. 책에는 적용할 수 있는 사례가 너무 많아 한 번 읽은 것으로는 모든 것을 개선할 수 없다고 느꼈다. 내비게이션이 알려주는 처음 가는 먼 길을 내비게이션 없이 다음에 또 올 수 있을까? 정말 성공적인 그로스 해킹을 위해서라면 이 책이 주는 조언을 체화해야겠다고 생각한다.\n","date":"2022-11-04T07:02:39+09:00","image":"https://Haebuk.github.io/p/%EC%A7%84%ED%99%94%EB%90%9C-%EB%A7%88%EC%BC%80%ED%8C%85-%EA%B7%B8%EB%A1%9C%EC%8A%A4%ED%95%B4%ED%82%B9-%ED%9B%84%EA%B8%B0/Untitled_hu2de9d0fd70d2718c6f985d582005644f_661356_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EC%A7%84%ED%99%94%EB%90%9C-%EB%A7%88%EC%BC%80%ED%8C%85-%EA%B7%B8%EB%A1%9C%EC%8A%A4%ED%95%B4%ED%82%B9-%ED%9B%84%EA%B8%B0/","title":"진화된 마케팅, 그로스해킹 후기"},{"content":"AWS Well-Architected 프레임워크 운영 우수성 시스템을 실행 및 모니터링하여 비즈니스 가치를 제공, 지속적으로 지원 프로세스 및 절차를 개선하는 능력 코드로 작업 수행, 문서에 주석 추가, 실패 예측, 되돌릴 수 있는 소규모 변경을 자주 수행이 포함 보안성 위험 평가 및 완화 전략을 통해 비즈니스 가치를 제공하는 동시에 정보, 시스템, 자산을 보호하는 능력 가능한 보안 모범 사례 자동화, 모든 계층에 보안 적용, 전송 중/저장 시 데이터 보호가 포함 안정성 인프라 또는 서비스 중단으로부타 복수, 컴퓨터 리소스 동적 확보로 수요 충족, 잘못된 구성 또는 일시적 네트워크 문제와 같은 중단 완화가 포함 복구 절차 테스트, 전체 시스템 가용성을 높이기 위한 수평 확장, 장애 발생 시 자동 복구가 포함 성능 효율성 컴퓨팅 리소스의 효율적인 사용으로 시스템 요구 사항을 충족하고, 수요 변화와 기술 진화에 따라 효율성을 유지하는 능력 실험 빈도 증가, 서버리스 아키텍처 사용, 몇 분 만에 전세계 배포가 가능한 시스템 설계 등이 포함 비용 최적화 가장 낮은 가격으로 비즈니스 가치를 제공하도록 시스템을 실행하는 능력 소비 모델 채택, 비용 분석 및 귀속, 관리형 서비스를 사용해 소유 비용 절감이 포함 AWS 클라우드의 이점 선행 비용을 가변 비용으로 대체 컴퓨팅 리소스를 사용할 때만 비용 지불 거대한 규모의 경제로 얻는 이점 인프라를 소유할 때보다 가변 비용 낮아짐 수많은 고객의 사용량이 클라우드에 누적되므로 AWS와 같은 공급자는 더 높은 규모의 경제 달성 용량 추정 불필요 필요한 용량에만 액세스하고 수요에 따라 확장 또는 축소 속도 및 민첩성 개선 클라우드 컴퓨팅의 유연성 덕에 애플리케이션을 더 쉽게 개발하고 배포할 수 있음 데이터 센터 운영 및 유지 관리에 비용 투자 불필요 인프라 및 서버 관리에 신경을 덜 쓰고 애플리케이션과 고객에 더 집중할 수 있음 몇 분 만에 전 세계에 배포 전 세계 고객에게 짧은 지연 시간을 제공하면서 애플리케이션을 신속히 배포 ","date":"2022-10-23T23:21:30+09:00","permalink":"https://Haebuk.github.io/p/aws-cloud-practitioner-%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%97%AC%EC%A0%95-%EC%A0%95%EB%A6%AC/","title":"[AWS Cloud Practitioner] 클라우드 여정 정리"},{"content":"AWS Cloud Adoption Framework(AWS CAF) 6가지 주요 관점 비즈니스 관점: IT가 비즈니스 요구 사항을 반영하고 IT 투자가 주요 비즈니스 결과와 연계되도록 보장 인력 관점: 클라우드 채택을 성공하기 위한 조직 전반의 변화 관리 전략 개발 지원 거버넌스 관점: IT 전략이 비즈니스 전략에 부합하도록 조정하는 기술 및 프로세스에 중점 비즈니스 가치를 극대화하고 위험 최소화 플랫폼 관점: 클라우드 기반으로 새로운 솔루션 구축, 온프레미스 워크로드를 클라우드로 마이그레이션하기 위한 원칙과 패턴 포함 보안 관점: 조직이 가시성, 감사 가능성, 제어 및 민첩성에 대한 보안 목표를 충족하도록 보장 운영 관점: 비즈니스 이해당사자와 합의된 수준까지 IT 워크로드를 구현, 실행, 사용, 운영 및 복구하는 데 도움 마이그레이션 전략 6가지 마이그레이션 전략 리호스팅(Rehosting): 애플리케이션을 변경 없이 이전 리플랫포밍(Replatforming): 몇 가지 클라우드 최적화 수행. 최적화는 애플리케이션의 핵심 아키텍처를 변경하지 않고 달성 리팩터링(Refactoring)/아키텍처 재설계(Re-architecting): 클라우드 네이티브 기능을 사용해 애플리케이션을 설계하고 개발하는 방식을 재구성 다른 방법으로는 기존 환경의 애플리케이션에서 실현하기 까다로운 기능 추가, 확장 또는 성능 개선의 필요성이 클 때 활용 재구매(Repurchasing): 기존 라이선스를 SaaS 모델로 전환 유지(Retaining): 비즈니스에 중요한 애플리케이션을 소스 환경에 유지 마이그레이션하려면 대규모 리팩토링이 필요한 애플리케이션 또는 이후로 연기할 수 있는 워크로드 폐기(Retiring): 더 이상 필요하지 않은 애플리케이션을 제거 AWS Snow 패밀리 AWS와 고객 간 최대 엑사바이트 규모의 데이터를 물리적으로 이동할 수 있는 물리적 디바이스 모음\n각 디바이스는 각각 다른 용량 포인트를 제공, 대부분 내장 컴퓨팅 기능 포함 AWS는 Snow 패밀리 디바이스를 소유 및 관리하고 보안, 모니터링, 스토리지 관리 및 컴퓨팅 기능과 통합 AWS Snowcone 작고 견고하며 안전한 엣지 컴퓨팅 및 데이터 전송 디바이스 CPU 2개, 4GB 메모리 및 8TB 가용 스토리지 AWS Snowball Snowball Edge Storage Optimized: 대규모 데이터 마이그레이션 및 반복 전송 워크플로뿐 아니라 큰 용량이 필요한 로컬 컴퓨팅에 적합 스토리지: 블록 볼륨 및 S3 호환을 위한 80TB HDD, 블록 볼륨을 위한 1TB의 SSD 컴퓨팅: EC2 sbe1 인스턴스(C5와 동등)를 지원하기 위한 40개의 vCPU와 80GiB의 메모리 Snowball Edge Compute Optimized: 기계 학습, 풀 모션 비디오 분석, 분석 및 로컬 컴퓨팅 스택과 같은 사용 사례에 적합 스토리지: 블록 볼륨 및 S3 호환을 위한 42TB HDD, EBS 호환 블록 볼륨을 위한 7.68TB의 가용 SSD 컴퓨팅: 52개의 vCPU, 208GiB 메모리 및 NVIDIA Tesla V100 GPU 옵션, 디바이스는 C5, M5a, G3 및 P3 인스턴스와 동등한 EC2 sbe-c 및 sbe-g 인스턴스 실행 AWS Snowmobile 대용량 데이터를 AWS에 이동하는 데 사용하는 엑사바이트 규모의 데이터 전송 서비스 세미 트레일러 트럭으로 견인되며, 1대당 최대 100페타바이트의 데이터 전송 가능 AWS를 통한 혁신 현재 상태, 원하는 상태, 해결하려는 문제를 명확히 설명할 수 있다면 클라우드에서 혁신을 추진할 수 있음 ","date":"2022-10-23T20:21:30+09:00","permalink":"https://Haebuk.github.io/p/aws-cloud-practitioner-%EB%A7%88%EC%9D%B4%EA%B7%B8%EB%A0%88%EC%9D%B4%EC%85%98-%EB%B0%8F-%ED%98%81%EC%8B%A0-%EC%A0%95%EB%A6%AC/","title":"[AWS Cloud Practitioner] 마이그레이션 및 혁신 정리"},{"content":"AWS 프리 티어 지정된 기간 동안 비용을 신경 쓸 필요 없이 특정 서비스 사용 가능\n제공되는 상품은 다음과 같은 세 가지 유형이 존재 상시 무료: 만료되지 않으며 모든 AWS고객에게 제공 12개월 무료: 처음 AWS에 가입한 날로부터 12개월간 무료 제공 평가판: 특정 서비스를 활성화한 날짜부터 시작 AWS 요금 개념 AWS 요금 적용 방식 실제 사용한 만큼만 지불 장기 계약 또는 복잡한 라이선스 없이 각 서비스에서 실제로 사용한 리소스의 양에 대해 정확히 지불 예약하는 경우 비용 감소 온디맨드 인스턴스 요금에 비해 상당한 할인을 제공 많이 사용할수록 볼륨 기반 할인으로 비용 감소 일부 서비스는 계층화된 요금을 제공 사용량이 증가함에 따라 점차 단위 비용 감소 AWS 요금 계산기 AWS 요금 계산기를 사용하면 AWS 서비스를 탐색하고 AWS 기반 사용 사례에 대한 비용 추정 가능 AWS 비용 추정을 정의된 그룹별 구성 가능 생성된 비용 추정을 저장하고 링크를 생성해 다른 사용자와 공유 가능 결제 대시보드 AWS 청구서를 결제하고, 사용량을 모니터링하고, 비용을 분석 및 제어할 수 있음\n이번 달 누계 금액을 지난 달과 비교, 현재 사용량을 기준으로 내달 사용량 예측 서비스별 월 누계 지출 확인 서비스별 프리 티어 사용량 확인 Cost Explorer에 액세스해 예산 생성 Savings Plans 구매 및 관리 AWS 비용 및 사용 보고서 게시 통합 결제 조직의 모든 AWS 계정에 대한 단일 청구서 발행 가능\n조직에 허용되는 최대 계정 수는 기본적으로 4개지만, 할당량 늘릴 수 있음 조직의 계정 전체에서 대량 할인 요금, Savings Plans 및 예약 인스턴스 공유 가능 AWS 예산 서비스 사용, 서비스 비용 및 인스턴스 예약을 계획할 수 있음\n정보가 하루에 세 번 업데이트됨 사용량이 예산 금액을 초과하거나 초과할 것으로 예상되면 알려주는 사용자 지정 알림 설정 가능 AWS Cost Explorer 시간 경과에 따라 AWS 비용 및 사용량을 시각화, 이해, 관리할 수 있는 도구\n발생 비용 기준 상위 5개 AWS 서비스의 비용 및 사용량에 대한 기본 보고서 포함 사용자 지정 필터 및 그룹을 적용해 데이터 분석 가능 AWS Support 플랜 Basic Support 모든 고객에게 무료 제공 백서, 설명서 및 지원 커뮤니티에 대한 액세스 포함 AWS에 결제 관련 질문 및 서비스 한도 증가에 대해 문의 가능 제한된 AWS Trusted Advisor 검사에 액세스 가능 사용자에게 영향을 줄 수 있는 이벤트가 발생할 때 알림 및 수정 지침을 제공하는 도구인 AWS Personal Health Dashboard 사용 가능 Developer Support 모범 사례 지침 액세스 클라이언트 측 진단 도구 액세스 AWS 제품, 기능 및 서비스를 함께 사용하는 방법에 대한 지침으로 구성된 빌딩 블록 아키텍처 지원 Business Support 특정 요구 사항을 가장 잘 지원해줄 수 있는 AWS 제품, 기능 및 서비스를 식별하기 위한 사용 사례 지침 액세스 모든 AWS Trusted Advisor 검사 일반적인 운영 체제 및 애플리케이션 스택 구성 요소와 같은 타사 소프트웨어에 대한 제한된 지원 Enterprise Support 회사의 특정 사용 사례 및 애플리케이션을 지원하기 위한 컨설팅 관계인 애플리케이션 아키텍처 지침 인프라 이벤트 관리 지원 기술 계정 관리자 지원 기술 지원 관리자 (TAM) 애플리케이션을 계획, 배포, 최적화할 때 TAM이 지속적으로 권장 사항, 아키텍처 검토를 제공 모든 AWS 서비스에 대한 전문 지식 제공 여러 서비스를 함께 효율적으로 사용하는 솔루션을 설계하는 과정 지원 AWS Marketplace Independent Software Vendor(ISV)의 소프트웨어가 포함된 디지털 카탈로그\nAWS에서 실행되는 소프트웨어를 검색하고 평가, 구매할 수 있음 산업 및 사용 사례별로 소프트웨어 솔루션 탐색 가능 ","date":"2022-10-23T17:27:30+09:00","permalink":"https://Haebuk.github.io/p/aws-cloud-practitioner-%EC%9A%94%EA%B8%88-%EB%B0%8F-%EC%A7%80%EC%9B%90-%EC%A0%95%EB%A6%AC/","title":"[AWS Cloud Practitioner] 요금 및 지원 정리"},{"content":"Amazon CloudWatch 다양한 지표를 모니터링 및 관리하고 해당 지표의 데이터를 기반으로 경보 작업을 구성할 수 있는 웹 서비스\n지표를 사용해 리소스의 데이터 포인트를 나타냄 AWS 서비스는 지표를 CloudWatch로 전송 CloudWatch가 이 지표를 사용해 시간 경과에 따라 어떻게 성능이 변화했는지 보여주는 그래프를 자동 생성 CloudWatch 경보 지표의 값이 미리 정의된 임계값을 상회 또는 하회할 경우 자동으로 작업을 수행 CloudWatch 대시보드 단일 위치에서 리소스에 대한 모든 지표에 액세스 가능 다양한 비즈니스 용도, 애플리케이션 또는 리소스에 따라 별도의 대시보드를 사용자 지정 가능 AWS CloudTrail 계정에 대한 API 호출을 기록\n기록되는 정보에는 API 호출자 ID, API 호출 시간, API 호출자의 소스 IP 주소 등이 포함 누군가 남긴 이동 경로(작업 로그)의 ‘추적’으로 생각하면 됨 애플리케이션 및 리소스에 대한 사용자 활동 및 API 호출의 전체 내역 열람 가능 일반적으로 API 호출 후 15분 이내에 업데이트 됨 CloudTrail Insights AWS 계정에서 비정상적인 API 활동을 자동으로 감지할 수 있음 AWS Trusted Advisor AWS 환경을 검사하고 AWS 모범 사례에 따라 실시간 권장 사항을 제시하는 웹 서비스\n비용 최적화, 성능, 보안, 내결함성, 서비스 한도라는 5개 범주에서 결과를 모범 사례와 비교 각 범주의 검사에 대해 권장 작업 목록 제공, AWS 모범 사례를 알아볼 수 있는 추가 자료 제공 AWS Trusted Advisor가 제공하는 권장 사항은 배포의 모든 단계에서도 유용 기존 애플리케이션 및 리소스를 지속적으로 개선하는데 활용 AWS Trusted Advisor 대시보드 위에서 언급한 5개 범주에서 완료된 검사 검토 가능 녹색 체크 표시: 문제가 감지되지 않은 항목 수 주황색 삼각형: 권장 조사 항목 수 빨간색 원: 권장 조치 수 ","date":"2022-10-23T16:05:30+09:00","permalink":"https://Haebuk.github.io/p/aws-cloud-practitioner-%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81-%EB%B0%8F-%EB%B6%84%EC%84%9D-%EC%A0%95%EB%A6%AC/","title":"[AWS Cloud Practitioner] 모니터링 및 분석 정리"},{"content":"공동 책임 모델 리소를 안전하게 유지할 책임은 AWS와 고객 둘 다 있음\nAWS 환경을 단일 객체로 취급하지 않기 때문 공동 책임 모델은 고객 책임(일반적으로 ‘클라우드 내부의 보안’)과 AWS 책임(일반적으로 ‘클라우드 자체의 보안’)으로 분류됨 고객: 클라우드 내부의 보안 클라우드 내에서 생성하고 배치하는 모든 것의 보안 책임 저장하는 콘텐츠, 사용하는 AWS 서비스, 액세스 사용자 및 콘텐츠 보안 요구 사항 같은 영역 포함 AWS: 클라우드 자체의 보안 인프라의 모든 계층에서 구성 요소를 운영, 관리 및 제어 호스트 운영 체제, 가상화 계층, 데이터 센터의 물리적 보안 같은 영역 포함 리소스를 호스팅하는 물리적 인프라 관리 고객이 AWS 데이터 센터를 방문해 확인할 수는 없지만 외부 감사 기관이 작성한 여러 보고서를 확인할 수는 있음 사용자 권한 및 액세스 Identity and Access Management(IAM) AWS 서비스와 리소스에 대한 액세스를 안전하게 관리\n회사의 고유한 운영 및 보안 요구 사항에 따라 액세스 권한을 구성할 수 있는 유연성 제공 다음과 같은 IAM 기능을 조합하여 사용 IAM 사용자, 그룹 및 역할 IAM 정책 Multi-Factor Authentication AWS 계정 루트 사용자 AWS 계정을 처음 만들면 루트 사용자라는 자격 증명으로 시작 모든 AWS 서비스 및 리소스에 대한 전체 액세스 권한 가짐 일상 작업에서는 루트 사용자를 사용하지 말 것 루트 사용자로 첫 번째 IAM 사용자 생성 후, 이 사용자에게 다른 사용자를 생성할 수 있는 권한 할당 루트 사용자만 사용할 수 있는 제한된 종류의 작업(루트 사용자 이메일 변경, AWS Support Plan 변경)에만 사용 IAM 사용자 사용자가 AWS에서 생성하는 자격 증명 AWS 서비스 및 리소스와 상호 작용하는 사람 또는 애플리케이션 IAM 사용자가 특정 작업을 수행할 수 있도록 허용하려면 IAM 사용자에게 필요한 권한을 부여 AWS에 액세스하는 사용자별로 개별 IAM 사용자를 생성하는 것이 좋음 IAM 정책 AWS 서비스 및 리소스에 대한 권한을 허용하거나 거부하는 문서 사용자가 리소스에 액세스할 수 있는 수준을 지정 가능 권한 부여 시 최소 권한 보안 ********************************원칙을 따를 것 IAM 그룹 IAM 사용자 모음 그룹에 IAM 정책을 할당하면 해당 그룹의 모든 사용자에게 정책에 지정된 권한이 부여됨 IAM 역할 임시로 권한에 액세스하기 위해 수임할 수 있는 자격 증명 서비스 또는 리소스에 대해 액세스 권한을 장기적이 아닌 일시적으로 부여하는 상황에 이상적 Multi-Factor Authentication AWS 계정에 추가 보안 계층 제공 루트 사용자 및 계정 내 모든 IAM 사용자에 대해 MFA를 활성화하는 것이 가장 좋음 AWS Organizations 중앙 위치에서 여러 AWS 계정을 통합하고 관리할 수 있음\n조직 생성 시 AWS Organizations가 조직 모든 계정에 대한 상위 컨테이너 루트 자동 생성 서비스 제어 정책(SCP)을 사용해 조직의 계정에 대한 권한을 중앙에서 제어 가능 개별 멤버 계정 및 조직 단위(OU)에 적용 각 계정의 사용자 및 역할이 액세스할 수 있는 AWS 서비스, 리소스 및 개별 API 작업 제한 가능 조직 단위 AWS Organizations에서는 계정을 조직 단위(OU)로 그룹화해 비슷한 비즈니스 또는 보안 요구 사항이 있는 계정을 손쉽게 관리할 수 있음 OU에 정책을 적용하면 OU에 모든 계정이 정책에 지정된 권한을 자동으로 상속 개별 계정을 OU로 구성하면 특정 보안 요구 사항이 있는 워크로드 또는 애플리케이션을 보다 간편하게 격리 가능 규정 준수 AWS Artifact AWS 보안 및 규정 준수 보고서 및 일부 온라인 계약에 대한 온디맨드 액세스를 제공하는 서비스\nAWS Artifact Agreements와 AWS Artifact Reports 두 가지 기본 섹션으로 구성 AWS Artifact Agreements 회사에서 AWS 서비스 전체에서 특정 유형의 정보를 사용하기 위해 AWS와 계약 체결 할 경우 개별 계정 및 AWS Organizations 내 모든 계정에 대한 계약을 검토, 수락 및 관리 가능 특정 규정의 적용을 받는 고객의 니즈를 해결하기 위한 다양한 유형의 계약 제공 AWS Artifact Reports 회사의 팀원 중 한 명이 특정 규제 표준을 준수하기 위한 책임에 대한 추가 정보가 필요할 경우 외부 감사 기관이 작성한 규정 준수 보고서 제공 감사 또는 규제 기관에 AWS 보안 제어 항목의 증거로서 AWS 감사 아티팩트를 제공하면 됨 고객 규정 준수 센터 AWS 규정 준수에 대해 자세히 알아볼 수 있는 리소스가 포함\n고객 규정 준수 사례를 읽고 규제 대상 업종의 기업들이 다양한 규정 준수, 거버넌스 및 감사 과제를 어떻게 해결했는지 확인 가능 다음과 같은 주제에 액세스 가능 주요 규정 준수 질문에 대한 AWS 답변 AWS 위험 및 규정 준수 개요 보안 감사 체크리스트 감사자 학습 경로 포함 내부 운영에서 AWS 클라우드를 사용한 규정 준수를 입증할 수 있는 방법을 자세히 알아보려는 감사, 규정 준수 및 법무 담당자를 위해 고안 서비스 거부 공격 서비스 거부 공격(DoS) 사용자들이 웹 사이트 또는 애플리케이션을 이용할 수 없게 만드려는 의도적인 시도\n공격이 단일 소스로부터 발생 분산 서비스 거부(DDoS) 공격 여러 소스를 사용해 웹 사이트 또는 애플리케이션을 사용할 수 없게 만드는 공격\n감염된 여러 컴퓨터(봇)를 사용해 과도한 트래픽을 웹 사이트 또는 애플리케이션으로 전송할 수 있음 AWS Shield DDoS 공격으로부터 애플리케이션을 보호하는 서비스\nAWS Shield Standard 모든 AWS 고객을 자동으로 보호하는 무료 서비스 가장 자주 발생하는 일반적인 DDoS 공격으로부터 보호 네트워크 트래픽이 애플리케이션으로 들어오면 다양한 분석 기법을 사용해 실시간으로 악성 트래픽을 탐지하고 자동으로 완화 AWS Shield Advanced 상세한 공격 진단 및 정교한 DDoS 공격을 탐지하고 완화할 수 있는 기능을 제공하는 유료 서비스 CloudFront, Route 53, ELB과 같은 다른 서비스와도 통합 가능 복잡한 DDoS 공격을 완화하기 위한 사용자 지정 규칙을 작성해 AWS Shield를 AWS WAF와 통합 가능 추가 보안 서비스 AWS Key Management Service(KMS) 암호화 키를 사용해 암호화 작업 수행\n암호화 키는 데이터 잠금(암호화) 및 잠금 해제(암호 해독)에 사용되는 임의의 숫자 문자열 암호화 키를 생성, 관리 및 사용 가능 광범위한 서비스 및 애플리케이션에서 키 사용 제어 가능 키에 필요한 액세스 제어를 특정 수준으로 선택 가능 키를 관리하는 IAM 사용자 및 역할 지정 가능 더 이상 사용되지 않도록 비활성화 가능 AWS WAF 웹 애플리케이션으로 들어오는 네트워크 요청을 모니터링할 수 있는 웹 애플리케이션 방화벽\nCloudFront 및 ALB와 함께 작동 NACL과 유사한 방식으로 트래픽을 차단 및 허용하나, AWS 리소스를 보호하기 위해 웹 ACL을 사용 요청이 차단된 IP 주소 중 하나에서 나온 것이 아니면 애플리케이션에 대한 액세스 허용 Amazon Inspector 자동화된 보안 평가를 실행해 애플리케이션의 보안 및 규정 준수를 개선할 수 있는 서비스\nEC2 인스턴스에 대한 오픈 액세스, 취약한 소프트웨어 버전 설치와 같은 보안 모범 사례 위반 및 보안 취약성을 애플리케이션에서 검사 심각도 수준에 따라 우선 순위 결정, 각 문제에 대한 자세한 설명 및 권장 해결 방법 포함 그러나 제공된 권장 사항으로 모든 잠재적 문제가 해결됨을 AWS는 보장하지 않음 공동 책임 모델에 따라 고객은 AWS에서 실행되는 애플리케이션, 프로세스 및 도구의 보안에 대한 책임이 있기 때문 Amazon GuardDuty AWS 인프라 및 리소스에 대한 지능형 위협 탐지 기능을 제공하는 서비스\nAWS 환경 내의 네트워크 활동 및 계정 동작을 지속적으로 모니터링하여 위협 식별 추가 보안 소프트웨어를 배포하거나 관리할 필요 없음 VPC Flow Logs 및 DNS 로그를 비롯한 여러 AWS 소스의 데이터를 지속적으로 분석 위협을 탐지한 경우 AWS Management Console에서 위협에 대한 자세한 탐지 결과 검토 가능 문제 해결을 위한 권장 단계 포함 탐지 결과에 대한 응답으로 자동으로 문제 해결 단계를 수행하도록 Lambda 함수 구성도 가능 ","date":"2022-10-23T14:55:30+09:00","permalink":"https://Haebuk.github.io/p/aws-cloud-practitioner-%EB%B3%B4%EC%95%88-%EC%A0%95%EB%A6%AC/","title":"[AWS Cloud Practitioner] 보안 정리"},{"content":"인스턴스 스토어 및 Elastic Block Store(EBS) 인스턴스 스토어 EC2 인스턴스에 임시 블록 수준 스토리지 제공\n물리적으로 EC2 인스턴스의 호스트 컴퓨터와 연결 인스턴스와 수명이 동일 인스턴스가 종료되면 인스턴스 스토어의 데이터 손실 장기적으로 필요하지 않은 임시 데이터가 포함된 사용 사례에 적합 Elastic Block Store(EBS) EC2 인스턴스에서 사용할 수 있는 블록 수준 스토리지 볼륨을 제공하는 서비스\nEC2 인스턴스를 중지 또는 종료해도 연결된 EBS 볼륨의 모든 데이터 사용 가능 EBS 볼륨을 생성하려면 구성을 정의하고 프로비저닝 EBS 볼륨 생성 후 볼륨을 EC2 인스턴스에 연결할 수 있음 EBS 스냅샷을 통해 EBS 볼륨을 증분 백업할 수 있음 EBS 스냅샷 증분 백업\n처음 볼륨을 백업하면 모든 데이터가 복사 이후의 백업은 가장 최근 스냅샷 이후의 변경된 데이터 블록만 저장 Simple Storage Service(S3) 객체 스토리지 각 객체는 데이터, 메타데이터, 키로 구성 데이터: 이미지, 동영상 또는 기타 유형의 파일 메타데이터: 데이터의 내용, 사용 방법, 객체 크기 등에 대한 정보 키: 고유 식별자 블록 스토리지와 달리 파일 수정 시 전체 개체가 업데이트 Simple Storage Service(S3) 객체 수준 스토리지를 제공하는 서비스\n데이터를 버킷에 객체로 저장 저장 공간 무제한 저장할 수 있는 객체의 최대 파일 크기는 5TB 파일 업로드 시 권한 설정하여 파일에 대한 표시 여부 및 액세스 제어 가능 버전 관리 기능을 사용해 시간 경과에 따른 객체 변경 사항 추적 가능 S3 스토리지 클래스 비즈니스 및 비용 요구 사항에 맞춰 다양한 스토리지 클래스 중 선택 가능\n데이터 검색 빈도와 필요한 데이터 가용성을 고려 클래스 종류 S3 Standard 자주 액세스하는 데이터 최소 3개의 가용 영역에 데이터 저장 다른 스토리지 클래스보다 비용 높음 S3 Standard-Infrequent Access(S3 Standard-IA) 자주 액세스하지 않는 데이터 최소 3개의 가용 영역에 데이터 저장 S3 Standard에 비해 스토리지 가격 저렴, 검색 가격 높음 S3 One Zone-Infrequent Access(S3 One Zone-IA) 단일 가용 영역에 데이터 저장 S3 Standard-IA보다 낮은 스토리지 가격 스토리지 비용 절감, 가용 영역 장애 발생 시 데이터를 손쉽게 재현할 수 있는 경우 적합 S3 Intelligent-Tiering 액세스 패턴을 알 수 없거나 자주 변화하는 데이터에 적합 객체당 소량의 월별 모니터링 및 자동화 요금 부과 사용자가 30일 연속 객체에 액세스하지 않으면 자동으로 해당 객체를 S3 Standard-IA로 이동 이 객체에 다시 액세스하면 자동으로 S3 Standard로 이동 S3 Glacier 데이터 보관용으로 설계된 저비용 스토리지 객체를 몇 분에서 몇 시간 이내에 검색 S3 Glacier Deep Archive 보관에 이상적인 가장 저렴한 객체 스토리지 클래스 객체를 12시간 내에 검색 얼마나 빨리 검색해야 하는지 고려하여 두 Glacier 서비스 간에 어떤 서비스 사용할 지 결정 Elastic File System(EFS) 파일 스토리지 여러 클라이언트가 공유 파일 폴더에 저장된 데이터에 액세스 가능 스토리지 서버가 블록 스토리지를 로컬 파일 시스템과 함께 사용하여 파일 구성 클라이언트는 파일 경로를 통해 데이터에 액세스 많은 수의 서비스 및 리소스가 동시에 동일한 데이터에 액세스해야 하는 사례에 적합 파일을 추가 또는 제거하면 EFS는 자동으로 확장되거나 축소됨 애플리케이션을 중단하지 않고 온디맨드로 페타바이트 규모로 확장 가능 EBS와 EFS 비교 EBS 단일 가용 영역에 데이터 저장 EC2 인스턴스를 EBS 볼륨에 연결하려면 둘 다 동일한 가용 영역에 상주해야 함 EFS 리전별 서비스. 여러 가용 영역에 걸쳐 데이터 저장 온프레미스 서버는 AWS Direct Connect를 사용해 EFS에 액세스 가능 Relational Database Service(RDS) AWS 클라우드에서 관계형 데이터베이스를 실행할 수 있는 서비스\n하드웨어 프로비저닝, 데이터베이스 설정, 패치 적용 백업과 같은 작업을 자동화 Lambda를 사용하여 서버리스 애플리케이션에서 데이터베이스를 쿼리하는 등 다른 서비스와 통합하여 비즈니스 및 운영 요구 사항 충족 가능 다양한 보안 옵션 제공 저장 시 암호화(데이터가 저장되는 동안 데이터 보호) 전송 중 암호화(데이터를 전송 및 수신하는 동안 데이터 보호) RDS 데이터베이스 엔진 메모리, 성능 또는 입출력에 최적화된 6개의 데이터베이스 엔진에서 사용 가능 Amazon Aurora PostgreSQL MySQL MariaDB Oracle Database Microsoft SQL Server Amazon Aurora 엔터프라이즈급 관계형 데이터 베이스\nMySQL, PostgreSQL 관계형 데이터베이스와 호환 표준 MySQL 데이터베이스보다 최대 5배 빠르며 표준 PostgreSQL 데이터베이스보다 최대 3배 빠름 데이터베이스 리소스의 안정성 및 가용성을 유지하면서 불필요한 입출력 작업을 줄여 데이터베이스 비용 절감 워크로드에 고가용성이 필요한 경우 적합 6개의 데이터 복사본을 3개의 가용 영역에 복제하고 지속적으로 S3에 백업함 DynamoDB 키-값 데이터베이스 서비스\n모든 규모에서 한 자릿수 밀리초 성능 제공 서버리스 데이터베이스 데이터베이스 크기가 축소 또는 확장되면 용량 변화에 맞춰 자동으로 크기를 조정하면서 일관된 성능 유지 크기를 조정하는 동안 고성능이 필요한 경우에 적합 Redshift 빅 데이터 분석에 사용할 수 있는 데이터 웨어하우징 서비스\n여러 원본에서 데이터를 수집해 데이터 간 관계 및 추세를 파악하는데 도움이 되는 기능 제공 AWS Database Migration Service(DMS) 관계형 데이터베이스, 비관계형 데이터베이스 및 기타 유형의 데이터 저장소를 마이그레이션 하는 서비스\nDMS 를 사용하면 원본 데이터베이스와 대상 데이터베이스 간에 데이터 이동 가능 원본 데이터베이스와 대상 데이터베이스는 유형이 같을 필요 없음 마이그레이션동안 원본 데이터베이스가 계속 작동하므로 다운타임 최소화 DMS 사용 사례 개발 및 테스트 데이터베이스 마이그레이션: 프로덕션 사용자에게 영향을 주지 않고 개발자가 프로덕션 데이터에서 테스트할 수 있도록 지원 데이터베이스 통합: 여러 데이터베이스를 단일 데이터베이스로 통합 연속 복제: 일회성 마이그레이션이 아닌 데이터의 진행 중 복제본을 다른 대상 원본으로 전송 추가 데이터베이스 서비스 DocumentDB MongoDB 워크로드를 지원하는 문서 데이터베이스 서비스 Neptune 그래프 데이터베이스 서비스 추천 엔진, 사기 탐지, 지식 그래프와 같이 고도로 연결된 데이터 세트로 작동하는 애플리케이션 빌드 및 실행 Quantum Ledger Database(QLDB) 원장 데이터베이스 서비스 애플리케이션 데이터에 발생한 모든 변경 사항의 전체 기록 검토 가능 Managed Blockchain 오픈소스 프레임워크를 사용해 블록체인 네트워크를 생성하고 관리하는 데 사용할 수 있는 서비스 블록체인은 여러 당사자가 중앙 기관없이 거래를 실행하고 데이터를 공유할 수 있는 분산형 원장 시스템 ElastiCache 자주 사용되는 요청의 읽기 시간을 향상시키기 위해 데이터베이스 위에 캐싱 계층을 추가하는 서비스 Redis, Memcached 지원 DynamoDB Accelerator(DAX) DynamoDB용 인 메모리 캐시 응답 시간을 한 자릿수 밀리초에서 마이크로초까지 향상 가능 ","date":"2022-10-22T22:53:39+09:00","permalink":"https://Haebuk.github.io/p/aws-cloud-practitioner-%EC%8A%A4%ED%86%A0%EB%A6%AC%EC%A7%80-%EB%B0%8F-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B2%A0%EC%9D%B4%EC%8A%A4-%EC%A0%95%EB%A6%AC/","title":"[AWS Cloud Practitioner] 스토리지 및 데이터베이스 정리"},{"content":"AWS와의 연결 Virtual Private Cloud(VPC) AWS 리소스에 경계를 설정하는 데 사용할 수 있는 네트워킹 서비스\nVPC를 사용해 AWS 클라우드의 격리된 섹션을 프로비저닝할 수 있음 한 VPC 내에서 여러 서브넷으로 리소스를 구성할 수 있음 서브넷: 리소스를 포함할 수 있는 VPC 섹션 인터넷 게이트웨이 인터넷의 퍼블릭 트래픽이 VPC에 액세스하도록 허용하려면 인터넷 게이트웨이를 VPC에 연결해야 함 인터넷 게이트웨이가 없으면 아무도 VPC내의 리소스에 접근 불가 가상 프라이빗 게이트웨이 VPC내의 비공개 리소스에 액세스하려면 가상 프라이빗 게이트웨이를 사용 보호된 인터넷 트래픽이 VPC로 들어오도록 허용하는 구성 요소 다른 트래픽과 동일한 네트워크를 사용하기 때문에 체증 발생 가능 VPC와 프라이빗 네트워크간에 가상 프라이빗 네트워크(VPN) 연결 설정 가능 승인된 네트워크에서 나오는 트래픽만 VPC로 들어가도록 허용 AWS Direct Connect 데이터 센터와 VPC간에 비공개 전용 연결을 설정하는 서비스 다른 트래픽과 네트워크를 공유하지 않고 바로(Direct) 연결 네트워크 비용을 절감하고 대역폭을 놀리는데 도움이 됨 서브넷 및 네트워크 액세스 제어 목록 서브넷 보안 또는 운영 요구 사항에 따라 리소스를 그룹화할 수 있는 VPC 내의 한 섹션\n서브넷은 퍼블릭 또는 프라이빗일 수 있음 퍼블릭 서브넷: 누구나 액세스할 수 있어야 하는 리소스가 포함 프라이빗 서브넷: 데이터베이스와 같이 프라이빗 네트워크를 통해서만 액세스할 수 있는 리소스가 포함 VPC내에서 서브넷은 서로 통신할 수 있음. 예: 퍼블릭 서브넷에 있는 EC2 인스턴스가 프라이빗 서브넷에 있는 데이터베이스와 통신 가능 VPC의 네트워크 트래픽 AWS 클라우드에서 호스팅되는 애플리케이션에 데이터를 요청하면 이 요청은 패킷으로 전송됨 패킷: 인터넷이나 네트워크를 통해 전송되는 데이터의 단위 패킷은 인터넷 게이트웨이를 통해 VPC로 들어간다. 패킷이 서브넷으로 들어가거나 나오려면 먼저 권한을 확인해야 함 이를 확인하는 VPC 구성 요소는 네트워크 ACL(액세스 제어 목록)임 네트워크 ACL(액세스 제어 목록) 서브넷 수준에서 인바운드 및 아웃바운드 트래픽을 제어하는 가상 방화벽\n각 AWS 계정에는 기본 네트워크 ACL이 포함 VPC를 구성할 때 계정의 기본 기본 네트워크 ACL을 사용하거나 사용자 지정 네트워크 ACL을 생성 가능 계정의 기본 기본 네트워크 ACL: 기본적으로 모든 인바운드 및 아웃바운드 트래픽 허용 사용자 자체 규칙 추가 수정 가능 사용자 지정 네트워크 ACL: 사용자가 허용할 트래픽을 지정하는 규칙을 추가할 때까지 모든 인바운드 및 아웃바운드 트래픽 거부 모든 네트워크 ACL에는 명시적 거부 규칙 존재 패킷이 목록의 다른 모든 규칙과 일치하지 않으면 해당 패킷 거부 상태 비저장 패킷 필터링 네트워크 ACL은 상태 비저장 패킷 필터링을 수행 아무것도 기억하지 않음. 각 방향(인바운드, 아웃바운드)으로 서브넷 경계를 통과하는 패킷만 확인 해당 요청에 대한 패킷이 서브넷으로 반환될 때 네트워크 ACL은 이전 요청 기억하지 못함 네트워크 ACL은 규칙 목록에 따라 패킷 응답 확인 → 허용 또는 거부 결정 패킷이 서브넷에 들어간 후에는 서브넷 내 리소스(예:EC2 인스턴스)에 대한 권한이 평가되어야 함 보안 그룹 Amazon EC2 인스턴스에 대한 인바운드 및 아웃바운드 트래픽을 제어하는 가상 방화벽\n기본적으로 보안 그룹은 모든 인바운드 트래픽 거부, 아웃바운드 트래픽 허용 상태 저장 패킷 필터링 보안 그룹은 상태 저장 패킷 필터링을 수행 들어오는 패킷에 대한 이전 결정을 기억 해당 요청에 대한 패킷 응답이 인스턴스로 반환될 때 보안 그룹이 이전 요청을 기억 보안 그룹은 인바운드 보안 그룹 규칙에 관계 없이 응답이 진행하도록 허용 글로벌 네트워킹 Route 53 DNS 웹 서비스\n개발자와 비즈니스가 최종 사용자를 AWS에서 호스팅되는 인터넷 어플리케이션으로 라우팅할 수 있는 안정적인 방법 제공 사용자 요청을 AWS에서 실행되는 인프라(예: EC2 or ELB)에 연결 사용자를 AWS 외부 인프라로 라우팅 가능 도메인 이름의 DNS 레코드 관리하는 기능도 존재 새 도메인 이름을 직접 등록 가능 다른 도메인 등록 대행자가 관리하는 기존 도메인 이름의 DNS 레코드 전송 가능 Route 53 및 CloudFront가 콘텐츠를 전송하는 방식 고객이 웹 사이트로 이동하여 애플리케이션에서 데이터 요청 Route53은 DNS 확인을 사용해 IP를 식별, 이 정보는 고객에게 다시 전송 고객의 요청은 CloudFront를 통해 가장 가까운 엣지 로케이션으로 전송 CloudFront는 수신 패킷을 EC2 인스턴스로 전송하는 Application Load Balancer(ALB)에 연결 ","date":"2022-10-22T21:12:39+09:00","permalink":"https://Haebuk.github.io/p/aws-cloud-practitioner-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%82%B9-%EC%A0%95%EB%A6%AC/","title":"[AWS Cloud Practitioner] 네트워킹 정리"},{"content":"AWS 글로벌 인프라 AWS 리전 적합한 리전을 결정할 때 다음 네 가지 요소를 고려해야 함\n1. 데이터 거버넌스 및 법적 요구 사항 준수 회사와 위치에 따라 특정 영역에서 데이터를 실행해야 하는 경우 모든 회사에 위치 기반 데이터 규정이 있는 것은 아니므로 다음 세 가지 요소에 더 집중해야 할 수도 있음 2. 고객과의 근접성 고객과 가까운 리전을 선택하면 콘텐츠를 더 빠르게 제공할 수 있음 3. 리전 내에서 사용 가능한 서비스 고객에게 제공하려는 기능이 가장 가까운 리전에 없을수도 있음 제공하려면 이 서비스를 제공하는 리전 중 하나에서 실행해야 함 4. 요금 리전마다 비용이 다를 수 있음 가용 영역 리전 내의 단일 데이터 센터 또는 데이터 센터 그룹 가용 영역은 서로 수십 마일 떨어져 있음 가용 영역 간 지연 시간은 짧을 정도로 충분히 가까우면서 한 곳에서 재해가 발생할 경우 다른 가용 영역이 영향을 받을 가능성을 줄일 만큼 멀리 떨어져 있음 엣지 로케이션 CloudFront 글로벌 콘텐츠 전송 서비스\n더 빠른 콘텐츠 전송을 위해 고객과 가까운 위치에 콘텐츠 사본을 캐시하는 데 사용하는 사이트 본사는 브라질, 고객은 중국에 있는 경우 브라질에 있는 모든 콘텐츠를 중국 리전중 하나로 이동할 필요 없음 중국 내 고객과 가까운 엣지 로케이션에 사본을 로컬로 캐시 중국 내 고객이 파일을 요청하면 엣지 로케이션의 캐시에서 해당 파일을 검색하여 고객에게 전송 → 원본이 아닌 중국 근처에서 가져온 것이므로 더 빠르게 전달 AWS 리소스를 프로비저닝하는 방법 AWS 서비스와 상호 작용하는 방법 AWS Management Console 웹 기반 인터페이스 콘솔 모바일 애플리케이션을 사용해 리소스 모니터링, 경보 보기, 결제 정보 확인 등의 작업 수행 가능 여러 ID가 동시에 AWS 콘솔 모바일 앱에 로그인 가능 AWS 명령줄 인터페이스 명령줄에서 여러 AWS 서비스 제어 가능 스크립트를 통해 서비스 및 애플리케이션이 수행하는 작업을 자동화할 수 있음 소프트웨어 개발 키트 프로그래밍 언어 또는 플랫폼용으로 설계된 API를 통해 보다 서비스를 간편하게 사용 가능 Elastic Beanstalk 사용자가 코드 및 구성 설정을 제공하면 다음 작업을 수행하는 데 필요한 리소스 배포\n용량 조정 로드 밸런싱 자동 조정 애플리케이션 상태 모니터링 CloudFormation 인프라를 코드로 취급할 수 있음\n리소스를 안전하고 반복 가능한 방식으로 프로비저닝하므로 수작업을 수행하거나 사용자 지정 스크립트를 작성할 필요 없이 인프라 및 애플리케이션을 빈번히 구축할 수 있음 스택을 관리할 때 수행해야 할 적절한 작업을 결정하고 오류를 감지하면 변경 사항을 자동으로 롤백함 ","date":"2022-10-22T17:21:39+09:00","permalink":"https://Haebuk.github.io/p/aws-cloud-practitioner-%EA%B8%80%EB%A1%9C%EB%B2%8C-%EC%9D%B8%ED%94%84%EB%9D%BC-%EB%B0%8F-%EC%95%88%EC%A0%95%EC%84%B1-%EC%A0%95%EB%A6%AC/","title":"[AWS Cloud Practitioner] 글로벌 인프라 및 안정성 정리"},{"content":"클라우드 컴퓨팅 💡 인터넷을 통해 IT 리소스와 애플리케이션을 온디맨드로 제공하는 것\n클라우드 컴퓨팅 배포 모델 클라우드 기반 배포 모든 애플리케이션을 클라우드에서 실행 기존 애플리케이션을 클라우드로 마이그레이션 클라우드에서 새 애플리케이션 설계 및 빌드 온프레미스 배포(프라이빗 클라우드 배포) 가상화 및 리소스 관리 도구를 사용하여 리소스 배포 애플리케이션 관리 및 가상화 기술을 사용하여 리소스 활용도 높임 하이브리드 배포 클라우드 기반 리소스를 온프레미스 인프라에 연결 클라우드 기반 리소스를 레거시 IT 애플리케이션과 통합 이점 선행 비용을 가변 비용으로 대체 데이터 센터 운영 및 유지 관리에 비용 투자 불필요 용량 추정 불필요 규모의 경제로 얻게 되는 이점 속도 및 민첩성 향상 몇 분 만에 전 세계 배포 Elastic Compute Cloud(EC2) 안전하고 크기 조정이 가능한 컴퓨팅 용량을 클라우드에서 제공 EC2 작동 방식 시작 인스턴스 시작 기본 구성(운영 체제, 애플리케이션 서버 또는 애플리케이션) 인스턴스가 포함되어 있는 템플릿을 선택하여 시작 인스턴스 유형 선택 보안 설정 지정 연결 여러가지 방법으로 연결 가능 사용 명령을 실행하여 소프트웨어 설치, 스토리지 추가, 파일 복사 및 정리 등의 작업 수행 EC2 인스턴스 유형 유형 선택시 워크로드 및 애플리케이션의 구체적 요구 사항을 고려해야 함 범용 인스턴스 컴퓨팅, 메모리, 네트워킹 균형 제공 애플리케이션, 게임, 엔터프라이즈 애플리케이션용 백엔드 서버, 중소 규모 데이터베이스 컴퓨팅 최적화 인스턴스 고성능 프로세서를 활용하는 컴퓨팅 집약적인 애플리케이션에 적합 고성능 웹서버, 컴퓨팅 집약적 애플리케이션 서버, 게임 전용 서버에 적합 단일 그룹에서 많은 트랜잭션을 처리해야 하는 일괄 처리 워크로드에 사용 메모리 최적화 인스턴스 메모리에서 대규모 데이터 셋을 처리하는 워크로드를 위한 빠른 성능을 제공 많은 데이터를 미리 로드해야 하는 워크로드에 적합 고성능 데이터베이스 엑셀러레이티드 컴퓨팅 인스턴스 하드웨어 엑셀러레이터 또는 코프로세서를 사용하여 일부 기능을 CPU보다 더 효율적으로 수행 부동 소수점 수 계산, 그래픽 처리, 데이터 패턴 일치 등 데이터 처리를 가속화 할 수 있는 구성 요소 그래픽 애플리케이션, 게임 스트리밍, 애플리케이션 스트리밍에 적합 스토리지 최적화 인스턴스 로컬 스토리지의 대규모 데이터 셋에 대한 순차적 읽기 및 쓰기 액세스가 많이 필요한 워크로드에 적합 분산 파일 시스템, 데이터 웨어하우징 애플리케이션, OLTP 시스템 짧은 임의 IOPS(초당 입출력 작업 수) 제공 EC2 요금 온디맨드 중단할 수 없는 불규칙한 단기 워크로드가 있는 애플리케이션에 적합 선결제 비용, 최소 약정은 적용되지 않음 개발 및 테스트와 예측할 수 없는 사용 패턴이 있는 애플리케이션 실행 사례에 적합 1년 이상 지속되는 워크로드에는 권장되지 않음 Savings Plans 1년또는 3년 기간 동안 일정한 컴퓨팅 사용량을 약정하여 컴퓨팅 비용 절감 (최대 72%) 약정 사용량까지는 할인된 Savings Plan 요금 청구. 초과 시 일반 온디맨드 요금 부과 AWS Cost Explorer를 통해 EC2 사용량 분석 가능, Savings Plans를 위한 맞춤형 권장 사항 제공 예약 인스턴스 온디맨드 인스턴스를 사용할 때 적용되는 결제 할인 옵션 표준 예약 및 컨버터블 예약 인스턴스는 1년 또는 3년 약정 정기 예약 인스턴스는 1년 약정 3년 약정 옵션을 통해 더 큰 비용 절감 실현 가능 약정 기간이 끝나면 중단없이 사용가능하나, 종료 또는 인스턴스 속성(유형, 리전, 테넌시, 플랫폼)과 일ㅊ하는 새 예약 인스턴스 구입 전 까지는 온디맨드 요금 부과 스팟 인스턴스 시작 및 종료 시간이 자유롭거나 중단을 견딜 수 있는 워크로드에 적합 미사용 EC2 컴퓨팅 용량을 사용하며 온디맨드 요금의 최대 90%까지 비용 절감 가능 스팟 요청을 하고 EC2 용량을 사용할 수 없는 경우 용량을 사용할 수 있을 때 까지 요청 성공 하지 않음 스팟 인스턴스 시작 후 용량을 더 이상 사용할 수 없거나 수요가 늘면 중단될 수 있음 전용 호스트 사용자 전용의 EC2 인스턴스 용량을 갖춘 물리적 서버 가장 비용이 많이 듦 EC2 확장(Auto Scaling) 변화하는 애플리케이션 수요에 따라 EC2 인스턴스를 자동으로 추가하거나 제거 → 가용성을 효과적으로 유지 동적 조정: 수요 변화에 대응 예측 조정: 예측된 수요에 따라 적절한 수의 EC2 인스턴스를 자동으로 예약 두 조정은 함께 사용하면 더 빠르게 조정할 수 있음 최소 용량: Auto Scaling 그룹을 생성한 직후 시작되는 EC2 인스턴스의 수 희망 용량: 유지되었으면 하는 EC2 인스턴스의 수. 지정하지 않으면 최소 용량이 기본값 최대 용량: Auto Scaling 그룹을 구성하되 EC2 인스턴스의 최대 수 제한 Elastic Load Balancing 들어오는 애플리케이션 트래픽을 Amazon EC2 인스턴스와 같은 여러 리소스에 자동으로 분산하는 서비스 Auto Scaling 그룹으로 들어오는 모든 웹 트래픽의 단일 접점 역할. 들어오는 트래픽 양에 맞춰 EC2 인스턴스를 추가하거나 제거하므로 요청이 로드 밸런서로 먼저 라우팅된 후 요청을 처리할 여러 리소스로 분산 메시징 및 대기열 Simple Notification Service(SNS) 게시/구독 서비스 게시자는 SNS 주제를 사용하며 구독자에게 메시지 게시 구독자는 웹 서버, 이메일 주소, Lambda 함수 등등의 옵션 가능 Simple Queue Service(SQS) 메시지 손실이나 다른 서비스 사용 없이 소프트웨어 구성 요소 간에 메시지를 전송, 저장, 수신할 수 있음 애플리케이션이 메시지를 대기열로 전송 사용자 또는 서비스는 대기열에서 메시지를 검색, 처리 후 대기열에서 삭제 추가 컴퓨팅 서비스 서버리스 컴퓨팅 서버를 프로비저닝하거나 관리할 필요가 없음 처리량 및 메모리와 같은 소비 단위를 수정하여 애플리케이션의 용량 조정 Lambda 컨테이너 애플리케이션의 코드와 종속성을 하나의 객체로 패키징하는 표준 방식 제공 보안성, 안정성, 확장성이 매우 중요한 프로세스 및 워크플로에도 사용 Elastic Container Service(ECS) 컨테이너식 애플리케이션을 실행하고 확장할 수 있는 고성능 컨테이너 관리 시스템 Elastic Kubernetes Service(EKS) AWS에서 쿠버네티스를 실행하는 데 사용할 수 있는 완전 관리형 서비스 Fargate 컨테이너용 서버리스 컴퓨팅 엔진 ECS와 EKS에서 작동 서버를 프로비저닝하거나 관리할 필요 없음 ","date":"2022-10-22T16:05:39+09:00","permalink":"https://Haebuk.github.io/p/aws-cloud-practitioner-%ED%81%B4%EB%9D%BC%EC%9A%B0%EB%93%9C-%EC%BB%B4%ED%93%A8%ED%8C%85-%EC%A0%95%EB%A6%AC/","title":"[AWS Cloud Practitioner] 클라우드 컴퓨팅 정리"},{"content":"What is Airflow? Core Components Web server: UI 담당 웹 서버 Scheduler: 워크플로우 스케줄링 Metastore: 메타데이터가 저장되는 데이터 베이스 Executor: 작업이 어디서 실행될지 정의 Worker: 작업이 실행되는 프로세스 DAG dag_id: 유니크한 dag 이름 start_date: dag가 처음 스케줄되는 시간. datetime 모듈로 정의 schedule_interval: 스케줄되는 간격. cron으로 정의하거나 \u0026quot;@daily\u0026quot;와 같이 정의 default_args owner: str, \u0026quot;airflow\u0026quot; email_on_failure: bool email_on_retry: bool email: str retries: int retry_delay: datetime.timedelta catchup: airflow가 트리거하지 않았던 날짜에 대해 실행 여부, False로 하는 것이 좋음 DAGrun 스케줄러가 DAGrun object 생성 주어진 시간의 dag 정보를 담고 있는 인스턴스 실행될 task들을 가지고 있음 원자성, 멱등성 Operator Operator = Task 실행하고자 하는 작업을 캡슐화했다고 생각하면 된다. task_id: 하나의 dag내에서 유니크한 이름을 가져야 한다. 종류 Action Operator: 실행 (bash) Transfer Operator: 전송 (mysql, postgres) Sensor Operator: 감지 How Airflow works? One Node Architecture Web server가 metastore에서 메타데이터 정보를 가져온다. Schduler가 Metastore와 Executor에서 DAG, 작업을 트리거한다. Executor가 작업 업데이트를 metastore에 완료되었다고 업데이트한다. Executor에는 큐가 있는데, 실행이 정해진 순서대로 되게 한다. Multi Nodes Architecture (Celery) Node1에는 웹서버, 스케줄러, 익스큐터가 있음 Node2에는 메타스토어와 큐가 있음. 큐는 rabbit MQ나 redis 같은 서비스를 사용함 워커 노드들에는 Airflow 워커들이 있음 실행 방식은 1 노드 구조와 유사하나, 익스큐터는 외부 큐에 작업을 푸시한다. 큐 내부에 있는 작업은 워커에 의해 풀 된다. folder dags dags 폴더에 파일이 저장 웹 서버와 스케쥴러가 이를 파싱함 스케쥴러가 메타스토어에 dagrun object를 생성 dag가 실행이 되어야 할 경우 스케줄러가 TaskInstance object를 메타스토어에 스케줄함 TaskInstance를 익스큐터에 보냄 실행 중에 메타스토어의 정보를 실행중으로 업데이트 완료되면 메타스토어의 정보를 완료로 업데이트 dagrun이 종료되었는지 검증 웹서버가 metastore의 정보를 ui에 업데이트 caution dag에 파일이 업데이트 되면 dag_dir_list_interval 주기 후 (기본 5분) UI상에서 확인 가능하다. min_file_process_interval (기본 30초) dag를 파싱하는 시간. dag 코드가 업데이트 되도 해당 시간 후 반영된다. Commands useful commands airflow run: 하나의 task 인스턴스 실행 airflow list_dags: dag 목록 airflow dag_state: dag 상태 airflow task_state: task 상태 airflow test: 테스트 test $ airflow tasks test DAG_ID TASK_ID DATE dependencies task 디펜던시 줄 때 줄바꾸기 1 2 task1 \u0026gt;\u0026gt; task2 \u0026gt;\u0026gt; task3 task3 \u0026gt;\u0026gt; task4 \u0026gt;\u0026gt; task5 DAG Details date options start_date DAG의 task들이 언제부터 트리거되고 스케줄되는지 시간을 정의. ex) 2019-03-01로 정의했다면, 2019년 3월 1일 자정에 스케줄됨 python의 datetime 모듈로 정의 가능 과거나 미래로 설정 가능함 미래로 설정 시, 해당 시간이 될 때까지 기다림 과거로 설정 시, 기다리지 않고 실행 가능. 그러나 catchup=False로 주지 않으면 과거로 설정한 날짜로 부터 schedule_interval마다 task instance가 실행되므로 주의 datetime.now()와 같이 동적으로 할당하지 말 것 schedule_interval start_date의 최소값으로 부터 트리거되는 시간 간격 같은 dag에서도 task 별로 start_date를 따로 줄 수 있기 때문에 최소값으로 정의됨 그러나 같은 dag내 task들은 같은 start_date를 쓰는 것이 좋음 cron expression이나 datetime.timedelta 모듈로 정의 가능 cron을 쓰는 것이 더 정확한 표현을 할 수 있으므로 cron을 사용하는 것이 좋음 execution_date dag가 실행된 시각이 아님 start_date - schedule_interval end_date dag/task가 더이상 스케줄되지 않는 시간 기본값은 None Backfill 실행되지 않았던 dag/task를 실행하는 기능 catchup=True로 설정 시 수행 catchup=False로 설정 시 실행되지 않은 가장 마지막 dag/task만 실행하도록 되어 있음 CLI를 통해 실행 가능. airflow 공식문서 depends_on_past task 레벨에서 정의 default_args에 정의 해서 모든 task에도 적용 가능 이전 dagrun의 특정 task가 실패했다면, 이번 dagrun에서 그 task가 실행되는 것을 막을 수 있음 wait_for_downstream task 레벨에서 정의 default_args에 정의 해서 모든 task에도 적용 가능 wait_for_downtstream이 정의된 task의 downstream task들이 이전 dagrun에서 완료될 때 까지 이번 dagrun 대기 DAGs folder structure 1. Zip dag 파일은 zip 파일 root에 위치해야함 모듈 디펜던시가 필요하면 virtualenv와 pip 사용 2. DAGBag DAG 모음. 폴더 구조로 dag를 다룬다. dev/staging/prod와 같이 환경 분리에 이점이 있다 dagbag이 깨지면 airflow UI상에서 에러가 뜨지 않고 웹서버 로그로만 확인 가능하므로 주의 3. .airflowignore .gitignore와 유사 모든 dags 폴더에 넣는 것이 좋음 Failure Detection DAGs dagrun_timeout: dagrun이 타임아웃되는 시간. 스케줄된 dag만 해당하며(수동 실행은 해당되지 않음) 실행중인 dag의 수가 max_active_runs와 일치하는 경우에만 해당 sla_miss_callback on_failure_callback on_success_callback Tasks email email_on_failure email_on_retry retries retry_delay retry_exponential_backoff max_retry_delay execution_timeout on_failure_callback on_success_callback on_retry_callback Test DAG validation tests 유효한지 cycle이 없는지 default arguments가 잘 설정됐는지 DAG/Pipeline Definition Tests task의 숫자가 맞는지 (로직이 아닌) task가 잘 정의 됐는지 task의 upstream, downstream 디펜던시가 잘 정의됐는지 Unit tests 로직 체크 operator가 잘 동작되는지만 체크 복잡한 로직을 airflow가 하게 두지 마라 (airflow는 오케스트레이션 툴이다) Integration tests task가 데이터를 잘 교환하는지 task의 input 체크 여러 task간 의존성 체크 End to End Pipeline tests 결과가 올바른지 전체 로직 체크 성능 체크 Local Executor 병렬 실행 가능 (개발시 local executor 사용 권장) parallelism = 0: unlimit parallelism \u0026gt; 0: limit 코어수 - 1로 설정하는 것을 권장 dag_cuncurrency와 max_active_runs_per_dag 옵션에 따라 dag간 task 실행 순서를 조정할 수 있다. dag_cuncurrency: dag내에서 동시에 실행 가능한 task의 수 max_active_runs_per_dag: 동시에 실행 시킬 수 있는 dag 수(backfill 일 때 주로 신경쓸 듯) SubDAGs 유사한 DAG를 하나의 그룹으로 묶어 UI상에서 마치 하나의 DAG인 것 처럼 표시할 수 있음 SubDagOperator 사용 기본 Executor는 SequentialExecutor main DAG가 모든 subDAG 들을 task로 관리 Airflow UI는 오직 main DAG만 표시 subDAG는 부모 DAG와 동일한 시각에 스케줄되어야 함. 그렇지 않으면 예상치 못한 결과를 낳을 수 있음 데드락이 발생할 수 있음 Branching DAG가 특정 task의 결과에 따라 경로를 선택할 수 있게끔 하는 것 BranchPythonOperator BranchPythonOperator의 결과가 task_c를 반환하고 BranchPythonOperator의 downstream으로 task_a, task_b, task_c가 있다면 task_c를 실행하고, a와 b는 스킵 depends_on_past=True로 지정시, a와 b는 실패 상태로 뜨기 때문에 다음 DAGrun은 실행되지 않음 BranchPythonOperator에 empty path를 주면 의도하지 않은 결과를 줄 수 있기 때문에 반드시 주는 것이 좋다. path를 스킵하고 싶다면 (task를 끝내고 싶다면) dummy task를 줘서 끝내자 마지막 task도 skipped로 뜨는 것을 막고 싶다면 마지막 task operator에 trigger_rule='one_success'추가 Trigger Rule depends_on_past와 사용 가능 upstream task중 skipped task가 있으면 all_success와 all_failed는 skipped 상태로 표시됨 Kinds all_success: upstream task가 모두 성공하면 run all_failed: upstream task가 모두 실패하면 run all_done: upstream task가 성공 유무와 관련없이 끝나면 run one_failed: upstream task가 하나라도 실패해야 run one_success: upstream task가 하나라도 성공해야 run none_failed: upstream task가 failed가 없어야 run none_skipped: upstream task가 skipped가 없어야 run dummy Variables metadata DB에 저장되는 값 Key, value, Is encrypted로 구성 JSON 형식 가능 Templating placeholder {{}} 를 사용하여 값을 대체 Jinja template Macros https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#macros XCOMs task 간 메세지 공유 key, value, timestamp로 구성 value는 가벼워야 한다. (성능 문제) xcom_push(key, value)로 metadata DB에 푸시 key: returned_value, value: 리턴값 xcom_pull(key)로 받기 key를 명시하지 않으면 returned_value가 기본값 TriggerDagRunOperator 다른 DAG(컨트롤러)의 조건에 따라 특정 DAG(타겟)를 시작하게 함 branch나 subdag로는 너무 복잡해질때 사용 컨트롤러는 타겟이 종료될 때 까지 기다리지 않음 컨트롤러와 타겟은 독립적임 컨트롤러의 히스토리에 관한 시각화는 제공되지 않음 두 dag모두 스케줄되어야 함 타겟 interval은 None이어야 함 두 dag간에 메세지를 주고받을 수 있음(xcoms 대체) ExternalTaskSensor DAG간 종속성 줄 때 사용 예를 들어 DAG1(t1 \u0026gt;\u0026gt; t2 \u0026gt;\u0026gt; t3), DAG2(t3 \u0026gt;\u0026gt; t4 \u0026gt;\u0026gt; t5)가 있을 때 t3가 완료되면 (DAG1이 완료되면) DAG2가 실행되도록 한다. (t4부터) 두 DAG는같은 스케줄이어야 함 (또는 execution_delta 나 execution_date_fn 파라미터 사용) TriggerDagRunOperator와 쓰면 고장남 스케줄 인터벌 None이기 때문 Logging airflow.cfg의 base_log_folder: log 저장 경로 fab_logging_level: flask app builder의 로깅 수준. (flask 기반 웹서버) Metrics Counters: 실패한 task 수 Gauges: queued task 수 Timers: task 완료까지의 밀리초 ","date":"2022-10-14T20:15:22+09:00","image":"https://Haebuk.github.io/p/airflow-%EC%A0%95%EB%A6%AC/AirflowLogo_hubc3c506518d9161c24efab84ad5b3982_59053_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/airflow-%EC%A0%95%EB%A6%AC/","title":"Airflow 정리"},{"content":"\n여러 파이썬 코딩 스타일과 관련된 책을 봤었는데, 가장 잘 정리가 된 책이라고 생각했다.\n최근에 나온 책이라 코드 버전도 최신이다. 데코레이터를 이용한 우아한 재시도 로직은 꽤 멋지다고 생각해 실무에도 적용했다.\n책은 꽤 넓은 범위를 커버한다. 그렇지만 내용이 얕지는 않다고 생각한다. 책을 한 번 읽어서는 모든 내용을 숙지할 수 없을 것 같다.\n읽을 책이 밀려 바로 다시 읽을 순 없지만, 로테이션이 다 돈다면 다시 읽고 싶은 책. 아직 책 속에서 배워야 할 것이 무궁무진하다.\n모든 클린 코드 책이 그렇겠지만, 초보자가 읽으면 별로 도움이 되지 않을 책. 실무에 파이썬을 충분히 사용하고 있으며 평소 클린 코드에 대해 고민을 많이 하는 실무자가 읽으면 좋을 책이다.\n","date":"2022-10-08T11:05:39+09:00","permalink":"https://Haebuk.github.io/p/%ED%8C%8C%EC%9D%B4%EC%8D%AC-%ED%81%B4%EB%A6%B0%EC%BD%94%EB%93%9C-%ED%9B%84%EA%B8%B0/","title":"파이썬 클린코드 후기"},{"content":"주말에 열린 파이콘2022에 다녀왔다. 장소는 명동에 있는 마실 스튜디오.\n컨퍼런스에 참여하는 것은 처음이었는데, 파이썬을 사용하는 유저로서 꼭 참여해보고 싶었기 때문에 큰 마음을 먹고 결제를 했다. 가격은 상당히 비싸다고 생각했는데, 오프라인 참가비 4만원에 굿즈비 2만원이었다. 참가비야 그만한 가치가 있다고 생각했는데, 굿즈는 좀 마음에 들지 않았다. 티셔츠가 있으니 그러려니 하고 넘어갔다.\n토요일과 일요일 이틀동안 진행이 됐고, 각 요일에 열리는 세션 목록이 달랐다. (발표 시간표 링크 바로가기)\n데이터 엔지니어링 항목이 있고, 좀 더 끌리는 주제들이 많은 토요일로 신청하고 싶었으나 일정이 있어 일요일로 했다. 결론부터 말하면 오프라인에서도 발표 세션은 온라인으로 진행됐기 때문에 모든 세션이 녹화되어 상관없었다. (발표 세션 링크는 위 발표 시간표 링크 바로가기 링크 안에 있다.)\n9시 50분쯤 도착했는데, 10시부터 입장이 가능해서 앞에 사람들이 서있었다. 입장 수속을 하고나니 웬걸? 내 양손에 무언가를 바리바리 챙겨주셨다.\n내부는 총 2층으로 이루어져 있었다. 1층에는 데스크와 2개의 세션 룸, 파이썬 도서관, 몇 개의 스탬프 부스가 있었다.\n스탬프를 팜플렛에 찍고 나서 1층에 계신 요기요 직원들과 이벤트 이야기를 하고 (요기요에서 주최하는 이벤트) 2층으로 올라갔다.\n아직 채용부스가 준비되지 않아 분주했다. 부스는 10시 30분부터 운영됐다.\n그전까지 나는 분주히 15개의 스탬프 부스를 돌아 모두 채웠다.\n스탬프를 모두 채우고, 이직 생각이 없는 나는 \u0026lsquo;이제 뭐하지..\u0026rsquo; 생각을 하다 세션 시간표를 보니 파이썬으로 실시간 데이터 처리를 시도해보자. 라는 세션이 있길래 들으러 1층에 내려갔다. (유튜브 링크)\n세션룸은 앞에 빔프로젝터 2개와 의자가 놓여져 있는 다소 열악한 환경이었다. 내가 들은 처음이자 마지막 파이콘2022의 세션이었다.\n어느덧 점심시간이 되고, 12시부터 운영하는 스낵부스를 찾아갔다.\n모든 사람을 포용하는 파이콘의 행동강령때문인지, 비건 메뉴와 글루텐 프리 메뉴가 있었다. 나는 글루텐 프리를 골랐다.\n오후에 시간이 비었다. 자유롭게 이야기 할 수 있는 장소가 있고, 어떤 주제로 이야기할지 적어놓는 화이트보드가 있었다. AI에 대해 얘기하는 시간이 있길래 참가해서 네트워킹을 했다. 다른 분들은 어떻게 일하고 어떤 일을 하는지 알 수 있었던 좋은 시간이었던 것 같다. (파이콘이 끝나고 이분들과 저녁까지 먹었다.)\n3시부터는 토크 콘서트가 열렸다. 채용 담당자분들을 필두로 이야기하는 취업 이야기를 시작으로, \u0026lsquo;알면 쓸모있는 파이썬 잡학 지식(알쓸파잡)\u0026lsquo;에 나도 참가했다.\n파이썬 의존성 관리 툴로 Poetry를 소개했다. 자기소개시 데이터 사이언티스트로 소개한 덕인지, 끝나고 학생분들이 찾아와서 커리어에 대한 질문을 하셨다. 취업전 내가 떠올라 아는 지식 내에서 열심히 상담해드렸다.\n그 후로 o/x 퀴즈랑, 경품 추첨등을 했는데 아쉽게 다 떨어졌다.\n집에 돌아와서 언박싱을 했다.\n티셔츠들도 귀여워서 찍어봤다.\n사실 굿즈만으로도 마음이 훈훈해졌지만 (집에 들고오는게 힘들었다.) 관심사가 일치하는 다양한 사람들과 네트워킹하는 재미도 쏠쏠해서 6만원이 전혀 아깝지 않았다. 내년에도 또 갈 것 같다.\n","date":"2022-10-07T06:42:15+09:00","image":"https://Haebuk.github.io/p/%ED%8C%8C%EC%9D%B4%EC%BD%982022-%ED%9B%84%EA%B8%B0/pycon_hucdfeeccba171252542d11eaec912541f_21276_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%ED%8C%8C%EC%9D%B4%EC%BD%982022-%ED%9B%84%EA%B8%B0/","title":"파이콘2022 후기"},{"content":"들어가며 S3에 저장된 객체를 퍼블릭으로 공개를 할 경우, 자신의 S3 버킷 구조가 그대로 노출되는 문제와 함께, 보안이 우려되는 경우가 있다.\n이 경우 Cloudfront와 원본 액세스 ID, 즉 OAI를 사용해 S3 버킷 구조를 숨기고, 객체를 외부에 공개할 수 있다.\n객체를 공개할 S3 버킷 생성 외부 사용자들에게 공개할 객체를 담는 버킷을 생성한다. 나같은 경우에는 링크드인에 수료증 링크를 걸기 위해 kade-certificate라는 버킷을 생성하고 그 안에 수료증 파일들을 저장했다.\n콘솔을 사용하여 OAI 생성 및 Cloudfront 배포에 추가 Cloudfront 페이지에서 배포 생성을 클릭하고, 생성한 S3 버킷을 원본 도메인으로 선택한다.\n이후 하단의 S3 버킷 액세스에서 Legacy access identities 항목을 선택하고, 새 OAI를 생성한다.\n생성한 OAI를 선택한다.\n그리고 버킷 정책 항목에서 예, 버킷 정책 업데이트를 선택하면 자동으로 S3 정책을 수정해준다.\nGET 요청만 수락하므로 하단의 모든 항목들의 수정 없이 배포 생성 버튼을 클릭한다.\n생성된 Cloudfront url로 접근 배포가 생성되고 나면 다음 화면과 같다.\n도메인 이름에 접근 가능한 URL이 생성된다. (d3sdxwnh25t6pc.cloudfront.net 처럼)\n이 URL 뒤에 버킷내에 저장된 객체명을 붙여 접근하면 다음과 같이 외부에서 접근이 가능함을 알 수 있다.\n이처럼 Cloudfront와 OAI를 사용하여 사용자의 버킷 URL은 숨긴채, 외부에서 접근을 허용할 수 있다.\nReference 원본 액세스 ID(OAI)를 사용하여 Amazon S3 콘텐츠에 대한 액세스 제한: https://docs.aws.amazon.com/ko_kr/AmazonCloudFront/latest/DeveloperGuide/private-content-restricting-access-to-s3.html ","date":"2022-09-09T23:13:41+09:00","permalink":"https://Haebuk.github.io/p/cloudfront-oai%EB%A5%BC-%ED%86%B5%ED%95%9C-s3-url-%EA%B3%B5%EA%B0%9C/","title":"Cloudfront - OAI를 통한 S3 URL 공개"},{"content":"들어가며 최근 흥미로운 프로그램을 발견했다. Udacity에서 제공하는 AWS Machine Learning Engineer Scholarship Program이다.\nAWS 이름을 달고 있는 이유는 이 과정 내에서 AWS 제품인 DeepRacer와 DeepComposer를 실습해 볼 수 있기 때문이다.\n끝까지 수료하면 평가를 치르게 되는데, 기준 점수 이상이 나오면 수료증을 발급해주고, 상위 325등 이내의 성적으로 수료시 후속 프로그램인 AWS Machine Learning Engineer Nanodegree 도 무료로 수강할 수 있는 기회를 제공한다.\nAt the end of the AWS Machine Learning Foundations Course, learners will take an assessment from which top performers will be selected for one of 325 follow-up scholarships to one of Udacity’s most popular and recently refreshed Nanodegree programs: The AWS Machine Learning Engineer Nanodegree program.\nAWS 기계 학습 기초 과정을 마친 325명의 학습자는 Udacity의 가장 인기 있고 최근에 새로워진 Nanodegree 프로그램 중 하나인 AWS 기계 학습 엔지니어 Nanodegree 프로그램을 수강할 수 있게 됩니다.\nUdemy의 nano degree는 평은 좋지만 악랄한 가격으로 유명한데, 무료로 제공한다는 이 기회를 놓칠 수 없어서 냉큼 신청했다.\n강의 구성 강의는 크게 4가지 구성으로 이루어져 있다.\nIntroduction to the Program AWS Machine Learning Foundations Certification Assessment Certificate of Completion Assessment 1. Introduction to the Program 첫번째 챕터는 오리엔테이션으로 이루어져 있다.\nUdacity에 대해 소개하고, 강사진은 누구이며, 선수지식은 무엇이 필요한지, 이 코스에서 무엇을 배울지에 대해 소개한다.\n선수지식은 통계와 파이썬 프로그래밍에 대해 기초수준만 필요하다고 나와있다.\n가볍게 넘어갔다.\n2. AWS Machine Learning Foundations 본격적인 강의 챕터로, 크게 네 가지 구성으로 이루어져 있다.\n첫 번째는 아주 간단한 머신러닝 태스크들의 정의와 문제들을 해결하기 위해 머신러닝을 어떻게 적용하는지 학습한다. 지도, 비지도 및 강화학습에 대한 소개로 봐도 무방한다.\n두 번째는 AWS를 활용한 머신러닝인데, DeepRacer와 DeepComposer 서비스를 이용한다.\nDeepRacer는 강화 학습을 통한 자율 주행 자동차를 훈련시킬 수 있는 서비스, DeepComposer는 작곡 서비스이다. 솔직히 말하면 장난감에 가깝다. 차라리 SageMaker와 같은 서비스를 소개했으면 어땠을까 싶었는데, 워낙 기초 수준의 강의이다 보니 수강생의 흥미를 돋구기 위해 선택한건지, 잘 사용되지 않는 AWS의 서비스를 살리고자(\u0026hellip;)하는 건지는 잘 모르겠다. 여기서 이 강의에 대해 크게 실망을 했다.\n뒤에 두 파트는 소프트웨어 엔지니어링 파트로, 리팩토링, 테스팅, 깃을 통한 버전관리에 대해 소개한다. 모두 기초적인 수준이기 때문에 가볍게 듣고 넘어갔다.\n세 번째 챕터는 평가(시험)을 치루게 된다.\n평가 세 번째 챕터는 두 번째 챕터(AWS Machine Learning Foundations)에서 학습한 내용을 바탕으로 온라인 시험을 치룬다.\n감독은 따로 없고 온라인상으로 60분동안 시험을 치르게 된다.\n60%이상은 수료, 90%이상 부터는 나노 디그리 프로그램에 대한 자격을 얻는다고 한다. 위에 325등과 상이한 내용이라 혼란스러웠다.\n시험 난이도는 생각보다 까다로워서 당황했다. 굉장히 만만히 봤었는데, 당황스러움을 안겨주는 문제들이 많았다. 높은 성적을 받고 싶다면 꼼꼼하게 보는 것을 추천한다. 챕터 2에서 세부 파트가 끝날 때 마다 퀴즈가 나오는데, 틀린 문제들의 오답 노트를 미리 작성했음에도 불구하고 꽤 난항을 겪었다.\n그래도 다행히 한 번에 수료를 하였고, 점수는 따로 공개되지 않았다.\n후기 솔직히 이 강의는 굉장히 별로였고.. 초심자에게도 딱히 추천하지 않는다.\n얻을 게 없다. 머신러닝 기초에 대해 배우고 싶으면 양질의 강의나 책이 매우 많기 때문에 이를 사용하는 것을 추천하며, AWS의 머신러닝 서비스를 사용해보고자 하는 목적으로도 비추천한다.\n오로지 나노디그리를 공짜로 얻기 위한 목적으로 듣는 것이 좋은 것 같다. 11월 6일 나노디그리 대상자를 발표한다고 하니, 그때까지 기다려야 겠다.\n","date":"2022-09-06T13:47:25+09:00","image":"https://Haebuk.github.io/p/udacity-aws-machine-learning-engineer-foundations-2022-%ED%9B%84%EA%B8%B0/udacity-logo-vector_hu025f0b4574d82671ba11d73228d867b5_4100_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/udacity-aws-machine-learning-engineer-foundations-2022-%ED%9B%84%EA%B8%B0/","title":"[Udacity] AWS Machine Learning Engineer Foundations 2022 후기"},{"content":"보안 서비스 어카운트 사용자 어카운트(UserAccount): EKS에서는 IAM과 연결되어 있어 쿠버네티스 관리 대상이 아니고, 네임스페이스의 영향을 받지 않음 서비스 어카운트(ServiceAccount): 쿠버네티스에서만 사용, 파드에서 실행되는 프로세스를 위해 할당. 네임스페이스와 연결된 리소스 파드 기동 시 반드시 서비스 어카운트 한 개를 할당해야 하며, 서비스 어카운트 기반 인증/인가를 하고 있음 지정하지 않을 시 기본 서비스 어카운트가 할당 서비스 어카운트 생성 1 2 ## 서비스 어카운트 생성 $ kubectl create serviceaccount sample-serviceaccount 인증이 필요한 개인 저장소에 저장된 이미지를 가져오기 위해 시크릿인 imagePullSecrets를 설정하는 경우 kubectl patch 명령어를 사용하거나 생성할 때 매니페스트를 사용하여 서비스 어카운트 생성 1 2 3 ## 생성 후 kubectl patch 명령어로 적용 $ kubectl patch serviceaccount sample-serviceaccount \\ -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;myregistrykey\u0026#34;}]}\u0026#39; 서비스 어카운트와 토큰 생성할 때는 지정하지 않은 시크릿 항목이 존재 kubernetes.io/service-account-token 타입의 시크릿으로 자동 생성 토큰을 변경하고 싶을 떄는 해당 시크릿을 삭제하면 자동으로 재생성됨 1 2 3 4 5 ## 서비스 어카운트 정보 확인 $ kubectl get serviceaccounts sample-serviceaccount -o yaml ## 연결된 시크릿 $ kubectl get secrets sample-serviceaccount-token-nmmb9 -o yaml 서비스 어카운트는 이 토큰으로 쿠버네티스 API에 대한 인증 정보로 사용가능 파드에 할당된 서비스 어카운트의 할당된 권한이 그대로 파드에 할당되는 권한이 됨 파드의 서비스 어카운트를 명시적으로 지정하려면 spec.serviceAccountName 지정 서비스 어카운트를 지정한 컨테이너를 생성 후 기동된 파드 정보를 확인하면 토큰이 볼륨으로 자동으로 포함되어 있는 것을 알 수 있음 1 2 3 4 5 ## 토큰이 볼륨으로 마운트 된 것을 확인 $ kubectl get pods sample-serviceaccount-pod -o yaml ## API 서버 인증에 사용되는 토큰과 인증서 등의 파일 확인 $ kubectl exec -it sample-serviceaccount-pod -- ls /var/run/secrets/kubernetes.io/serviceaccount/ 서비스 어카운트에 적절한 RBAC 설정을 한 후 컨테이너에서 다음과 같이 실행하면 API 접근을 위한 인증/인가가 성공하고 기본 네임스페이스로 동작하고 있는 파드 목록 확인 가능 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 컨테이너에서 bash 실행 $ kubectl exec -it sample-serviceaccount-pod -- bash ## curl 명령어 설치 root@sample-serviceaccount-pod:/## apt update \u0026amp;\u0026amp; apt -y install curl ## 토큰을 환경변수로 정의 root@sample-serviceaccount-pod:/## TOKEN=`cat /var/run/secrets/kubernetes.io/serviceaccount/token` ## 쿠버네티스 API 서버에서 파드 목록 확인 root@sample-serviceaccount-pod:/## curl -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; \\ --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt \\ https://kubernetes/api/v1/namespaces/default/pods 토큰 자동 마운트 설정을 비활성화 하려면 automountServiceAccountToken을 false로 설정 해당 서비스 어카운트로 기동하는 파드는 토큰을 볼륨으로 마운트 하지 않음 토큰을 사용할 때는 명시적으로 spec.automountServiceAccountToken을 true로 설정하면 마운트 가능 도커 레지스트리 인증 정보 자동 설정 imagePullSecret이 지정된 서비스 어카운트를 할당한 파드가 기동한 경우 자동으로 파드의 서비스 어카운트을 사용하여 도커 이미지를 가져온다. 1 2 ## spec.imagePullSecrets가 자동으로 포함된 것을 확인 $ kubectl get pods sample-sa-pullsecret-pod -o yaml spec.imagePullSecrets는 서비스 어카운트에 복수로 지정이 가능하므로 자동으로 여러 인증 정보 설정 가능 RBAC(Role Based Access Control) 어떤 조작을 허용하는지를 결정하는 롤을 생성하고 서비스 어카운트 등의 사용자에게 롤을 연결(롤바인딩)하여 권한을 부여 AggregationRule을 사용해 여러 롤을 집약한 롤을 생성하여 관리성을 향상할 수 있음 롤과 롤바인딩에는 네임스페이스 수준의 리소스와 클러스터 수준의 리소스 두 가지 존재 네임스페이스 수준의 리소스: 롤, 롤바인딩 클러스터 수준의 리소스: 클러스터롤, 클러스터롤바인딩 롤과 클러스터롤 네임스페이스 범위의 리소스를 대상으로 인가 설정 가능 클러스터롤의 경우 노드/네임스페이스/영구볼륨과 같은 클러스터 범위의 리소스나 쿠버네티스 API 정보를 가져오는 nonResourceURL에 대한 권한도 설정 가능 주로 apiGroups, resources, verbs 세 가지 지정 apiGroups와 resources로 지정된 리소스에 대해 verbs 권한 인가 롤에 지정할 수 있는 실행 가능한 조작(verbs)\n종류 개요 * 모두 처리 create 생성 delete 삭제 get 조회 list 목록 조회 patch 일부 업데이트 update 업데이트 watch 변경 감시 주의 사항 디플로이먼트 리소스에 대해 롤을 기술할 때 여러 apiGroups가 존재 extensions/v1beta1, extensions/v1beta2, apps/v1으로 변경되어 왔음 모든 디플로이먼트 리소스를 대상으로 롤을 생성하는 경우에 주의 deployment 리소스와 deployment/scale 서브 리소스는 개별적으로 지정 deployment/scale이 지정되어 있지 않으면 레플리카 수를 변경하는 스케일 처리 불가 롤 생성 rules는 여러 개 설정 가능 네임스페이스 지정 클러스터롤 생성 rules에 noneResourceURLs 지정 가능 헬스 체크용 엔드포인트나 버전 정보 표시용 엔드포인트의 URL metadata.namespace 지정 불가 클러스터롤의 Aggregation 여러 클러스터롤의 정의를 읽는 기능 클러스터롤에 정의된 레이블 기반, 집계되는 쪽 클러스터롤에 정의된 롤은 반영되지 않음 1 2 ## 집계된 클러스터롤 확인 $ kubectl get clusterroles sample-aggregated-clusterrole -o yaml 나중에 집계되는 측의 클러스터롤을 변경하는 경우에도 집계되는 측의 클러스터롤에 자동으로 반영 쿠버네티스가 생성하는 클러스터롤 단순한 권한을 사용하는 경우 프리셋 사용 클러스터롤명 내용 cluster-admin 모든 리소스 관리 가능 admin 클러스터롤 편집 + 네임스페이스 수준의 RBAC edit 읽기 쓰기 view 읽기 전용 롤바인딩과 클러스터롤바인딩 roleRef에서 연결하는 롤과 subjects에 연결하는 사용자나 서비스 어카운트 지정 하나의 롤바인딩당 하나의 롤만 가능, subjects에는 여러 사용자나 서비스 어카운트 지정 가능 롤바인딩: 특정 네임스페이스에 롤 또는 클러스터 롤에서 정의된 권한 부여 네임스페이스 새로 생성시 그 네임스페이스에도 같은 롤바인딩 추가해야 함 클러스터롤바인딩: 모든 네임스페이스에 클러스터롤에서 정의한 권한 부여 네임스페이스 새로 생성해도 네임스페이스 간에 같은 권한 부여 가능 롤바인딩 생성 사용자에 대해 특정 네임스페이스에서 롤 또는 클러스터롤에 정의한 권한 부여 클러스터롤바인딩 생성 사용자에 대해 모든 네임스페이스에서 클러스터롤로 정의된 권한 부여 보안 컨텍스트 각 컨테이너에 대한 보안 설정 설정 가능 항목\n종류 개요 privileged 특수 권한을 가진 컨테이너로 실행 capabilities Capabilities 추가와 삭제 allopPrivilegeEscalation 컨테이너 실행 시 상위 프로세스보다 많은 권한을 부여할지 여부 readOnlyRootFileSystem root 파일 시스템을 읽기 전용으로 할지 여부 runAsUser 실행 사용자 runAsGroup 실행 그룹 runAsNonRoot root에서 실행 거부 seLinuxOptions SELinux 옵션 특수 권한 컨테이너 생성 spec.containers[].securityContext.privileged를 true로 설정 컨테이너 내부에서 기동하는 프로세스의 리눅스 Capabilities가 호스트와 동등한 권한을 가짐 Capabilities 부여 더 세분화된 특정 Capabilities 부여/제거 가능 1 2 ## Capabilities 확인 $ kubectl exec -it sample-capabilities -- capsh --print | grep Current root 파일 시스템의 읽기 전용 설정 컨테이너 이미지에 포함된 파일 등을 읽기 전용으로 설정 가능 root 파일 시스템을 읽기 전용으로 설정하면 커널 관련 파일 등을 변경할 수 없어 보안성 향상 1 2 ## 파일 시스템을 읽기 전용으로 마운트 $ kubectl exec -it sample-rootfile-readonly -- touch /etc/os-release 네트워크 정책 클러스터 내부에서 파드 간에 통신할 경우 트래픽 룰을 규정 네트워크 정책을 사용하지 않을 경우 모든 파드는 서로 통신이 가능 네트워크 정책을 사용한다면 네임스페이스별로 트래픽을 전송하지 못하게 하거나 모든 파드 간 통신을 차단하고 특정 파드 간 통신을 허용하는 화이트리스트 방식 사용 가능 네트워크 정책 생성 네트워크 정책은 인그레스(수신)과 이그레스(송신)로 구성 인그레스: 인바운드 방향의 트래픽 룰을 설정 이그레스: 아웃바운드 방향의 트래픽 룰 설정 설정 범위를 podSelector로 지정 네트워크 정책은 네임스페이스별로 생성해야 함 네트워크 정책 종류와 통신 제한 범위\n정책 종류 인그레스 룰의 경우 이그레스 룰의 경우 podSelector 특정 파드에서 들어오는 통신 허가 특정 파드로 나가는 통신 허가 namespaceSelector 특정 네임스페이스상에 있는 파드에서 들어오는 통신 허가 특정 네임스페이스상에 있는 파드로 나가는 통신 허가 ipBlock 특정 CIDR(IP 주소)에서 들어오는 통신 허용 특정 CIDR(IP 주소)로 나가는 통신 허용 화이트리스트 방식과 블랙리스트 방식 화이트리스트 방식: 모든 트래픽을 차단해 두고 특정 트래픽만 허가 블랙리스트 방식: 모든 트래픽 허가해 두고 특정 트래픽만 차단 클라우드의 네트워크 기본 설정에서 인바운드는 전체를 차단하고 아웃바운드는 전체를 허용하는 것이 일반적 시크릿 리소스 암호화 시크릿 리소스는 base64로 인코드되어 있을 뿐으로 깃 저장소에 업로드하는 것은 보안상 위험할 수 있음 별도의 암호화 필요 kubesec 오픈 소스 소프트웨어 KMS를 사용해 암호화 가능 파일 전체를 암호화하지 않고 시크릿 구조를 유지한 채 값만 암호화하므로 가독성 높음 시크릿 매니페스트를 암호화하기 때문에 깃 저장소에도 업로드 가능 SealedSecret 오픈 소스 소프트웨어 암호화된 SealedSecret이라는 커스텀 리소스 생성 후 클러스터에 등록하면 클러스터 내부에서 SealedSecret에서 시크릿 리소스로 변환되는 구조 생성된 SealedSecret 리소스는 암호화되어 있기 때문에 깃 저장소에 배치 가능 ExternalSecret 오픈 소스 소프트웨어 SealedSecret과 유사한 구조 ","date":"2022-07-10T17:12:39+09:00","image":"https://Haebuk.github.io/p/%EB%B3%B4%EC%95%88/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EB%B3%B4%EC%95%88/","title":"보안"},{"content":"유연한 고급 스케줄링 필터링과 스코어링 필터링: 파드 스케줄시 충분한 리소스가 있는지, 필수 조건으로 지정한 레이블을 가진 노드인지 등을 체크 스코어링: 필터링 후 노드 목록에 순위를 매겨 가장 적합한 노드를 계산 필터링과 스코어링 이후 우선순위가 같은 스케줄링 대상 노드가 여러개 존재 시 무작위 선택 매니페스트에서 지정하는 스케줄링 사용자가 배치하고 싶은 노드를 선택하는 방법과 관리자가 배치하고 싶지 않은 노드를 지정하는 방법이 있음 쿠버네티스 사용자가 배치하고 싶은 노드를 선택하는 방법 종류 개요 nodeSelector(가장 단순한 노드 어피니티) 단순한 노드 어피니티 기능 노드 어피니티 특정 노드상에서만 실행 노드 안티어피니티 특정 노드 이외에서 실행 인터파트 어피니티 특정 파드가 존재하는 도메인(노드, 존)에서 실행 인터파트 안티어피니티 특정 파드가 존재하지 않는 도메인에서 실행 빌트인 노드 레이블과 레이블 추가 빌트인 노드 레이블: 노드에 미리 지정되어 있는 레이블 1 2 ## 노드에 할당된 레이블 정보 $ kubectl get nodes -o json | jq \u0026#34;.items[] | .metadata.labels\u0026#34; 1 2 3 4 5 6 ## kubectl get nodes에서 확인한 노드명을 지정하여 레이블 부여 $ kubectl label node NODE_NAME disktype=hdd cpuspec=low cpugen=3 $ kubectl label node NODE_NAME disktype=ssd cpuspec=low cpugen=2 $ kubectl label node NODE_NAME disktype=hdd cpuspec=high cpugen=4 1 2 ## 노드 목록과 disktype 레이블 표시 $ kubectl -L=disktype,cpuspec,cpugen get node nodeSelector(가장 단순한 노드 어피니티) 단순한 노드 어피니티만을 실행하는 경우 equality-based는 단일 조건만 지정할 수 있음 1 2 ## 반드시 disktype=ssd인 레이블을 가진 노드에서 기동 $ kubectl get pods sample-nodeselector -o wide 노드 어피니티 파드를 특정 노드에 스케줄링하는 정책 spec.affinity.nodeAffinity에 작성 노드 어피티니의 필수 스케줄링 정책과 우선 스케줄링 정책\n설정 항목 개요 requiredDuringSchedulingIgnoredDuringExecution 필수 스케줄링 정책 preferredDuringSchedulingIgnoredDuringExecution 우선적으로 고려되는 스케줄링 정책 필수 조건 requiredDuringSchedulingIgnoredDuringExecution이 만족되지 않은 스케줄링을 수행시 스케줄링에 실패\npreferredDuringSchedulingIgnoredDuringExecution는 어디까지나 우선적으로 스케줄링을 하는 것 필수 조건의 스케줄링 정책\nnodeSelectorTerms: 어떤 조건의 노드가 스케줄링 가능한 노드인지 정의 복수 지정 가능, OR 조건 matchExpressions: AND 조건이 되도록 여러 조건 지정 1 2 3 4 5 6 7 8 ## (A and B) or (C and D) nodeSelectorTerms: - matchExpressions: - A - B - matchExpressions: - C - D 우선 조건의 스케줄링 정책에서도 복수 조건 지정 가능 우선순위의 가중치(weight)와 조건 쌍 여러개 1 2 3 4 5 6 7 8 9 10 11 12 ## (A and B)가 가중치 X, (C and D)가 가중치 Y의 우선순위로 스케줄링 실시 preferredDuringSchedulingIgnoredDuringExecution: - weight: X preference: matchExpressions: - A - B - weight: Y preference: matchExpressions: - C - D matchExpressions 오퍼레이터와 집합성 기준 조건 matchExpressions는 key 레이블, 오퍼레이터, values 레이블이라는 세 가지 요소로 구성 values 레이블은 배열로 취급 레플리카셋/디플로이먼트/데몬셋/스테이트풀셀/잡의 셀렉터에서 사용 가능 1 2 3 4 5 6 7 8 9 10 - matchExpressions: - key: disktype operator: In values: - ssd - hdd ## 한 줄로 정의한 경우 - matchExpressions: - {key: disktype, operator: In, values: [ssd, hdd]} matchExpressions에서 사용 가능한 오퍼레이터\n오퍼레이터 종류 사용 방법 의미 In A In [B, \u0026hellip;] 레이블 A의 값이 [B, \u0026hellip;] 중 어느 하나 이상과 일치 NotIn A NotIn [B, \u0026hellip;] 레이블 A의 값이 [B, \u0026hellip;] 중 어느 것에도 일치하지 않음 Exists A Exists [] 레이블 A가 존재 DoesNotExist A DoesNotExist [] 레이블 A가 존재하지 않음 Gt A Gt [B] 레이블 A의 값이 B보다 큼 Lt A Lt [B] 레이블 A의 값이 B보다 작음 Exists는 values 지정 불가 (key가 존재하는지 여부만이 조건) DoesNotExist는 key가 존재하지 않는 것이 조건 Gt/Lt는 values에 하나의 정수 값만 지정 노드 안티어피니티 파드를 특정 노드 이외의 다른 노드에 스케줄링하는 정책 spec.affinity.nodeAffinity 조건에 부정형 오퍼레이터를 지정(엄밀히 말하면 노드 안티어피니티는 존재하지 않음) 인터파드 어피니티 특정 파드가 실행되는 도메인에 파드를 스케줄링 하는 정책 spec.affinity.podAffinity에 조건 지정 가까이 있는 파드끼리는 통신 레이턴시가 적음 노드들간 기능 차이가 적을 경우 노드를 의식하지 않고 파드 기반의 스케줄링만으로 제어 가능 topology는 어느 범위(도메인)를 스케줄링 대상으로 할지를 지정 requiredDuringSchedulingIgnoredDuringExecution은 배열로 여러 조건 지정 가능 1 2 3 4 5 6 7 8 9 10 11 12 ## (A and B)의 파드와 같은 X에 있는 노드 or (C and D)의 파드와 같은 Y에 있는 노드 requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - A - B topologyKey: X - labelSelector: matchExpressions: - C - D topologyKey: Y 인터파드 안티어피니티 특정 파드가 없는 도메인에서 동작시키는 정책 spec.affinity.PodAntiAffinity 지정 가능 여러 조건을 조합한 파드 스케줄링 노드 어피니티/노드 안티어피니티/인터파드 어피니티/인터파드 안티어피니티 조합하여 사용 가능 테인트와 톨러레이션 쿠버네티스 관리자가 배치하고 싶지 않은 노드를 지정하는 방법 노드에 대한 테인트를 설정해 두고 그것을 허용할 수 있는 파드만 스케줄링 허가 조건에 맞지 않는 파드를 노드에서 축출 가 테인트 부여 세 가지 파라미터를 사용한 Key=Value:Effect 형식으로 구성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 특정한 한 대의 노드에 env=prd:Noschedule 테인트 부여 $ kubectl taint node NODE_NAME env:prd=NoSchedule ## 특정 레이블을 가진 모든 노드에 env=prd:NoSchedule 테인트 부여 $ kubectl taint node -l kubernetes.io/os=linux env=prd:NoSchedule ## env를 키로 하는 테인트 삭제 $ kubectl taint node NODE_NAME env- ## env를 키로 하는 NoSchedule 테인트 삭제 $ kubectl taint node NODE_NAME env:NoSchedule- ## 부여된 테인트 확인 $ kubectl describe node NODE_NAME Key, Value는 임의의 값으로 지정하고 노드 어피니티와 동일하게 일치 여부를 조건으로 사용 Effect는 테인트와 톨러레이션이 일치하지 않을 경우의 동작 Effect 종류\nEffect 종류 개요 PreferNoSchedule 가능한 한 스케줄링하지 않음 NoSchedule 스케줄링하지 않음(이미 스케줄링된 파드는 유지) NoExecute 실행을 허가하지 않음(이미 스케줄링된 파드는 정지 PreferNoSchedule 파드에 톨러레이션 설정이 없는 경우나 테인트에 일치하지 않아도 스케줄링 대상 노드가 됨 NoSchedule 파드에 톨러레이션 설정이 없는 경우나 테인트에 일치하지 않으면 스케줄링 허가하지 않음 NoExecute 일반적인 파드 정지처럼 terminationGracePeriodSeconds로 유예 기간 설정 톨러레이션을 지정한 파드 기동 Key/Value/Effect 지정 후 테인트에서 부여된 Key/Value/Effect가 같은 경우 허용 완전 일치뿐만 아니라 Key/Value/Effect중 하나를 미지정한 경우 와일드카드로 처리 spec.tolerations로 지정 톨러레이션에서 지정 가능한 오퍼레이터\n오퍼레이터 개요 Equal Key = Value Exists Key 존재 NoSchedule과 NoExecute의 경우 조건과 Effect 모두 일치할 경우 스케줄링 PreferNoSchedule은 조건이 일치하지 않아도 스케줄링 되지만 우선순위 하락 어떤 테인트가 부여되든 관계엾이 스케줄링하는 경우 Exists 오퍼레이터만 지정하면 모든 조건에 일치 가능 1 2 tolerations: - operator: Exists NoExecute 일정 시간 허용 조건이 일치하는 파드는 일정 기간만 가동을 허용하는 스케줄링을 할 때 사용 여러 개의 테인트와 톨러레이션 여러 테인트가 부여된 경우 파드의 톨러레이션은 모든 테인트 조건을 만족해야 해당 노드에 스케줄링 가능 장애 시 부여되는 테인트와 축출 노드 장애 발생 시 자동으로 NoExecute의 테인트 부여하여 자동으로 파드 축출 노드 장애시 부여되는 테인트\nEffect Key 개요 NoSchedule node.kubernetes.io/not-ready 노드 상태가 NotReady NoSchedule node.kubernetes.io/unreachable 노드와의 네트워크 접속이 되지 않는 경우 (Unknown) 장애가 발생해도 계속 기동하고 싶은 경우 위 테인트에 대응하는 톨러레이션을 설정해야 함 기본적으로 tolerationsSecond=300으로 부여되며 300초 이내에 노드 문제를 해결하지 않으면 파드가 축출됨 1 2 3 4 5 6 7 8 9 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operators: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operators: Exists tolerationSeconds: 300 쿠버네티스가 부여하는 그 외 테인트 Effect Key 개요 NoSchedule node.kubernetes.io/memory-pressure 노드에 메모리 부족 NoSchedule node.kubernetes.io/disk-pressure 노드에 디스크 뷰족 NoSchedule node.kubernetes.io/pid-pressure 노드에 PID 고갈 NoSchedule node.kubernetes.io/network-unavailable 노드의 네트워크가 연결되지 않음 NoSchedule node.kubernetes.io/unschedulable kubectl cordon에 의해 스케줄링에서 제외됨 문제가 발생해도 스케줄링하고 실행해야 하는 중요도가 높은 파드는 톨러레이션 설정 클라우드 환경에서 노드가 생성되고 삭제될 때 부여되는 테인트도 있음 PriorityClass를 이용한 파드 우선순위와 축출 파드가 이미 리소스의 한계까지 스케줄링된 상태에서 우선순위가 높은 파드를 생성하는 경우 기존 파드 축출 가능 여러 파드가 스케줄링 대기 상태일 경우 스케줄링 순서는 수시로 정렬되어 우선순위가 높은 파드부터 스케줄링 PriorityClass 생성 system-으로 시작하는 이름은 예약어로 등록되어 있으 붙일 수 없음 우선순위(value)와 설명(description)으로 구성됨 globalDefault 옵션을 true로 설정시 PriorityClass가 지정되지 않은 파드에 기본 우선순위 설정으로 PriorityClass 부여 globalDafault: true의 PriorityClass가 여러 개 존재할 경우 가장 우선순위가 낮은 PriorityClass가 기본 우선순위 설정으로 부여 PriorityClass가 전혀 연결되지 않는 경우 priority는 0 PriorityClass를 파드의 spec.podPriorityClass에 지정 해당 podPriorityClass를 가지고 spec.Priority를 자동 업데이트 수동으로 spec.Priority 지정한 경우 파드 생성 불가 1 2 ## 파드에 지정된 우선순위 확인 $ kubectl get pods sample-high-priority -o jsonpath=\u0026#39;{.spec.priority}\u0026#39; 파드 축출시 노드상에 원순위가 낮은 파드를 제외한 상태에서 어피니티 등의 조건을 만족하는지 판단 우선순위가 낮은 파드에 인터파드 어피니티를 설정하면 스케줄링이 수행되지 않는 경우 있음 인터파드 어피니티는 우선순위가 높거나 같은 파드에 스케줄링 해야함 우선순위 축출 비활성화 우선순위를 설정하고 싶지만 다른 파드를 축출하고 싶지 않은 경우 사용 스케줄링시 우선순위가 높은 파드부터 스케줄링됨 preemtionPolicy를 Never로 설정 PriorityClass와 PodDisruptionBudget의 경합 PriorityClass에 의한 축출은 podDisruptionBudget을 고려하여 스케줄링 기타 스케줄링 커스텀 스케줄러 생성 가능 기존 쿠버네티스 스케줄러에 할당되지 않도록 파드에 spec.schedulerName 지정해야 함 스캐줄러를 구현하지 않은 경우 Pending 상태 유지 ","date":"2022-06-25T17:12:36+09:00","image":"https://Haebuk.github.io/p/%EC%9C%A0%EC%97%B0%ED%95%9C-%EA%B3%A0%EA%B8%89-%EC%8A%A4%EC%BC%80%EC%A4%84%EB%A7%81/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EC%9C%A0%EC%97%B0%ED%95%9C-%EA%B3%A0%EA%B8%89-%EC%8A%A4%EC%BC%80%EC%A4%84%EB%A7%81/","title":"유연한 고급 스케줄링"},{"content":"메인터넌스와 노드 정지 스케줄링 대상에서 제외와 북귀(cordon/uncordon) 쿠버네티스 노드는 두 상태를 가짐 SchedulingDisabled: 노드가 스케줄링 대상에서 제외 (파드 신규 생성 x), 이미 실행 중인 파드에는 영향 없음 SchedulingEnabled: 기본 상태 1 2 3 4 5 ## 노드 중 하나를 SchedulingDisabled로 변경 $ kubectl cordon NODE_NAME ## 노드 중 하나를 SchedulingEnabled로 변경 $ kubectl uncordon NODE_NAME 노드 배출 처리로 인한 파드 축출(drain) 실행 중인 파드를 축출해야 할 경우 사용 SchedulingDisabled 상태로 바꾼 후 각 파드에 SIGTERM 신호를 보내므로 cordon을 실행할 필요 없음 1 2 ## 실행 중인 파드를 모두 축출(데몬셋 이외) $ kubectl drain NODE_NAME --force --ignore-daemonsets drain시 다음과 같은 케이스에서는 에러가 발생 디플로이먼트등으로 관리되지 않는 파드 삭제(단일 파드는 파드 삭제 후 재성성이 불가하기 때문) \u0026ndash;force옵션으로 해결 가능 로컬 스토리지 사용하는 파드 삭제(로컬 스토리지 데이터가 삭제되므로) \u0026ndash;delete-local-data로 해결 가능 데몬셋이 관리하는 파드 삭제 \u0026ndash;ignore-daemonset 옵션을 사용해 해결 가능 PodDisruptionBudget(PDB)을 사용한 안전한 축출 파드 축출 시 특정 디플로이먼트하에 있는 레플리카가 동시에 정지되면 다운타임이 발생 여러 노드에서 동시 배출 처리를 한 경우 해당 현상이 발생할 확률 증가 노드가 배출 처리를 할 때 파드를 정지할 수 있는 최대 수를 제한하는 리소스 최소 기동 개수와 최대 정지 개수를 보면서 노드상의 파드 축출 HPA에 의해 파드 수가 변화하는 환경에서는 백분율로 지정하는 것이 좋음 동시에 여러 노드를 배출 처리하는 경우에도 효과적 minAvailable, minUnavailable은 둘 중 하나만 설정 가능 파드에 여러 PDB가 연결되면 축출 실패 ","date":"2022-06-22T17:12:29+09:00","image":"https://Haebuk.github.io/images/Kubernetes-Logo.wine.png","permalink":"https://Haebuk.github.io/p/%EB%A9%94%EC%9D%B8%ED%84%B0%EB%84%8C%EC%8A%A4%EC%99%80-%EB%85%B8%EB%93%9C-%EC%A0%95%EC%A7%80/","title":"메인터넌스와 노드 정지"},{"content":"헬스 체크와 컨테이너 라이프 사이클 헬스 체크 파드가 정상인지 판단하는 기능 이상 종료된 경우 spec.restartPolicy에 따라 파드 재시작 헬스체크 방법 Probe 종류 역할 실패 시 동작 Liveness Probe 파드 내부의 컨테이너가 정상 동작 중인지 확인 컨테이너 재가동 Readiness Probe 파드가 요청을 받아들일 수 있는지 확인 트래픽 차단(파드 재기동x) Startup Probe 파드의 첫 번째 기동이 완료되었는지 확인 다른 Probe 실행 시작 x Liveness Probe: 헬스 체크에 한 번 실패하면 재시작 없이는 복구가 어려울 때 사용 Readiness Probe: db에 정상적으로 접속되는지, 캐시에 로드가 끝났는지, 기동 시간이 오래 걸리는 프로세스가 끝났는지 등등 체크 실패한 경우 트래픽이 파드에 전송되지 않도록 함 Startup Probe: 처음 기동하는데 시간이 오래 걸릴 경우 사용, 완료까지 다른 Probe나 서비스가 시작되지 않음 헬스체크 방식 헬스 체크는 컨테이너별로 이루어짐 하나의 컨테이너라도 실패하면 전체 파드가 실패한 것으로 간주 헬스 체크 방식 내용 exec 명령어를 실행하고 종료 코드가 0이 아니면 실패 httpGet HTTP GET 요청 실행 후 Status Code가 200~399가 아니면 실패 tcpSocket TCP 세션이 연결되지 않으면 실패 명령어 기반의 체크(exec) 명령어로 실행하고 종료 코드로 확인 가장 유연성이 높은 체크 명령어는 컨테이너별로 실행 1 2 3 livenessProbe: exec: command: [\u0026#34;test\u0026#34;, \u0026#34;-e\u0026#34;, \u0026#34;/ok.txt\u0026#34;] HTTP 기반의 체크(httpGet) HTTP GET 요청의 Status Code로 확인 HTTP GET 요청은 kubelet에서 이루어짐 1 2 3 4 5 6 7 8 9 livenessProbe: httpGet: path: /health port: 80 scheme: HTTP host: web.example.com httpHeaders: - name: Authorization value: Bearer Token TCP 기반의 체크(tcpSocket) TCP 세션 활성화를 검증하여 확인 1 2 3 livenessProbe: tcpSocket: port: 80 헬스 체크 간격 Liveness Probe는 실패시 파드가 재시작하므로 실패까지의 체크 횟수를 설정 successThreshold는 1이상, Liveness Probe와 Startup Probe의 경우 반드시 1 첫 번째 체크까지의 지연은 가급적 사용 x, Startup Probe 사용 설정 항목 내용 initialDelaySeconds 첫 헬스 체크까지의 지연(최대 failureThreshold까지 연장) periodSeconds 헬스 체크 간격 시간(초) timeoutSeconds 타임아웃까지의 시간(초) successThreshold 성공이라고 판단하기까지 체크 횟수 failureThreshold 실패라고 판단하기까지 체크 횟수 1 2 3 4 5 6 livenessProbe: initialDelaySeconds: 5 periodSeconds: 5 timeoutSecods: 1 successThreshold: 1 failureThreshold: 1 헬스 체크 생성 헬스 체크 방법 세 가지와 헬스 체크 방식 세가지로 총 9가지 패턴의 헬스 체크 가능 Liveness, Readiness, Startup Probe는 하나 이상 지정 가능 1 2 ## 파드에 생성된 Probe 확인 $ kubectl describe pod sample-healthcheck | egrep -E \u0026#34;Liveness|Readiness\u0026#34; 추가 Ready 조건을 추가하는 파드 ready++(ReadinessGate) 파드가 정말 Ready 상태인지 추가 체크 클라우드 외부 로드밸런서와의 연계에 시간이 걸리는 경우 사용 일반적인 파드의 Ready 판단만으로는 롤링 업데이트시 기존 파드가 한 번에 사라져 안전하게 업데이트 할 수 없는 경우 사용 ReadinessGate를 통과할 때까지 서비스 전송 대상 x, 롤링 업데이트 시 다음 파드 기동으로 이동 x 여러개 사용 가능. 모든 상태가 Ready가 되지 않으면 파드는 Ready 상태가 되지 않음 Readiness Probe를 무시한 서비스 생성 스테이트풀셋에서 Headless 서비스 사용 시 파드가 Ready가 되지 않아도 클러스터 구성을 위해 파드의 이름 해석이 필요한 경우가 존재 Readiness Probe가 실패한 경우에도 서비스에 연결되게 하려면 spec.publishNotReadyAddresses를 true로 설정 Startup Probe를 사용한 지연 체크와 실패 Startup Probe의 경우 failureThreshold의 값을 충분히 크게 두는 것이 좋음 1 2 3 4 5 ## Startup Probe에서 감시하고 있는 파일 생성 $ kubectl exec -it sample-startup -- touch /root/startup ## Startup Probe가 성공한 후 Liveness와 Readiness Probe가 실행된 것을 알 수 있음 $ kubectl exec -it sample-startup -- head /root/log 컨테이너 라이플사이클과 재기동(restartPolicy) 컨테이너 프로세스 정지 또는 헬스 체크 실패시 컨테이너 재기동 여부, 방식 설정 잡은 Always 선택 불가 restartPolicy 내용 Always 파드 정지시 항상 파드 재기동 OnFailure 예상치 못하게 파드가 정지한 경우(종료 코드 0 이외) 파드 재기동 Never 파드 재기동 x 초기화 컨테이너 파드 내에서 메인이 되는 컨테이너 기동 전 별도의 컨테이너 기동 설정에 필요한 스크립트 등을 메인 컨테이너에 보관하지 않는 상태 유지 가능(보안, 이미지 용량 감소) spec.initContainers 복수 지정 가능, 위에서부터 순차적 커맨드 실행 저장소에서 파일 가져오기, 컨테이너 기동 지연, 설정 파일 동적 생성, 서비스 생성 확인, 메인 컨테이너 기동 전 체크 작업 등 1 2 ## 초기화 컨테이너에서 추가된 파일 순서 확인 $ kubectl exec -it sample-initcontainer -- cat /usr/share/nginx/html/index.html 기동 직후와 종료 직전에 임의의 명령어 실행(postStart/preStop) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ## 기동 후 바로 확인한 상태 $ kubectl exec -it sample-lifecycle-exec -- ls /tmp started ## 기동 20초 경과 후 확인한 상태 $ kubectl exec -it sample-lifecycle-exec -- ls /tmp poststart started ## 파드 정지 $ kubectl delete -f sample-lifecycle-exec.yaml ## 삭제 요청 후 바로 확인한 상태 $ kubectl exec -it sample-lifecycle-exec -- ls /tmp poststart prestop started postStart는 비동기 실행이므로 기동시 파일을 배치하는 작업은 초기화 컨테이너를 사용하거나 Entrypoint 안에서 실행 postStart, preStop은 여러 번 실행될 가능성도 있음 postStart에 타임아웃 설정 불가 postStart 실행 중에는 Probe 실행되지 않음 파드의 안전한 정지와 타이밍 기동 중인 파드의 삭제 요청이 쿠버네티스 API 서버에 도착시 비동기로 \u0026lsquo;preStop 처리 + SIGTERM 처리\u0026rsquo;와 \u0026lsquo;서비스에서 제외 설정\u0026rsquo;이 이루어짐 서비스 제외 처리가 끝나기 전 몇 초간 \u0026lsquo;preStop 처리 + SIGTERM 처리\u0026rsquo;에서 대기하거나 요청을 받으면서 정지 처리하는 것이 효과적 전자는 애플리케이션 수정 필요 x 파드에는 spec.terminationGracePeriodSeconds(기본 30초) 설정값이 있음 해당 시간 안에 \u0026lsquo;preStop 처리 + SIGTERM 처리\u0026rsquo; 끝내야 함 끝나지 않는 다면 SIGKILL 신호가 컨테이너에 전달되어 강제 종료 강제 종료를 막기 위해 \u0026lsquo;preStop 처리 + SIGTERM 처리\u0026rsquo;완료 시간을 충분히 확보해야 함 preStop 처리만으로 terminationGracePeriodSeconds 시간을 모두 사용한 경우에느 추가로 2초가 SIGTERM 시간으로 확보됨 1 2 3 4 5 ## terminationGracePeriodSeconds=3으로 삭제 $ kubectl delete pod sample-termination --grace-period 3 ## 강제로 즉시 삭제 $ kubectl delete pod sample-termination --grace-period 0 --force 리소스를 삭제했을 때의 동작 레플리카셋등의 상위 리소스가 삭제되면 하위 리소스가 되는 파드 등을 삭제하기 위해 gc수행 생성된 파드에는 어떤 레플리카셋으로 생성됐는지 판별하기 위해 metadata.ownerReferences 아래에 자동으로 정보 저장 1 2 ## 파드 정의에 포함돈 상위 리소스 정보 확인 $ kubectl get pods sample-rs-l23kds -o json | jq .metadata.ownerReferences 삭제됐을 때 동작 Background(기본값) 레플리카셋 즉시 삭제 후 파드는 gc가 백그라운드에서 비동기로 삭제 Foreground 레플리카셋을 즉시 삭제하지 않고 deletionTimestamp, metadata.finalizers = foregroundDeletion으로 설정 gc가 각 파드에서 blockOwnerDeletion = true 인것을 삭제 (false인 것은 Foreground 삭제라도 Background로 삭제) 모든 삭제가 끝나면 레플리카셋 삭제 Orphan 레플리카셋 삭제시 파드 삭제를 하지 않음 Foreground 사용하려면 API 조작해여 함 ","date":"2022-06-19T17:12:20+09:00","image":"https://Haebuk.github.io/p/%ED%97%AC%EC%8A%A4%EC%B2%B4%ED%81%AC%EC%99%80-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EB%9D%BC%EC%9D%B4%ED%94%84%EC%82%AC%EC%9D%B4%ED%81%B4/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%ED%97%AC%EC%8A%A4%EC%B2%B4%ED%81%AC%EC%99%80-%EC%BB%A8%ED%85%8C%EC%9D%B4%EB%84%88-%EB%9D%BC%EC%9D%B4%ED%94%84%EC%82%AC%EC%9D%B4%ED%81%B4/","title":"헬스체크와 컨테이너 라이프사이클"},{"content":"리소스 관리와 오토 스케일링 리소스 제한 CPU/메모리 리소스 제한 CPU는 클럭 수가 아닌 1vCPU를 1,000m 단위로 지정 단위 리소스 유형 단위 CPU 1 = 1000m = 1 vCPU 메모리 1G = 1000M (1Gi = 1024Mi) Requests: 사용하는 리소스 최솟값 지정한 양의 리소스가 노드에 존재하지 않으면 스케줄링 되지 않음 Limits: 사용할 리소스의 최댓값 노드에 Limits로 지정한 리소스가 없어도 스케줄링 됨 Requests만 설정한 경우 Limits는 자동 설정되지 않고 부하가 최대로 상승할 때까지 리소스 계속 소비 파드가 많이 가동하는 노드에서 리소스 뻇기 발생, OOM 발생 Limits만 설정한 경우 은 값이 Requests에 설정 시스템에 할당된 리소스와 Eviction 매니저 일반 리소스는 고갈 시 쿠버네티스 자체가 동작하지 않거나 그 노드 전체에 영향 가능성 각 노드에는 kube-reserved, system-reserved 두 가지 리소스가 시스템용으로 확보 실제 파드 할당 가능 리소스는 리소스 총량 - (kube-reserved, system-reserved) Eviction 매니저가 시스템 전체가 과부하되지 않도록 관리 Allocatable, system-reserved, kube-reserved 실제 사용되는 리소스 합계가 Eviction Threshold 넘지 않는지 정기적으로 확인, 초과한 경우 파드 Evict Eviction Threshold는 soft, hard 두 가지 존재 soft: SIGTERM 신호를 보내 지정한 시간 후 파드 정지 hard: SIGKILL 신호를 보내 바로 파드 정지 Evict 우선 순위 Requests에 할당된 양보다 초과하여 리소스를 소비하고 있는 것 PodPriority가 낮은 것 Requests에 할당된 양보다 초과하여 소비하고 있는 리소스 양이 더 많은 것 GPU 등의 리소스 제한 엔비디아 GPU 1 2 3 4 5 resources: requests: nvidia.com/gpu: 2 limits: nvidia.com/gpu: 2 오버커밋과 리소스 부족 스케일 아웃을 해도 리소스가 없으면 추가되는 파드 중 리소스를 할당할 수 없는 파드는 Pending 상태가 됨 생성된 파드에 부하가 증가시 리소스 사용량이 100%를 초과하더라도 오버커밋하여 실행 Cluster Autoscaler와 리소스 부족 Cluster Autoscaler는 수요에 따라 노드를 자동으로 추가하는 기능 pending 상태의 파드 발생시 처음으로 Cluster Autoscaler 동작 기본적으로 리소스에 의한 스케줄링은 Requests(최소) 기준으로 동작 Requests와 Limits에 너무 큰 차이를 주지 않을 것 Requests를 너무 크게 설정하지 않을 것 실제 값을 정할 때 Requests, Limits를 낮게 설정하고 성능 테스트를 통해 올려가는 것이 좋음 메모리의 경우 OOM이 발생하지 않을 정도의 리소스 할당 LimitRange를 사용한 리소스 제한 파드, 컨테이너, 영구볼륨 대해 리소스의 최솟값과 최댓값, 기본값 등을 설정 가능 신규 파드가 생성될 때 사용, 기존 파드에는 영향 X 설정 가능 항목 설정 항목 개요 default 기본 Limits defaultRequest 기본 Requests max 최대 리소스 min 최소 리소스 maxLimitRequestRatio Limits/Requests의 비율 LimitRange를 설정할 수 있는 리소스와 설정 항목 타입 사용 가능한 설정 항목 컨테이너 default/defaultRequest/max/min/maxLimitRequetsRatio 파드 max/min/maxLimitRequestRatio PVC max/min 컨테이너에 대한 LimitRange type: Container의 LimitRange로 설정 파드에 대한 LimitRange type: Pod의 LimitRange로 설정 컨테이너에서 사용하는 리소스 합계로 최대/최소 리소스 제한 PVC에 대한 LimitRange type: PersistentVolumeClaim의 LimitRange로 설정 일정 용량 이상으로 볼륨을 생성하지 못하게 할 수 있음 QoS Class 사용자가 직접 설정하지 않고 파드의 Requests/Limits 설정에 따라 자동으로 설정 Qos Class 조건 우선순위 BestEffort Requests/Limits 모두 미지정 3 Guaranteed Requests/Limits 같고 CPU, 메모리 모두 지정 1 Burstable Guranteed 충족하지 못하고 한 개 이상의 Requests/Limits 설정 2 쿠버네티스가 컨테이너에 oom score 설정할 때 사용 oom score: -1000(최고 순위) ~ 1000(최저 순위) Guaranteed의 경우 쿠버네티스 시스템 구성 요소(oom score=-999) 외에 우선순위가 높은 컨테이너가 없어 좀 더 안정적 실행 가능 QoS Class 조건 BestEffort 1000 Guaranteed -998 Burstable min(max(2, 1000 - (1000 * 메모리의 Requests) / 머신 메모리 용량), 999) 1 2 ## 파드 목록과 QoS Class 표시 $ kubectl get pods -o custom-columns=\u0026#34;NAME:{.metadata.name},QOS Class:{.status.qosClass}\u0026#34; BestEffort 리소스 제한이 전혀 없음 LimitRange가 설정된 환경에서는 지정되지 않은 경우에도 자동으로 리소스 제한이 설정되므로 되지 않음 Guaranteed 최소한으로 사용하는 리소스와 최대한으로 사용하는 리소스에 차이가 없는 상태 모든 파드를 Guaranteed로 한다면, 부하 증가에 따른 다른 파드로의 영향을 피할 수 있지만, 집약률 낮아짐 Burstable 특정 리소스만 제한 설정 Requests보다 Limits가 큰 경우 최악의 경우 노드가 과부하를 받을 가능성 리소스 쿼터를 사용한 네임스페이스 리소스 쿼터 제한 각 네임스페이스마다 사용 가능한 리소스를 제한 이미 생성된 리소스에는 영향 X 생성 가능한 리소스 수 제한과 리소스 사용량 제한으로 나눌 수 있음 리소스 쿼터가 설정된 경우 제한된 항목 설정은 필수 생성 가능한 리소스 수 제한 count/RESOURCE.GROUP 구문을 사용 리소스 사용량 제한 CPU/메모리에 대해 컨테이너에 할당 가능한 리소스 양 제한 스토리지는 Limits는 존재하지 않고 Requests만 지정 가능 스토리지클래스마다 제한을 둘 수 있음(SSD, HDD, \u0026hellip;) HorizontalPodAutoscaler 디플로이먼트/레플리카셋/레플리케이션 컨트롤러의 레플리카 수를 CPU 부하 등에 따라 자동으로 스케일링하는 리소스 부하가 높아지면 스케일 아웃, 낮아지면 스케일 인 파드에 Resource Requests가 설정되어 있지 않은 경우 동작하지 않음 30초에 한번씩 오토 스케일링 여부 확인 필요한 레플리카 수 = ceil(sum(파드의 현재 CPU 사용률) / targetAverageUtilization) CPU 사용률은 metrics-server에서 가져온 각 파드의 1분간 평균값 사용 최대 3분에 1회 스케일 아웃, 최대 5분에 1회 스케일 인 실행 스케일 아웃 조건 식: avg(파드의 현재 CPU 사용률) / targetAverageUtilization \u0026gt; 1.1 스케일 인 조건 식: avg(파드의 현재 CPU 사용률) / targetAverageUtilization \u0026lt; 0.9 1 2 ## CLI로 HPA 생성 $ kubectl autoscale deployment sample-deployment --cpu-percent=50 --min=1 --max=10 CPU 이외의 리소스를 사용하여 오토 스케일링 하는 경우 프로메테우스나 그 외의 메트릭 서버와 연계하기 위한 설정 별도 필요 VerticalPodAutoscaler 파드에 할당하는 CPU/메모리의 Requests는 실제 성능을 측정하려면 서비스 환경에 배포해야 하므로 조절이 어려움 VPA는 컨테이너에 할당하는 리소스를 자동으로 스케일해주는 기능 스케일 아웃이 아닌 스케일 업 대상이 되는 워크로드 리소스, 파드 내부 컨테이너 제한, 업데이트 정책 세 부분으로 구성 Requests를 변경하려면 파드 재기동 필요 - 성능에 악영향 가능성 추천값 계산만 하고 참고로만 확인할 수 있는 옵션도 존재 쿠버네티스의 핵심 기능이 아니기 때문에 별도 구성 요소 설치해야 함 updateMode 내용 Off Requests의 추천값을 계산만 하고 실제 변경은 없음 Initial 파드가 재생성된 시점에만 추천값을 Requests로 변경 Recreate 추천값이 변경될 때 파드가 재생성되고 Requests를 변경 Inplace(미구현) 추천값이 변경될 때 파드를 기동한 상태로 Requests 변경 Auto 추천값이 변경될 때 Inplace 또는 Recreate로 Requests 변경 1 2 ## vpa 상태 확인 $ kubectl describe vpa sample-vpa 모드 내용 Lower Bound Requests의 최소 추천값, 밑도는 경우 성능에 큰 영향 Upper Bound Requests의 최대 추천값, 웃도는 경우 낭비 Target Requests의 추천값, 가장 효율적 Uncapped Target 리소스 제약을 고려하지 않은 Requests 추천 값 Requests 변경으로 파드 재작성이 빈번하게 실행되는 것을 막기 위해 Target이 변경된 후 Requests를 Lower Bound보다 작거나 Upper Bound 보다 크게 설정하면 파드 변경됨 PodDisruptionBudget을 고려해 점차적으로 파드 변경 설정되지 않은 경우 절반씩 변경 ","date":"2022-06-18T17:12:04+09:00","image":"https://Haebuk.github.io/images/Kubernetes-Logo.wine.png","permalink":"https://Haebuk.github.io/p/%EB%A6%AC%EC%86%8C%EC%8A%A4%EA%B4%80%EB%A6%AC%EC%99%80-%EC%98%A4%ED%86%A0%EC%8A%A4%EC%BC%80%EC%9D%BC%EB%A7%81/","title":"리소스관리와 오토스케일링"},{"content":"컨피그 \u0026amp; 스토리지 API 카테고리 컨테이너 설정 파일, 패스워드 같은 기밀 정보 추가 영구 볼륨 제공 시크릿 컨피그맵 영구 볼륨 클레임 환경 변수 사용 개별 컨테이너 설정 내용은 환경 변수나 파일이 저장되어 있는 영역을 마운트하여 전달하는 것이 일반적 파드 템플릿에 env 또는 envFrom 지정 다음과 같은 정보를 환경 변수에 포함 가능 정적 설정 파드 정보 컨테이너 정보 시크릿 리소스 기밀 정보 컨피그맵 리소스 설정값 정적 설정 spec.containers[].env에 정적인 값 설정 1 2 ## sample-env 파드의 환경변수 확인 $ kubectl exec -it sample-env -- env | grep MAX_CONNECTION 컨테이너 기본 타임존: UTC -\u0026gt; 환경 변수 지정하여 변경 가능 1 2 3 4 ## 타임존 설정 env: - name: TZ value: Asia/Seoul 파드 정보 fieldRef를 통해 참조 가능 1 2 3 4 5 ## 파드가 기동 중인 노드 확인 $ kubectl get pods -o wide sample-env-pod ## sample-env-pod 파드 환경 변수 \u0026#39;K8S_NODE\u0026#39; 확인 $ kubectl exec -it sample-env-pod -- env | grep K8S_NODE 컨테이너 정보 resourceFieldRef를 통해 참조 가능 1 $ kubectl exec -it sample-env-container -- env | grep CPU 환경 변수 사용시 주의 사항 command나 args로 실행할 명령어를 지정할 때는 ${}가 아닌 $()로 지정 매니페스트 내부에 정의된 환경 변수만 참조 가능 시크릿 범용 시크릿(Opaque) 스키마리스 시크릿 kubectl로 파일에서 값을 참조하여 생성(\u0026ndash;from-file) kubectl로 envfile에서 값을 참조하여 생성(\u0026ndash;from-env-file) kubectl로 직접 값을 전달하여 생성(\u0026ndash;from-literal) 매니페스트에서 생성(-f) 하나의 시크릿당 저장 가능한 데이터 사이즈는 총 1MB kubectl로 파일에서 값을 참조하여 생성(\u0026ndash;from-file) 일반적으로 파일명이 그대로 키가 되므로 확장자는 붙이지 않는 것이 좋음 파일 생성시 개행 코드 없도록 주의 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 시크릿에 포함된 값을 파일로 내보내기 $ echo -n \u0026#34;root\u0026#34; \u0026gt; ./username $ echo -n \u0026#34;rootpassword\u0026#34; \u0026gt; ./password ## 파일에서 값 참조하여 시크릿 생성 $ kubectl create secret generic --save-config sample-db-auth \\ --from-file=./username --from-file=./password ## 시크릿 확인 $ kubectl get secrets sample-db-auth -o json | jq .data ## base64 인코딩되어 있음 $ kubectl get secrets sample-db-auth -o json | jq -r .data.username ## base64 디코드 $ kubectl get secrets sample-db-auth -o json | jq -r .data.username | base64 --decode kubectl로 envfile에서 값을 참조하여 생성(\u0026ndash;from-env-file) 하나의 파일에서 일괄적으로 생성하는 경우 1 2 $ kubectl create secret generic --save-config sample-db-auth \\ --from-env-file ./env-secret.txt kubectl로 값을 직접 전달하여 생성(\u0026ndash;from-literal) 1 2 $ kubectl create secret generic --save-config sample-db-auth \\ --from-literal=username=root --from-literal=password=rootpassword 매니페스트에서 생성(-f) base64로 제대로 인코드되었는지 확인 data가 아닌 stringData 필드를 사용하면 일반 텍스트로 작성 가능 TLS 타입 시크릿 인증서로 사용할 시크릿을 사용하는 경우 인그레스 리소스 등에서 사용하는 것이 일반적 매니페스트로 생성할 수 있지만 기본적으로 비밀키와 인증서 파일로 생성하는 것이 좋음 도커 레지스트리 타입 시크릿 컨테이너 레지스트리가 프라이빗 저장소인 경우에 인증 정보를 시크릿으로 정의하여 사용 가능 kubectl로 직접 생성하는 것이 편리 ~/.docker/config.json 파일 대체용으로 사용 1 2 3 4 5 6 7 8 9 10 11 12 ## 도커 레지스트리 인증 정보의 시크릿 생성 $ kubectl create secret docker-registry --save-config sample-registry-auth \\ --docker-server=REGISTRY_SERVER \\ --docker-username=REGISTRY_USER \\ --docker-password=REGISTRY_USER_PASSWORD \\ --docker-email=REGISTRY_USER_EMAIL ## base64로 인코드된 dockercfg 형식의 JSON 데이터 $ kubectl get secrets -o json sample-registry-auth | jq .data ## base64로 디코드한 dockercfg 형식의 JSON 데이터 $ kubectl get secrets sample-registry-auth -o yaml | grep \u0026#34;\\.dockerconfigjson\u0026#34; | awk -F \u0026#39; \u0026#39; \u0026#39;{print $2}\u0026#39; | base64 --decode 이미지 다운로드 시 시크릿 사용 인증이 필요한 도커 레지스트리의 프라이빗 저장소에 저장된 이미지를 다운로드할 때, 시크릿을 사전에 생성한 후 파드 정의 spec.imagePullSecrets에 docker-registry 타입의 시크릿 지정 imagePullSecrets는 복수 설정 가능 기본 인증 타입의 시크릿 사용자명과 패스워드로 인증하는 시스템을 사용하는 경우 kubectl로 직접 값을 전달하여 생성(\u0026ndash;from-literal) 리소스를 생성하지 않고 매니페스트를 출력하는 경우 \u0026ndash;dry-run, -o yaml 옵션 사용 1 2 3 4 ## 직접 옵션에서 type과 값을 지정하여 시크릿 생성 $ kubectl create secret generic --save-config sample-basic-auth \\ --type kubernetes.io/basic-auth \\ --from-literal=username=root --from-literal=password=rootpassword 매니페스트에서 생성(-f) type에 kubernetes.io/basic-auth 지정 데이터 스키마로 username과 password 지정 SSH 인증 타입의 시크릿 비밀키로 인증하는 시스템을 사용하는 경우 kubectl로 파일에서 값을 참조하여 생성(\u0026ndash;from-file) 1 2 3 4 5 6 7 ## SSH 비밀키 생성 $ ssh-keygen -t rsa -b 2048 -f sample-key -C \u0026#34;sample\u0026#34; ## 파일에서 type과 값을 참조하여 시크릿 생성 $ kubectl create secret generic --save-config sample-ssh-auth \\ --type kubernetes.io/ssh-auth \\ --from-file=ssh-privatekey=./sample-key 매니페스트에서 생성(-f) type에 kubernetes.io/ssh-auth 지정 데이터 스키마로 ssh-privatekey 지정 시크릿 사용 컨테이너에서 사용할 경우 두 가지 패턴이 존재 환경 변수로 전달 시크릿의 특정 키만 시크릿의 전체 키 볼륨으로 마운트 시크릿의 특정 키만 시크릿의 전체 키 환경 변수로 전달 특정 키를 전달할 경우 spec.containers[].env의 valueFrom.secretKeyRef 사용 env로 하나씩 정의하기 때문에 환경 변수명 지정 가능 전체를 전달할 경우 매니페스트가 길어지지는 않지만 시크릿에 어떤 값이 있는지 매니페스트 정의에서 알기 힘듦 여러 시크릿을 가져오면 키가 충돌할 수 있으므로 접두사를 붙여 충돌 방지 1 2 3 4 5 6 7 8 ## sample-secret-single-env 파드의 DB_USERNAME 확인 $ kubectl exec -it sample-secret-single-env -- env | grep DB_USERNAME ## sample-secret-multi-env 파드의 환경 변수 확인 $ kubectl exec -it sample-secret-multi-env -- env ## sample-secret-prefix-env 파드의 접두사가 DB인 환경 변수 확안 $ kubectl exec -it sample-secret-prefix-env -- env | egrep ^DB 볼륨으로 마운트 특정 키를 마운트하는 경우 spec.volumes[]의 secret.items[]를 사용 마찬가지로 시크릿 전체를 마운트 가능, 매니페스트가 길어지지 않지만 어떤 값이 있는지 알기 힘듦 1 2 3 4 5 ## sample-secret-single-volume 파드의 /config/username.txt 확인 $ kubectl exec -it sample-secret-single-volume -- cat /config/username.txt ## 파드 내부의 /config 디렉터리 내용 확인 $ kubectl exec -it sample-secret-multi-volume -- ls /config 동적 시크릿 업데이트 일정 기간마다(kubelet의 Sync Loop 타이밍) kube-apiserver로 변경 확인 변경이 있을경우 파일 교체(기본 60초) 주기를 조정하려면 kubelet의 \u0026ndash;sync-frequency 옵션 지정 환경 변수를 사용한 시크릿의 경우 파드를 기동할 때 환경 변수가 정해지므로 동적 변경 불가 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ## 시크릿에 마운트된 디렉터리 확인 $ kubectl exec -it sample-secret-multi-volume -- ls -la /config drwxrwxrwt 3 root root 120 Jun 11 08:47 . drwxr-xr-x 1 root root 42 Jun 11 08:47 .. drwxr-xr-x 2 root root 80 Jun 11 08:47 ..2022_06_11_08_47_42.732252402 lrwxrwxrwx 1 root root 31 Jun 11 08:47 ..data -\u0026gt; ..2022_06_11_08_47_42.732252402 lrwxrwxrwx 1 root root 15 Jun 11 08:47 password -\u0026gt; ..data/password lrwxrwxrwx 1 root root 15 Jun 11 08:47 username -\u0026gt; ..data/username ## 파드의 /config/username 파일 내용 확인 $ kubectl exec -it sample-secret-multi-volume -- cat /config/username ## 시크릿 변경 전 경과 시간 확인 $ kubectl get pods sample-secret-multi-volume ## 시크릿 내용 업데이트 $ cat \u0026lt;\u0026lt; EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: sample-db-auth type: Opaque data: ## root \u0026gt; admin으로 변경 username: YWRtaW4= EOF ## 시크릿에 마운트된 디렉터리 확인 $ kubectl exec -it sample-secret-multi-volume -- ls -la /config total 0 drwxrwxrwt 3 root root 100 Jun 11 08:54 . drwxr-xr-x 1 root root 42 Jun 11 08:47 .. drwxr-xr-x 2 root root 60 Jun 11 08:54 ..2022_06_11_08_54_21.841196102 lrwxrwxrwx 1 root root 31 Jun 11 08:54 ..data -\u0026gt; ..2022_06_11_08_54_21.841196102 lrwxrwxrwx 1 root root 15 Jun 11 08:47 username -\u0026gt; ..data/username ## root에서 admin으로 변경됨 $ kubectl exec -it sample-secret-multi-volume -- cat /config/username ## 동적으로 파일이 변경된 후의 경과시간 확인 -\u0026gt; 파드가 재생성되지 않음 $ kubectl get pods sample-secret-multi-volume 시크릿 내용을 username으로만 하고 kubectl apply를 실행하여 그 외의 파일(password) 삭제됨 처음 시크릿 생성시 kubectl apply나 kubectl create \u0026ndash;save-config를 사용하지 않은 경우 매니페스트 병합 처리가 불완전하여 결과가 달라짐 1 2 ## password 파일은 삭제됨 $ kubectl exec -it sample-secret-multi-volume -- ls /config 컨피그맵 설정 정보 등을 key-value로 저장할 수 있는 데이터 저장 리소스 하나의 컨피그맵마다 저장할 수 있는 사이즈 총 1MB Generic 타입의 시크릿과 거의 동일한 방법으로 생성 kubectl로 파일에서 값을 직접 참조 kubectl로 직접 값을 전달 매니페스트로 생성 매니페스트로 생성시 시크릿과 다르게 base64로 인코드되지 않고 추가됨 value를 여러 행으로 전달할 경우 YAML 문법에 맞게 Key: |등과 같이 다음 행부터 정의 숫자는 큰 따옴표로 둘러싸기 1 2 3 4 5 6 7 8 ## 파일로 컨피그맵 생성 $ kubectl create configmap --save-config sample-configmap --from-file=./nginx.conf ## 컨피그맵에 등록된 데이터 확인 1 $ kubectl get configmaps sample-configmap -o json | jq .data ## 컨피그맵에 등록된 데이터 확인 2 $ kubectl describe configmap sample-configmap binaryData 필드를 사용하여 UTF-8이외의 데이터를 포함하는 바이너리 데이터도 저장 가능(시크릿도 동일) 매니페스트 파일로 저장하려면 \u0026ndash;dry-run=client -o yaml 옵션 사용 1 2 3 4 5 6 7 8 9 10 11 $ kubectl create configmap sample-configmap-binary \\ --from-file image.jpg \\ --from-literal=index.html=\u0026#34;Hello, Kubernetes\u0026#34; \\ --dry-run=client -o yaml \\ \u0026gt; sample-configmap-binary.yaml ## 로컬 8080 포트에서 파드의 80 포트로 포트 포워딩 $ kubectl port-forward sample-configmap-binary-webserver 8080:80 ## 브라우저로 표시 $ open http://localhost:8080/image.jpg 1 2 3 ## 인수에 각 직접 전달 $ kubectl create configmap --save-config web-config \\ --from-literal=connection.max=100 --from-literal=connection.min=10 컨피그맵 사용 두 가지 방법 환경 변수로 전달 특정 키만 spec.containers[].env의 valueFrom.configMapKeyRef 사용 전체 키 변수 명에 \u0026lsquo;.\u0026rsquo;, \u0026lsquo;-\u0026rsquo; 사용하지 않는 것이 좋음 여러 행의 경우 볼륨으로 마운트하는 것을 권장 볼륨으로 마운트 특정 키만 spec.volumes[]의 configMap.items[] 사용 전체 키 1 2 3 4 5 6 7 8 9 10 11 ## 파드의 CONNECTION_MAX 환경 변수 내용 확인 $ kubectl exec -it sample-configmap-single-env -- env | grep CONNECTION_MAX ## 파드의 여러 환경 변수 확인 $ kubectl exec -it sample-configmap-multi-env -- env ## 파일로 저장된 컨피그맵 확인 $ kubectl exec -it sample-configmap-single-volume -- cat /config/nginx-sample.conf ## 파드에 마운트된 /config 아래 파일 확인 $ kubectl exec -it sample-configmap-multi-volume -- ls /config 시크릿과 컨피그맵의 공통 주제 사용 구분 시크릿 데이터는 etcd에 저장됨 쿠버네티스 노드상에 영구적으로 데이터가 남지 않게 tmpfs 영역에 저장 base64로 인코드되어 있어 화면에서 판단하기 어려우나 단순 base64 인코드 이므로 깃 저장소 업로드는 금지 시크릿을 암호화하는 OSS나 Vault와 같은 서드 파티 솔류션 사용 마운트 시 퍼미션 변경 파드에서 실행하는 경우 볼륨을 생성할 때 실행 권한 부여 가능 기본값 0644(rw-r\u0026ndash;r\u0026ndash;)로 마운트 퍼미션은 8진수 표기에서 10진수 표기로 변환한 형태를 사용 동적 컨피그맵 업데이트 볼륨 마운트 사용시 일정 기간 마다 파일 교체 (기본값 60초) 환경 변수를 사용한 컨피그맵은 동적 업데이트 불가 데이터 변경 거부 immutable 설정 변경하면 데이터 변경 방지 가능 변경하려면 리소스를 삭제하고 다시 생성 볼륨 마운트의 경우 파드 재생성 필요 볼륨, 영구 볼륨, 영구 볼륨 클레임의 차이 볼륨 미리 준비된 사용 가능한 볼륨을 매니페스트에 직접 지정하여 사용 사용자가 설정된 볼륨을 사용할 수 있지만 신규 볼륨 생성 또는 기존 볼륨 삭제 불가 매니페스트에서 볼륨 리소스 생성 불가 영구 볼륨 외부 영구 볼륨을 제공하는 시스템과 연계하여 신규 볼륨 생성 또는 기존 볼륨 삭제 가능 매니페스트에서 영구 볼륨 리소스를 별도로 생성하는 형태 영구 볼륨 클레임 영구 볼륨 리소스를 할당하는 리소스 영구 볼륨은 클러스터에 볼륨을 등록만 함 -\u0026gt; 실제 사용하려면 영구 볼륨 클레임 정의 동적 프로비저닝 기능 사용시 영구 볼륨 클레임이 사용된 시점에 영구 볼륨 동적으로 생성 가능 볼륨 추상화하여 파드와 느슨하게 결합된 리소스 emptyDir hostPath downwardAPI projected nfs iscsi cephfs 파드에 정적으로 볼륨을 지정 -\u0026gt; 플러그인에 따라 충돌 가능성 emptyDir 파드용 임시 디스크 영역으로 사용 가능 파드 종료시 삭제 호스트의 임의 영역 마운트 불가 호스트의 파일 참조 불가 1 2 ## 기동 중인 쿠버네티스 노드의 디스크 영역 할당 확인 $ kubectl exec -it sample-emptydir -- df -h | grep /cache emptyDir.sizeLimit으로 리소스 제한 가능 용량 초과시 Evict(축출) 1 2 3 4 5 ## 150MB 파일을 /cache/dummy에 생성 $ kubectl exec -it sample-emptydir-limit -- dd if=/dev/zero of=/cache/dummy bs=1M count=150 ## 파드 상태 모니터링 $ kubectl describe pods sample-emptydir-limit 고속 tmpfs 메모리 영역 사용 가능 emptyDir.medium에 Memory 지정 컨테이너에 대한 메모리 사용 상한 설정에도 영향을 줌 1 2 ## tmpfs 영역 할당 확인 $ kubectl exec -it sample-emptydir-memory -- df -h | grep /cache hostPath 쿠버네티스 노드상의 영역을 컨테이너에 매핑하는 플러그인 호스트의 임의 영역 마운트 가능 어떤 영역 사용할지 지정 type: Directory/DirectoryOrCreate/File/Socket/BlockDevice 등 Directory/DirectoryOrCreate 차이: 디렉터리가 존재하지 않을 때 생성 후 기동 유무 보안상의 이유로 안전하지 않은 컨테이너가 업로드될 수 있으므로 사용하지 않는 것이 좋음 1 2 3 4 5 ## 호스트 OS 이미지 확인 $ kubectl exec -it sample-hostpath -- cat /srv/os-release | grep PRETTY_NAME ## 컨테이너 OS 이미지 확인 $ kubectl exec -it sample-hostpath -- cat /etc/os-release | grep PRETTY_NAME downwardAPI 파드 정보 등을 파일로 배치하기 위한 플러그인 환경 변수 fieldRef와 ResourceFieldRef 사용 방법과 동일 1 2 ## 파드 정보등이 파일로 배치 $ kubectl exec -it sample-downward-api -- ls /srv projectd 시크릿/컨피그맵/downwardAPI/serviceAccountToken의 볼륨 마운트롤 하나의 디렉터리에 통합하는 플러그인 1 2 3 4 5 6 7 8 ## /srv 디렉터리 확인 $ kubectl exec -it sample-projectd -- ls /srv ## /srv/configmap 디렉터리 확인 $ kubectl exec -it sample-projectd -- ls /srv/configmap ## /srv/secret 디렉터리 확인 $ kubectl exec -it sample-projectd -- ls /srv/secret 영구볼륨(PV) 기본적으로 네트워크를 통해 디스크를 attach하는 디스크 타입 개별 리소스로 생성 후 사용 pluggable한 구조로 되어 있음 생성 레이블 동적 프로비저닝을 사용하지 않고 영구 볼륨을 생성하는 경우 영구 볼륨 종류를 알 수 없으므로, 레이블을 사용하는 것이 좋음 용량 동적 브로비저닝을 사용할 수 없는 상황에서는 작은 용량의 영구 볼륨도 준비해야함 (가장 비슷한 용량이 할당되므로) 접근 모드 ReadWriteOnce: 단일 노드에서 Read/Write 가능 ReadOnlyMany: 여러 노드에서 Read 가능 하나라도 쓰기 요청이 있는 파드가 있으면 다른 노드에서 마운트 불가능 파드에서 영구 볼륨 지정할 때 ReadOnly 지정 ReadWriteMany: 여러 노드에서 Read/Write 가능 Reclaim Policy 영구 볼륨 사용 후 처리 방법을 제어하는 정책 영구 볼륨 클레임에서 사용된 후 영구 볼륨 클레임이 삭제되었을 때 영구 볼륨 자체의 동작 설정 세가지 방법 존재 Delete: 영구 볼륨 자체가 삭제 GCP/AWS 등에서 확보되는 외부 볼륨의 동적 프로비저닝 때 사용되는 경우가 많음 Retain: 영구 볼륨 삭제하지 않고 유지 또다른 PVC에 의해 다시 마운트 되지는 않음 Recycle: 영구 볼륨 데이터 삭제 후 재사용 가능 상태로 만듦 다른 영구 볼륨 클레임에서 마운트 가능 동적 프로비저닝을 사용하는 것이 좋음 스토리지클래스 동적으로 영구 볼륨을 프로비저닝하는 구조 영구 볼륨 클레임(PVC) 영구 볼륨을 요청하는 리소스 PVC에서 지정된 조건(용량, 레이블)을 기반으로 PV에 대한 요청이 들어오면 스케줄러는 현재 가지고 있는 PV에서 적당한 볼륨을 할당 설정 다음과 같은 항목 설정 가능 레이블 셀렉터 용량 접근 모드 스토리지클래스 PVC 용량이 PV 보다 작아야 할당(PV보다 큰 용량이 할당됨) 파드에서 사용하려면 spec.volumes에 persistentVolumeClaim.claimName 지정 동적 프로비저닝 PVC가 생성되는 타이밍에 동적으로 영구 볼륨 생성 사전에 영구 볼륨을 생성할 필요 없으며, 용량 낭비 발생하지 않음 사전에 어떤 PV를 생성할지 정의한 스토리지클래스 생성 영구 볼륨 할당 타이밍 제어 동적 프로닝 사용 -\u0026gt; PVC 생성시 파드에 PVC가 붙어있지 않아도 PV가 생성됨 실제 파드에 붙기전에 PV가 생성되고 연결할 수 있음 volumeBindingMode 설정값 설정값 개요 Immediate(기본값) 즉시 PV가 생성되고 연결할 수 있게 됨 WaitForFirstConsumer 처음으로 파드에 사용될 때 PV가 생성되고 연결할 수 있게 됨 PVC 조정을 사용한 볼륨 확장 동적 프로비저닝을 사용하고 크기 조정을 지원하는 볼륨 플러그인을 사용할 땐 PVC 확장 가능 사전에 스토리지클래스에 allowVolumeExpantion:true 설정 축소는 불가 1 2 3 4 5 ## 마운트된 PV 크기 확인 $ kubectl exec -it sample-pvc-resize-pod --df -h | grep /usr/share/nginx/html ## PVC에서 요청하는 용량 변경 $ kubectl patch pvc sample-pvc-resize --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;resources\u0026#34;: {\u0026#34;requests\u0026#34;: {\u0026#34;storage\u0026#34;: \u0026#34;16Gi\u0026#34;}}}}\u0026#39; 스테이트풀셋에서 PVC spec.volumeClaimTemplate 항목을 사용하면 PVC를 별도 정의하지 않아도 자동으로 생성 가능 volumeMounts에서 사용 가능한 옵션 읽기 전용(ReadOnly) 마운트 여러 볼륨을 컨테이너에 마운트할 때 readonly 옵션 지정 가능 hostPath는 컨테이너에 호스트 영역을 보여주므로 보안상 좋지 않음 최소한 ReadOnly로 마운트하자 subPath 볼륨 마운트시 특정 디렉터리를 루트로 마운트하는 기능 각 컨테이너가 하나의 볼륨을 사용하면서도 서로 영향이 없도록 디렉터리 나눌 수 있음 1 2 3 4 5 ## subPath /path1을 지정한 컨테이너 $ kubectl exec -it sample-subpath -c container-b -- find /data ## subPath를 지정하지 않은 컨테이너 $ kubectl exec -it sample-subpath -c container-a -- find /data ","date":"2022-06-16T17:11:40+09:00","image":"https://Haebuk.github.io/p/%EC%BB%A8%ED%94%BC%EA%B7%B8%EC%99%80%EC%8A%A4%ED%86%A0%EB%A6%AC%EC%A7%80-api/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EC%BB%A8%ED%94%BC%EA%B7%B8%EC%99%80%EC%8A%A4%ED%86%A0%EB%A6%AC%EC%A7%80-api/","title":"컨피그와스토리지 API"},{"content":"서비스 API 카테고리 클러스터 컨테이너에 대한 엔드포인트를 제공하거나 레이블과 일치하는 컨테이너의 디스커버리에 사용되는 리소스\n서비스 ClusterIP ExternalIP(ClusterIP의 한 종류) NodePort LoadBalancer Headless(None) ExternalName Node-Selector 인그레스 파드는 서비스를 사용하지 않고도 파드간 통신이 가능하나, 서비스를 사용하면 두 가지 큰 장점이 있음\n파드에 트래픽 로드 밸런싱 서비스 디스커버리와 클러스터 내부 DNS 파드에 트래픽 로드 밸런싱 수신한 트래픽을 여러 파드에 로드 밸런싱 ClusterIP 클러스터 내부에서만 사용 가능한 가상 IP를 가진 엔드포인트 제공하는 로드 밸런서 구성 spec.selector에 정의한 조건에 따라 트래픽 전송 1 2 3 4 5 6 7 8 ## 서비스 생성 $ kubectl apply -f sample-clusterip.yaml ## 지정한 레이블을 가진 파드 중 특정 JSON Path를 컬럼으로 출력 $ kubectl get pods -l app=sample-app -o custom-columns=\u0026#34;NAME:{metadata.name},IP:{status.podIP}\u0026#34; ## 서비스 상세 정보 확인 $ kubectl describe service sample-clusterip Endpoints에 app=sample-app 라벨을 가진 파드의 IP 정보가 있음 Endpoints 항목에 아무것도 없을 경우 셀렉터 조건이 맞지 않을 가능성 있음 1 2 3 4 5 ## 일시적으로 파드를 시작하여 서비스 엔드포인트로 요청 ### (여러 번 실행 시 비슷한 빈도로 세 개의 파드명 표시) $ kubectl run --image=amsy810/tools:v2.0 --restart=Never --rm -i testpod --command -- curl -s http://{ClusterIP}:8080 Host=10.100.50.111 Path=/ From=sample-deployment-687d589688-8nbcs ClientIP=172.31.9.8 XFF= pod \u0026#34;testpod\u0026#34; deleted 여러 포트 할당 하나의 서비스에 여러 포트 할당 가능 (바람직) 이름을 사용한 포트 참조 파드의 포트 정의에 이름을 지정하면 이름을 사용하여 참조 가능 1 2 3 4 5 ## 서비스 목적지 엔드포인트 확인 $ kubectl describe service sample-named-port-service ## 파드 IP 주소 확인 $ kubectl get pods -o wide 클러스터 내부 DNS와 서비스 디스커버리 서비스에 속한 파드를 보여주거나 서비스명에서 엔드포인트를 반환하는 것 서비스 디스커버리 방법 환경 변수 사용 DNS A 레코드 사용 DNS SRV 레코드 사용 환경 변수를 사용한 서비스 디스커버리 파드 내부에서는 환경 변수에서도 같은 네임스페이스 서비스 확인 가능 -이 포함된 서비스명은 _로 변경 후 대문자 변환됨 파드 생성 후 서비스 생성 또는 삭제에 따라 변경된 환경 변수가 기존 파드에 자동 등록되지 않음 먼저 생성한 파드 재생성 필요 1 2 ## 환경 변수에 등록된 서비스 정보 확인 $ kubectl exec -it sample-deployment-{}-{} -- env | grep -i kubernetes spec.enableServiceLinks를 false로 지정시 환경 변수 추가 비활성화 (기본값 true) DNS A 레코드를 사용한 서비스 디스커버리 IP 주소를 편하게 관리하기 위해 기본적으로 자동 할당된 IP 주소에 연결된 DNS 명을 사용하는 것이 좋음 서비스명의 이름 해석이 수행되고 해당 ip로 요청이 발송 다른 네임스페이스의 경우 sample-cluster.default와 같이 네임스페이스명을 붙여 이름 해석해야 함 1 2 3 ## 일시적으로 파드 기동하여 컨테이너 내부에서 sample-clusterip:8080으로 HTTP 요청 $ kubectl run --image=amsy810/tools:v2.0 --restart=Never --rm -i testpod \\ --command -- curl -s http://sample-clusterip:8080 DNS SRV 레코드를 사용한 서비스 디스커버리 포트명과 프로토콜을 사용해 서비스를 제공하는 포트 번호를 포함한 엔드포인트를 DNS로 해석 _서비스 포트명._프로토콜.서비스명.네임스페이스명.svc.cluster.local 1 2 3 4 5 6 7 8 9 10 11 ## 일시적으로 파드 기동하여 SRV 레코드가 다른 파드에서 해석이 가능한지 확인 $ kubectl run --image=amsy810/tools:v2.0 --restart=Never --rm -i testpod \\ --command -- dig _http-port._tcp.sample-clusterip.default.svc.cluster.local SRV ;; QUESTION SECTION: ;_http-port._tcp.sample-clusterip.default.svc.cluster.local. IN SRV ;; ANSWER SECTION: _http-port._tcp.sample-clusterip.default.svc.cluster.local. 5 IN SRV 0 100 8080 sample-clusterip.default.svc.cluster.local. ;; ADDITIONAL SECTION: sample-clusterip.default.svc.cluster.local. 5 IN A 10.100.50.111 목적지 호스트명 sample-clusterip.default.svc.cluster.local과 포트번호 8080 해석 가능 클러스터 내부 DNS와 클러스터 외부 DNS 파드의 DNS 서버 설정을 명시적으로 하지 않으면 클러스터 내부 DNS 사용하여 이름 해석 수행 내부 이외의 레코드는 외부 DNS에 재귀 질의 해야함 노드 로컬 DNS 캐시 대규모 클러스터에서 성능 향상을 위해 각 노드의 로컬에 DNS 캐시 서버를 포함하는 구조도 존재 활성화한 환경에서 파드의 질의 대상은 같은 노드에 있는 로컬 DNS 캐시 서버 ClusterIP 서비스 클러스터 내부에서만 통신 가능한 가상 IP 할당 클러스터 외부에서 트래픽을 수신할 필요가 없는 환경에서 내부 로드 밸런서로 활용 ExternalIP 서비스 지정한 쿠버네티스 노드 IP 주소:포트에서 수신한 트래픽을 컨테이너로 전달하는 형태로 외부와 통신 특별한 이유가 없다면 NodePort 서비스를 사용하는것이 좋음 type: ExternalIP를 지정하는 것이 아님 (ClusterIP에 해당함) NodePort 서비스 모든 쿠버네티스 노드 IP 주소:포트에서 수신한 트래픽을 컨테이너로 전달하는 형태로 외부와 통신 모든 쿠버네티스 노드의 IP 주소에서 해당 포트를 listen 하기 때문에 충돌 주의 할당된 포트 번호를 지정할 필요가 없을 경우, 포트를 지정하지 않으면 빈 포트 번호가 자동으로 선택됨 설정 항목 개요 spec.ports[].port ClusterIP에서 수신할 포트 번호 spec.ports[].targetPort 목적지 컨테이너 포트 번호 spec.ports[].nodePort 모든 쿠버네티스 노드 IP 주소에서 수신할 포트 번호 NodePort 주의점 가용한 포트 범위는 대부분 30000 ~ 32767 여러 NodePort 서비스에서 같은 포트 사용 불가 LoadBalancer 서비스 클러스터 외부로부터 트래픽을 수신할 때 가장 실용적인 서비스 쿠버네티스 노드와 별도로 외부 로드 밸런스를 사용 -\u0026gt; 노드 장애에도 크게 문제가 되지 않음 (장애 발생한 노드에는 트래픽을 전송하지 않음) 장애 감지 까지 일시적으로 서비스 중단 현상이 발생할 수 있음 생성시 EXTERNAL-IP가 \u0026lt;pending\u0026gt; 상태임 로드밸런서를 백그라운드에서 생성중이기 때문에 아직 미할당된 상태 정적으로 외부 LB에서 사용하는 IP 주소 지정 가능 방화벽을 지정하여 접속 제한 가능 1 2 3 ## 외부에서 통신 ## 여러번 실행 시 비슷한 빈도로 세 개의 파드명 표시 $ curl -s http://{lb EXTERNAL-IP}:8080 그 외 서비스 세션 어피니티 트래픽을 서비스에 연결된 어느 하나의 파트에 전송되면, 그 파드에 계속 보내고 싶을 때 사용 최대 세션 고정 시간 설정 가능 노드 간 통신 제외와 발신 측 IP 주소 유지 NodePort, LoadBalancer 서비스에서 노드에 도착한 요청은 2단계 로드 밸런싱이 이루어짐(레이턴시 오버헤드 발생) 발신 측 IP 주소가 유실되는 특징 데몬셋은 하나의 노드에 하나의 파드만 배치되므로 같은 노드에만 통신하고 싶은 경우 NodePort 서비스의 spec.externalTrafficPolicy를 local로 설정 해당 노드의 요청은 그 노드상에 있는 파드에만 전송 두 개 이상의 파드 존재시 균등 전송 파드가 없다면 요청에 응답할 수 없으므로 가능하면 사용 X LoadBalancer 서비스의 spec.externalTrafficPolicy를 local로 설정 별도의 헬스 체크용 NodePort가 있어 파드가 존재하지 않는 노드에는 요청이 전송되지 않음 헤드리스 서비스(None) 대상이 되는 개별 파드의 IP 주소가 직접 반환되는 서비스 로드 밸런싱을 위한 IP 주소는 제공되지 않음 DNS 라운드 로빈을 사용한 엔드포인트 제공 스테이트풀셋이 헤드리스 서비스를 사용하는 경우에만 파드명으로 IP주소를 찾을 수 있음 생성 조건 서비스의 spec.type이 ClusterIP일 것 서비스의 spec.ClusterIP가 None일 것 [옵션] 서비스의 metadata.name이 스테이트풀셋의 spec.serviceName과 같을 것 파드명으로 디스커버리하는 경우 ExternalName 서비스 외부도메인으로 CNAME 반환 다른 이름을 설정하고 싶거나, 클러스터 내부에서의 엔드포인트를 쉽게 변경하고 싶을 때 사용 외부 서비스와 느슨한 결합 확보 목적지가 변경되었을 때 ExternalName 서비스를 변경하는 것만으로 가능 ClusterIP 서비스에서 ExternalName 서비스로 전환할 경우 spec.clusterIP를 명시적으로 공란으로 두어야함 None-Selector 서비스 서비스명으로 이름 해석시 자신이 설정한 멤버에 대해 로드 밸런싱 수행 externalName을 지정하지 않고 셀렉터가 존재하지 않는 서비스 생성 + 엔드포인트 리소스 수동 생성하면 유연한 서비스 생성 가능 쿠버네티스 외부에 애플리케이션 서버에 대한 요청을 분산하는 경우에도 쿠버네티스 서비스 사용 가능 서비스 환경과 스테이징 환경에서 클러스터 내외부를 분리해도 애플리케이션에서 같은 ClusterIP로 보여줄 수 있음 인그레스 서비스들을 묶는 서비스들의 상위 객체 리소스와 컨트롤러 매니페스트를 쿠버네티스에 등록하는 것만으로는 아무 처리도 일어나지 않음 실제 처리를 하는 컨트롤러라는 시스템 구성 요소 필요 인그레스 리소스: 매니페스트에 등록된 API 리소스 인그레스 컨트롤러: 인그레스 리소스가 쿠버네티스에 등록되었을 때 어떠한 처리 수행 인그레스 종류 클러스터 외부 로드 밸런서를 사용한 인그레스 GKE 인그레스 클러스터 내부 로드 밸런서를 사용한 인그레스 Nginx 인그레스 클러스터 외부 로드 밸런서를 사용한 인그레스 인그레스 리소스 생성만으로 로드 밸런서의 가상 IP가 할당되어 사용 가능 순서(단계) 클라이언트 -\u0026gt; L7 로드 밸런서(NodePort 경유) -\u0026gt; 목적지 파드 클러스터 내부에 인그레스용 파드를 배포하는 인그레스 인그레스 리소스에서 정의한 L7 수준의 로드 밸런싱 처리를 하기 위해 인그레스용 파드를 클러스터 내부에 생성해야 함 클러스터 외부에서 접속할 수 있도록 별도로 인그레스용 파드에 LoadBalancer 서비스를 생성하는 등의 준비 필요 SSL 터미네이션이나 경로 기반 라우팅 등과 같은 L7 수준의 처리를 위해 부하에 따른 레플리카 수의 오토 스케일링 고려해야 함 순서(단계) 클라이언트 -\u0026gt; L4 로드 밸런서(type: LoadBalancer) -\u0026gt; Nginx 파드(Nginx 인그레스 컨트롤러) -\u0026gt; 목적지 파드 정리 파드 서비스 디스커버리나 L4 로드 밸런싱 기능을 제공하기 위한 서비스 리소스 L7 로드 밸런싱 기능을 제공하는 인그레스 서비스 L4 로드 밸런싱 클러스터 내부 DNS를 사용한 이름 해석 레이블을 사용한 파드의 디스커버리 인그레스 L7 로드 밸런싱 SSL 터미네이션 경로 기반 라우팅 서비스 종류 IP 엔드포인트 내용 ClusterIP 쿠버네티스 클러스터 내부에서만 통신 가능한 가상 IP ExternalIP 특정 쿠버네티스 노드의 IP 주소 NodePort 모든 쿠버네티스 노드의 모든 IP 주소(0.0.0.0) LoadBalancer 클러스터 외부에서 제공되는 로드 밸런서의 가상 IP Headless(None) 파드의 IP 주소를 사용한 DNS 라운드 로빈 ExternalName CNAME을 사용한 느슨한 연결 확보 None-Selector 원하는 목적지 멤버를 설정할 수 있는 다양한 엔드 포인트 인그레스 종류 구현 예제 클러스터 외부 로드 밸런서를 사용한 인그레스 GKE 클러스터 내부에 인그레스용 파드를 배포하는 인그레스 Nginx 인그레스 ","date":"2022-06-11T23:00:00+09:00","image":"https://Haebuk.github.io/p/%EC%84%9C%EB%B9%84%EC%8A%A4-api/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EC%84%9C%EB%B9%84%EC%8A%A4-api/","title":"서비스 API"},{"content":"워크로드 API 카테고리 클러스터에 컨테이너를 기동시키기 위해 사용되는 리소스 Pod Replication Controller(Deprecated) ReplicaSet Deployment DaemonSet StatefulSet Job CronJob 파드 워크로드 리소스의 최소 단위 파드 디자인 패턴 종류 개요 사이드카 패턴 메인 컨테이너에 기능 추가 앰배서더 패턴 외부 시스템과의 통신 중계 어댑터 패턴 외부 접속을 위한 인터페이스 제공 파드 명령어 1 2 ## 파드 생성 $ kubectl apply -f sample-pod.yaml 1 2 ## 파드 목록 표시 $ kubectl get pods 1 2 ## 파드 상세 정보 표시 $ kubectl get pods --output wide 1 2 3 4 5 6 7 8 9 ## 컨테이너에서 /bin/bash 실행 $ kubectl exec -it sample-pod -- /bin/bash root@sample-pod:/## (이후 컨테이너 내부에서 명령어 실행 가능) ## 컨테이너에서 ls 명령어 실행 $ kubectl exec -it sample-pod -- /bin/ls ## 다수의 컨테이너 포함한 파드의 경우 특정 컨테이너 지정 가능 $ kubectl exec -it sample-2pod -c nginx-container -- /bin/ls 파드 주의 사항 파드 내 컨테이너가 같은 포트로 바인드되면 안됨 쿠버네티스는 ENTRYPOINT를 command, CMD를 args라고 부름 파드명 제한 영어 소문자 또는 숫자 기호는 \u0026lsquo;-\u0026rsquo; 또는 \u0026lsquo;.\u0026rsquo; 시작과 끝은 영어 소문자 레플리카셋/레플레케이션 컨트롤러 파드의 레플리카를 생성하고 지정한 파드 수를 유지하는 리소스 노드나 파드에 장애가 발생했을 때도 지정한 파드 수를 유지하기 위해 다른 노드에서 파드를 기동하므로 장애 시에 많은 영향 받지 않음 모니터링은 특정 레이블을 가진 파드 수를 계산하는 형태로 이루어짐 레플리카셋 명령어 1 2 3 4 5 ## 레플리카셋 생성 $ kubectl apply -f sample-rs.yaml ## 레플리카셋 확인 $ kubectl get replicasets -o wide 1 2 3 ## 레이블 지정하여 파드 목록 표시 ## 레플리카셋 이름-임의의 문자열로 명명 ex. sample-rs-cnvm5 kubectl get pods -l app=sample-app -o wide 1 2 ## 레플리카셋 상세 정보 표시 (증감 이력 등) ## kubectl describe replicaset sample-rs 레플리카셋 주의점 spec.selector와 spec.template.metadata.labels의 레이블의 일치하지 않으면 에러 외부에서 같은 레이블을 가진 파드가 있으면 레플리카 수에 충족되는 만큼 파드 삭제 레플리카셋 스케일링 두 가지 방법 매니페스트 수정하여 kubectl apply -f 명령어 실행 IaC(Infrastructure as Code)를 구현하기 위해서라도 해당 방법 권장 1 2 $ sed -i -e \u0026#39;s|replicas: 3|replicas: 4|\u0026#39; sample-rs.yaml $ kubectl apply -f sample-rs.yaml kubectl scale 명령어를 사용하여 스케일 처리 레플리카셋 이외에도 디플로이먼트/스테이트풀셋/잡/크론잡에서 사용 가능 1 2 ## 레플리카 수를 5로 변경 $ kubectl scale replicaset sample-rs --replicas 5 디플로이먼트 여러 레플리카셋을 관리하며 롤링 업데이트나 롤백 등을 구현하는 리소스 디플로이먼트 -\u0026gt; 레플리카셋 -\u0026gt; 파드 (관리 순서) 롤링 업데이트 과정 신규 레플리카셋 생성 신규 레플리카셋 레플리카 수(파드 수)를 단계적으로 늘림 이전 레플리카셋의 레플리카 수(파드 수)를 단계적으로 줄임 2,3 반복 이전 레플리카셋은 레플리카 수를 0으로 유지 파드를 하나만 기동해도 디플로이먼트 사용을 권장(파드 장애시 자동 재생성, 롤링 업데이트 등) 디플로이먼트 명령어 1 2 ## --record를 사용하여 업데이트 이력을 저장. 디플로이먼트 가동 (deprecated option) $ kubectl apply -f sample-deployment.yaml --record 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ## 디플로이먼트 확인 $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE sample-deployment 3/3 3 3 2m30s ## 레플리카셋 확인 $ kubectl get replicasets NAME DESIRED CURRENT READY AGE sample-deployment-77c7b569f6 3 3 3 2m39s ## 파드 확인 $ kubectl get pods NAME READY STATUS RESTARTS AGE sample-deployment-77c7b569f6-bnbcw 1/1 Running 0 2m44s sample-deployment-77c7b569f6-fzh2m 1/1 Running 0 2m44s sample-deployment-77c7b569f6-wjvbn 1/1 Running 0 2m44s 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## 컨테이너 이미지 업데이트 $ kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record ## 디플로이먼트 업데이트 상태 확인 $ kubectl rollout status deployment sample-deployment ## 변경 이력 확인 $ kubectl rollout history deployment sample-deployment ## 초기 상태의 디플로이먼트 $ kubectl rollout history deployment sample-deployment --revision 1 ## 한 번 업데이트된 후의 디플로이먼트 $ kubectl rollout history deployment sample-deployment --revision 2 ## 버전 지정하여 롤백 $ kubectl rollout undo deployment sample-deployment --to-revision 1 ## 직전 버전으로 롤백 $ kubectl rollout undo deployment sample-deployment 1 2 3 4 5 ## 업데이트 일시 정지 $ kubectl rollout pause deployment sample-deployment ## 업데이트 일시 정지 해제 $ kubectl rollout resume deployment sample-deployment 1 2 3 4 5 6 ## 레플리카 수를 3에서 4로 변경한 매니페스트를 apply - 디플로이먼트 스케일링 $ sed -i -e \u0026#39;s|replicas: 3|replicas: 4|\u0026#39; sample-deployment.yaml $ kubectl apply -f sample-deployment.yaml ## kubectl scale 명령어 $ kubectl scale deployment sample-deployment --replicas=5 1 2 ## 매니페스트를 사용하지 않고 명령어로 디플로이먼트 생성 $ kubectl create deployment sample-deployment-by-cli --image nginx:1.16 디플로이먼트 업데이트 전략 Recreate 모든 파드 삭제하고 다시 파드 생성 다운타임 발생하지만 추가 리소스를 사용하지 않으며 전환이 빠름 기존 레플리카셋의 레플리카 수를 0으로 하고 리소스 반환 신규 레플리카셋 생성하고 레플리카 수 늘림 RollingUpdate 업데이트 중 정지 가능 최대 파드 수(maxUnavailable)와 생성 가능한 최대 파드 수(maxSurge)를 설정 가능 추가 리소스를 사용하지 않도록 하거나 많은 리소스를 소비하지 않고 빠르게 전환하는 등의 동작 제어 가능 maxUnavailable과 maxSurge 모두 0은 불가능 상세 업데이트 파라미터 minReadySeconds(최소 대기 시간(초)) 파드가 Ready 상태가 된 후 디플로이먼트 리소스에서 파드 기동이 완료되었다고 판단(다음 파드의 교체가 가능하다고 판단)하기까지의 최소 시간 revisionHistoryLimit(수정 버전 기록 제한) 디플로이먼트가 유지할 레플리카셋 수 롤백이 가능한 이력 수 progressDeadlineSeconds(진행 기한 시간(초)) Recreate/RollingUpdate 처리 타임아웃 시간 타임아웃 시간 경과시 자동 롤백 디플로이먼트 주의사항 실제 환경에서는 롤백 보다는 이전 매니테스트를 kubectl apply를 실행하여 적용하는 것이 호환성 면에서 좋음 pause 상태에서는 업데이트가 즉시 반영되지 않고, 롤백도 되지 않음 데몬셋 레플리카셋의 특수 형태 (각 노드에 파드를 하나씩 배치) 노드를 늘렸을 때도 데몬셋의 파드는 자동으로 늘어난 노드에서 기동 로그 수집또는 모니터링 프로세스를 위해 사용 데몬셋 명령어 1 2 ## 데몬셋 생성 $ kubectl apply -f sample-ds.yaml 데몬셋 업데이트 전략 OnDelete 데몬셋 매니페스트를 수정해도 기존 파드는 업데이트 X 일반적으로 모니터링, 로그 전송에 사용되므로 업데이트는 다음에 다시 생성하거나 수동으로 임의의 시점에 하도록 운영상 정지하면 안되는 파드, 업데이트가 급히 필요하지 않은 경우 사용 (이전 버전이 계속 사용되는 점 주의) 1 2 ## 파드 업데이트 시 수동 정지 후 자동화된 복구 기능으로 새 파드 생성 $ kubectl delete pod sample-ds-ondelete-xxxxx RollingUpdate 데몬셋에서는 하나의 노드에 동일 파드를 여러개 생성할 수 없음 maxSurge 설정 불가 maxUnavailable만 지정하여 RollingUpdate maxUnavailable 0 불가 스테이트풀셋 DB와 같은 stateful한 워크로드에 사용하기 위한 리소스 레플리카셋과의 주된 차이점 생성되는 파드의 접미사는 숫자 인덱스 (sample-statefulset-0, sample-statefulset-1, \u0026hellip;) 파드명 불변 데이터를 영구적으로 저장하는 구조 (영구 볼륨을 사용하는 경우 파드 재기동시 같은 디스크 사용) 동시에 여러 파드가 생성되지 않고 하나씩 생성하며, Ready 상태가 되면 다음 파드를 생성 podManagemetPolicy를 Parallel로 설정하여 병렬로 동시에 기동 가능 스테이트풀셋 명령어 1 2 3 4 5 ## 스테이트풀셋 생성 $ kubectl apply -f sample-statefulset.yaml ## 스테이트풀셋 확인 $ kubectl get statefulsets 1 2 3 4 5 ## 영구 볼륨 클레임 확인 $ kubectl get persistentvolumeclaims ## 영구 볼륨 확인 $ kubectl get persistentvolumes 스테이트풀셋 스케일링 1 2 3 4 5 6 ## 레플리카 수를 3에서 4로 변경한 매니페스트를 apply $ sed -i -e \u0026#39;s|replicas: 3|replicas: 4|\u0026#39; sample-statefulset.yaml $ kubectl apply -f sample-statefulset.yaml ## kubectl scale을 사용한 스케일링 $ kubectl scale statefulset sample-statefulset --replicas=5 기본적으로 파드를 동시에 하나씩만 생성하고 삭제하므로 시간이 더 걸림 스케일 아웃일 때는 인덱스가 가장 작은 것 부터 파드를 하나씩 생성하고, 이전에 생성된 파드가 Ready 상태가 되고 나서 다음 파드 생성 스케일 인일 때는 인덱스가 가장 큰(가장 최근) 파드 부터 삭제 항상 0번째 파드가 먼저 생성되고 나중에 삭제되므로 마스터 노드로 사용하는 이중화 구조 애플리케이션에 적합 스테이트풀셋 업데이트 전략 Ondelete 데몬셋과 동일하게 매니페스트 변경해도 기존 파드는 그대로임 영속성 영역을 가진 DB나 클러스터등에서 많이 사용하므로 임의의 시점이나 다음에 재기동할 때 업데이트 RollingUpdate maxSurge, maxUnavailable 둘 다 사용 불가능 파드마다 Ready 상태인지 확인하고 업데이트 Parallel로 설정되어 있는 경우에도 하나씩 업데이트가 이루어짐 partition 설정시 전체 파드 중 어떤 파드까지 업데이트할지 지정 가능 전체에 영향을 주지 않고 부분적으로 업데이트를 확인할 수 있어 보다 안전 수동으로 재기동해도 Ondelete와 달리 partition보다 작은 인덱스를 가진 파드는 업데이트 X 예) partition이 3이면 0,1,2 인덱스 파드는 업데이트 X partition 값 수정하면 해당 값에 맞는 인덱스를 가진 파드들 업데이트됨 영구 볼륨 데이터 저장 확인 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ## 컨테이너 내부에 영구 볼륨 마운트 확인 $ kubectl exec -it sample-statefulset-0 -- df -h | grep /dev/sd ## 영구 볼륨에 sample.html 이 없는지 확인 $ kubectl exec -it sample-statefulset-0 -- ls /usr/share/nginx/html/sample.html ls: cannot access \u0026#39;/usr/share/nginx/html/sample.html\u0026#39;: No such file or directory ## 영구 볼륨에 sample.html 생성 kubectl exec -it sample-statefulset-0 -- touch /usr/share/nginx/html/sample.html ## 영구 볼륨에 sample.html 이 있는지 확인 $ kubectl exec -it sample-statefulset-0 -- ls /usr/share/nginx/html/sample.html /usr/share/nginx/html/sample.html ## 예상치 못한 파드 정지 1(파드 삭제) $ kubectl delete pod sample-statefulset-0 ## 예상치 못한 파드 정지 2(nginx 프로세스 정지) $ kubectl exec -it sample-statefulset-0 -- /bin/bash -c \u0026#39;kill 1\u0026#39; ## 파드 정지, 복구 후에도 파일 유실 없음 $ kubectl exec -it sample-statefulset-0 -- ls /usr/share/nginx/html/sample.html /usr/share/nginx/html/sample.html 스테이트풀셋을 삭제해도 영구 볼륨은 해제되지 않음 영구 볼륨 해제하지 않고 스테이트풀셋 재기동시 그대로 파드가 기동 1 2 ## 스테이트풀셋이 확보한 영구 볼륨 해제 $ kubectl delete persistentvolumeclaims www-sample-statefulset-{0..4} 잡 컨테이너를 사용하여 한 번만 실행되는 리소스 N개의 병렬로 실행하면서 지정한 횟수의 컨테이너 실행(정상 종료)를 보장하는 리소스 파드의 정지가 정상 종료되는 작업에 적합 (레플리카셋의 경우 파드의 정지는 예상치 못한 에러임) 잡에서는 정상 종료한 파드 수(COMPLETION)을 표기함 잡 명령어 1 2 3 4 5 6 7 8 ## 잡 생성 $ kubectl apply -f sample-job.yaml ## 잡 목록 표시 $ kubectl get jobs ## 잡이 생성한 파드 확인 $ kubectl get pods --watch 1 2 ## 잡 상태 모니터링 $ kubectl get job sample-job-ttl --watch --output-watch-events 1 2 3 4 ## 매니페스트를 사용하지 않고 명령어로 잡 생성 $ kubectl create job sample-job-by-cli \\ --image=amsy810/tools:v2.0 \\ -- sleep 30 1 2 ## 크론잡 기반 잡 생성 $ kubectl create job sample-job-from-cronjob --from cronjob/sample-cronjob restartPolicy Never 파드에 장애가 발생하면 신규 파드가 생성 OnFailure 파드 장애 발생시 다시 동일한 파드를 사용하여 잡을 다시 시작 RESTART 카운터가 증가 파드 IP 주소는 변경 X, 영구 볼륨이나 hostPath 마운트하지 않은 경우 데이터 유실 태스크와 작업 큐 병렬 실행 completion: 몇 회 완료시 잡을 성공으로 표기할 것인지(기본값 1), 변경 불가 parallelism: 병렬성 지정(기본값 1), 변경 가능 backoffLimit: 실패 허용 횟수, 변경 가능 워크로드 completions parallelism backoffLimit 목적 1회만 실행하는 태스크 1 1 0 성공 유무에 관계 없이 반드시 1회 실행 N개 병렬로 실행시키는 태스크 M N P 병렬 태스크 수행 한 개씩 실행하는 작업 큐 미지정 1 P 한 번 정상 종료할 때까지 한 개씩 실행 N개 병렬로 실행하는 작업 큐 미지정 N P 큰 처리 전체가 정상 종료할 때까지 몇 개의 병렬 수로 계속 실행하고픈 경우 1 2 ## 병렬성을 2에서 3으로 변경한 매니페스트 apply $ sed -e \u0026#39;s|parallelism: 1|parallelism: 2|\u0026#39; sample-job.yaml | kubectl apply -f - 크론잡 스케줄링된 시간에 잡 생성 크론잡 -\u0026gt; 잡 -\u0026gt; 파드 (3계층 관리 구조) 크론잡 일시정지 점검이나 어떤 이유로 잡 생성 원하지 않을 경우 suspend(일시정지) 매니페스트 수정 후 kubectl apply 실행 kubectl patch 1 $ kubectl patch cronjob sample-cronjob -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;suspend\u0026#34;:true}}\u0026#39; 동시 실행 제어 잡이 의도한 시간 간격에서 정상 종료시 동시 실행되지 않고 알아서 새로운 잡 실행 기존 잡이 실행되고 있을 때를 제어하고 싶은 경우 정책 개요 Allow(기본값) 동시 실행에 대한 제한 X Forbid 이전 잡이 종료되지 않은 경우 다음 잡 실행 X (동시 실행 X) Replace 이전 잡 취소 후 잡 시작(이전 잡의 레플리카 수를 0으로 변경) 실행 시작 기한 제어 쿠버네티스 마스터가 일시적으로 정지되는 경우와 같이 시작 시간 지연시, 이 지연 시간을 허용하는 시간 기본값은 아무리 늦어져도 잡을 생성 예) 매시 00분 시작 잡을 \u0026lsquo;매시 00~05분에만 실행 가능\u0026rsquo; 설정 시 300초로 설정 크론잡 이력 설정 항목 개요 spec.successfulJobsHistoryLimit 성공한 잡을 저장하는 개수 spec.failedJobsHistoryLimit 실패한 잡을 저장하는 개수 실제 운영 환경에서는 컨테이너 로그를 외부 로그 시스템을 통해 운영하는 것이 좋음 둘 다 0으로 설정시 잡은 종료시 즉시 삭제 매니페스트 사용하지 않고 크론잡 생성 1 2 3 4 $ kubectl create cronjob sample-cronjob-by-cli \\ --image amsy810/random-exit:v2.0 \\ --schedule \u0026#34;*/1 * * * *\u0026#34; \\ --restart Never 정리 파드나 레플리카셋 생성 시 처음부터 디플로이먼트로 생성하는 것이 좋음 리소스 종류 사용 방법 파드 디버깅이나 확인 용도 레플리카셋 파드 스케일링, 관리 디플로이먼트 스케일링할 워크로드에 사용 데몬셋 각 노드에 파드 하나씩 배포할 때 스테이트풀셋 영속성 데이터 등의 상태를 가진 워크로드에 사용 잡 작업 큐나 태스크 등 컨테이너 종료가 필요한 워크로드에 사용 크론잡 정기적으로 잡을 생성하고 싶은 경우 ","date":"2022-06-10T16:51:33+09:00","image":"https://Haebuk.github.io/p/%EC%9B%8C%ED%81%AC%EB%A1%9C%EB%93%9C-api/Kubernetes-Logo.wine_hu7d34b56543d7b812e012200a129db38d_42706_120x120_fill_box_smart1_3.png","permalink":"https://Haebuk.github.io/p/%EC%9B%8C%ED%81%AC%EB%A1%9C%EB%93%9C-api/","title":"워크로드 API"},{"content":"Docker 내용 정리 도커 컨테이너 설계 도커 컨테이너 생성 시 주의해야 할 점 네 가지\n1 컨테이너당 1 프로세스 변경 불가능한 인프라(Immutable Infrastructure) 이미지로 생성 경량의 도커 이미지로 생성 실행 계정은 root 이외의 사용자로 설정 1. 1 컨테이너당 1 프로세스 기존 VM처럼 하나의 이미지 안에 여러 프로세스는 비추 여러 프로세스 기동 시 주변 에코 시스템과 맞지 않거나 관리가 힘들어짐 2. 변경 불가능한 인프라를 구현하는 이미지로 생성 변경 불가능한 인프라: \u0026ldquo;환경 변경 시 오래된 환경은 없애고 새로운 환경 생성\u0026rdquo; 또는 \u0026ldquo;한번 만든 환경은 절대 불변하게\u0026rdquo; 전자의 경우 쿠버네티스는 자동으로 만들어주지만 후자는 컨테이너 이미지 관리자가 고려해야 함 도커 컨테이너는 버전 관리 가능하므로, 컨테이너 이미지 내에 애플리케이션 실행 바이너리 또는 관련 리소스를 가능한 포함시켜야 함 3. 도커 이미지 경량화 컨테이너 실행 시 최초 1회는 이미지를 외부에서 pull해야 함 dnf, yum, apt로 패키지 설치 후 저장소 패키지 목록 등의 캐시파일 삭제 멀티 스테이지 빌드 활용하여 이미지에 필요한 파일만 추가 기본 이미지가 경량인 배포판 이미지 사용 (ex. alpine linux, distorless 등) 도커 파일 최적화에 따라 레이어 줄이기 도커 이미지 생성시 squash 사용 4. 실행 계정 권한 최소화 root 사용자는 최대한 사용하지 않도록 한다. ENTRYPOINT와 CMD 컨테이너가 기동할 때 실행하는 명령어를 지정할 때 사용 아주 간단히 설명하면 $ENTRYOINT $CMD가 실행된다고 볼 수 있음 ENTRYPOINT에 바꿀 필요가 없는 부분을 정의하고 CMD에 기본값 인수 등을 정의하는 것이 일반적 예) ENTRYPOINT에 /bin/sleep 지정, CMD에 sleep 시간 지정 ","date":"2022-06-03T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/docker-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC/","title":"Docker 내용 정리"},{"content":"들어가며 현업에서 간단하게 로컬에서 데이터를 뽑아보려해도 수 GB는 훌쩍 넘어가는 경우가 다반사기 때문에, Pandas로는 한계가 있음을 느꼈습니다.\nDask를 사용하여 기초적인 병렬 계산, 데이터프레임 다루기, 간단한 신경망을 통해 학습하는 과정을 살펴보겠습니다.\nhttps://www.youtube.com/watch?v=Alwgx_1qsj4를 참고했습니다.\n예전에 촬영되어서 그대로 코드를 작성하면 작동하지 않는 코드가 여럿 있습니다. 2022년 1월 10일 기준으로 작동하도록 수정했습니다.\nPre-required dask와 함께 진행에는 영향이 없지만 아래에서 제공하는 시각화를 위해서는 graphviz 라이브러리를 설치해야합니다.\n또한 Machine Learning 파트에서 Tensorflow를 사용합니다. M1 맥북에서 실행했기 때문에 출력문에 약간의 차이가 발생할 수 있습니다.\nBasic 첫번째로 dask가 제공하는 병렬 계산에 대해 살펴보도록 하겠습니다.\n아래와 같이 함수가 작동할 때 마다 1초씩 대기하는 코드가 있습니다.\n1 2 3 4 5 6 7 8 9 from time import sleep def inc(x): sleep(1) return x + 1 def add(x, y): sleep(1) return x + y 한번 실행시켜 보겠습니다. x와 y에 1과 2를 할당하고 x와 y를 더합니다.\n1 2 3 4 5 %%time x = inc(1) y = inc(2) z = add(x, y) CPU times: user 451 µs, sys: 697 µs, total: 1.15 ms\rWall time: 3.01 s\r1초, 1초, 1초 3번을 대기 했기때문에 총 실행시간이 약 3초가 나왔음을 알 수 있습니다.\n이를 Dask를 이용하여 기다리지 않고 계산하게 만들 수 있습니다.\n1 from dask import delayed 이를 위해서 Dask의 delayed 모듈을 임포트합니다. delayed 모듈은 병렬로 계산하고자 하는 것이 있을 때 매우 효과적입니다.\n1 2 3 4 5 6 7 8 9 @delayed def delayed_inc(x): sleep(1) return x + 1 @delayed def delayed_add(x, y): sleep(1) return x + y 위에서 정의했던 inc와 add의 함수와 동일합니다. 단지 함수 위에 @delayed 데코레이터를 붙여주기만 하면 끝입니다.\n한 번 시간을 측정해보겠습니다.\n1 2 3 4 %%time x = delayed(delayed_inc)(1) y = delayed(delayed_inc)(2) z = delayed(delayed_add)(x, y) delayed메서드 안에 위에서 정의한 함수를 넣고 바깥에 함수 값을 할당합니다.\nCPU times: user 116 µs, sys: 20 µs, total: 136 µs\rWall time: 130 µs\r놀랍게도 1초도 안걸려 모든 계산이 끝났습니다. (정확한 결론은 아래를 참조해주세요.)\ndask가 어떤 병렬 계산을 수행했는지 시각적으로 확인할 수 있습니다.\n1 z.visualize() 위에서부터 차례로 코드를 실행하는 것이 아닌 병렬로 계산한다는 것을 알 수 있습니다.\ndask의 메서드로 정의한 값을 알아보려면 평소와는 다른 방법을 써야하는데요, 아래와 같습니다.\nz값 (2+3=5)가 나오길 기대했지만, 엉뚱한 값이 나옵니다.\n1 z Delayed('delayed_add-0e54f9e1-941d-49e9-903f-34e96b0dba54')\r이는 실제 계산이 수행된 것이 아닌 어떤 메타데이터를 가르키고 있다고 볼 수 있습니다.\n우리가 원하는 계산을 수행하려면 compute()를 사용해야 합니다.\n1 2 %%time z.compute() CPU times: user 1.76 ms, sys: 1.56 ms, total: 3.32 ms\rWall time: 2.01 s\r5\rz의 값은 5가 나왔고, 실행 시간은 2초가 나왔습니다. 각 inc(x) inc(y)가 병렬로 수행되는 데 1초, add(x+y)에서 1초가 소요되었기 때문입니다.\nFor loop 조금 더 오래걸리는 예제를 살펴보겠습니다.\n파이썬의 for문은 악명이 자자한데요, 데이터 수를 무자비하게 늘리기보다는 앞에서 사용했던 함수를 사용해 시간을 늘려보겠습니다.\n1부터 8까지 담겨져 있는 파이썬 리스트를 선언합니다.\n1 data = [1, 2, 3, 4, 5, 6, 7, 8] 리스트에서 값을 뽑아 inc함수에 삽입하고 결과를 빈 리스트에 담아 최종 합을 산출하는 코드입니다.\n1 2 3 4 5 6 7 8 %%time results = [] for x in data: y = inc(x) results.append(y) total = sum(results) CPU times: user 1.19 ms, sys: 1.2 ms, total: 2.38 ms\rWall time: 8.03 s\rinc가 총 8번 호출됐기 때문에 실행시간이 8초가 나왔습니다.\n이를 dask의 delayed 메서드로 병렬화시켜보겠습니다. 과연 모든 inc가 병렬로 계산되어 1초 남짓한 시간이 걸릴까요?\n1 2 3 4 5 6 7 8 9 10 11 %%time results = [] for x in data: y = delayed(delayed_inc)(x) results.append(y) total = delayed(sum)(results) total.compute() CPU times: user 3.28 ms, sys: 2.15 ms, total: 5.43 ms\rWall time: 1.01 s\r44\r1부터 8까지 모두 더한 44가 결과값으로 나왔고, 실행시간은 예상한대로 1초가 나왔습니다. 모든 inc 함수가 병렬로 수행됐음이 분명합니다.\n검증을 위해 시각화해보겠습니다.\n1 total.visualize() 처음에 이 그래프를 보고 감탄했던 기억이(\u0026hellip;) 아름답게 병렬 계산을 하는 것을 알 수 있습니다.\nDataFrame dask하면 생각나는 것이 바로 대용량 dataframe입니다. dask를 이용해 빠르게 로드하고 집계하는 방법에 대해 살펴보겠습니다.\n먼저 데이터는 약 200MB의 데이터로 뉴욕 공항의 항공기 이착륙 관련 데이터입니다.\n아래와 같이 데이터를 다운로드 하고 로드합니다.\n1 2 3 4 5 import urllib print(\u0026#34;- Downloading NYC Flights dataset... \u0026#34;, end=\u0026#39;\u0026#39;, flush=True) url = \u0026#34;https://storage.googleapis.com/dask-tutorial-data/nycflights.tar.gz\u0026#34; filename, headers = urllib.request.urlretrieve(url, \u0026#39;nycflights.tar.gz\u0026#39;) - Downloading NYC Flights dataset... 1 2 3 4 import tarfile with tarfile.open(filename, mode=\u0026#39;r:gz\u0026#39;) as flights: flights.extractall(\u0026#39;data/\u0026#39;) 여기까지 했다면 data폴더 안에 10개의 csv파일이 생성된 것을 확인할 수 있습니다.\npandas를 사용할 경우 이를 반복문을 통해 pd.concat으로 순차적으로 데이터프레임을 합치는 방법으로 접근하는데요, dask는 아래와 같이 분할된 파일을 한번에 로드할 수 있는 기능을 제공합니다.\n1 2 3 4 5 6 import os import dask.dataframe as dd df = dd.read_csv(os.path.join(\u0026#39;data\u0026#39;, \u0026#39;nycflights\u0026#39;, \u0026#39;*.csv\u0026#39;), parse_dates={\u0026#39;Date\u0026#39;: [0, 1, 2]}) df .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rpandas와는 다르게 모든 값이 감춰져있습니다. 이를 살펴보는 방법은 조금 후에 살펴보도록 하고, dask 데이터프레임 로드시 주의해야할 점에 대해 먼저 설명하겠습니다.\n1 df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rhead()메서드는 pandas와 동일한 기능을 제공합니다. tail()도 한 번 살펴볼까요?\n1 df.tail() ---------------------------------------------------------------------------\rValueError Traceback (most recent call last)\r/var/folders/xm/8mvqw44j1md_q70lrkm9_wh00000gn/T/ipykernel_47466/281403043.py in \u0026lt;module\u0026gt;\r----\u0026gt; 1 df.tail()\r/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/dask/dataframe/core.py in tail(self, n, compute)\r1143 1144 if compute:\r-\u0026gt; 1145 result = result.compute()\r1146 return result\r1147 ... 중략 ...\rValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\r+----------------+---------+----------+\r| Column | Found | Expected |\r+----------------+---------+----------+\r| CRSElapsedTime | float64 | int64 |\r| TailNum | object | float64 |\r+----------------+---------+----------+\rThe following columns also raised exceptions on conversion:\r- TailNum\rValueError(\u0026quot;could not convert string to float: 'N54711'\u0026quot;)\rUsually this is due to dask's dtype inference failing, and\r*may* be fixed by specifying dtypes manually by adding:\rdtype={'CRSElapsedTime': 'float64',\r'TailNum': 'object'}\rto the call to `read_csv`/`read_table`.\rtail()은 head()와 다르게 오류가 발생합니다. 이는 dask가 dataframe을 생성할 때 데이터타입을 데이터의 초반 행을 통해 추론하기 때문입니다.\n오류문을 살펴보면 CRSElapsedTime은 int64를 기대했는데 float64였고, TailNum은 float64를 기대했는데 object가 나타났다고 합니다.\n이를 해결하기 위해서는 dask dataframe을 정의할 때 데이터타입을 명시해줘야 합니다.\n1 2 3 4 5 6 7 df = dd.read_csv( os.path.join(\u0026#39;data\u0026#39;, \u0026#39;nycflights\u0026#39;, \u0026#39;*.csv\u0026#39;), parse_dates={\u0026#39;Date\u0026#39;: [0, 1, 2]}, dtype={\u0026#39;TailNum\u0026#39;: str, \u0026#39;CRSElapsedTime\u0026#39;: float, \u0026#39;Cancelled\u0026#39;: bool} ) CRSElapsedTime과 TailNum의 데이터 타입을 명시하고, 이따가 사용할 Cancelled열도 미리 데이터 타입을 선언해주겠습니다.\n다시 한 번 마지막 값을 살펴보겠습니다.\n1 df.tail() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r정상적으로 값이 출력됨을 알 수 있습니다.\n이번에는 간단한 집계함수를 사용해보겠습니다.\n1 %time df.DepDelay.max().compute() CPU times: user 3.18 s, sys: 526 ms, total: 3.71 s\rWall time: 1.61 s\r1435.0\r매우 빠르게 최대값을 산출해냄을 알 수 있습니다. dask는 이를 어떻게 계산했을까요?\n시각적으로 살펴보겠습니다.\n1 df.DepDelay.max().visualize(rankdir=\u0026#39;LR\u0026#39;, size=\u0026#39;12, 12!\u0026#39;) 각각의 파티션(총 10개)에서 최대값 후보를 선정한다음에 최종 최대값을 선출해냄을 알 수 있습니다.\n단순하게 생각하면 pandas 집계보다 10배 빠르다고 볼 수도 있겠습니다.\nML with Dask 마지막으로 간단한 신경망을 통해 학습하는 방법을 살펴보고 마치겠습니다.\n먼저 학습에 사용할 데이터를 정의하고 정보를 확인합니다.\n1 df_train = df[[\u0026#39;CRSDepTime\u0026#39;, \u0026#39;CRSArrTime\u0026#39;, \u0026#39;Cancelled\u0026#39;]] 1 df_train.iloc[:, :-1].compute().values array([[1540, 1701],\r[1540, 1701],\r[1540, 1701],\r...,\r[1645, 1901],\r[1645, 1901],\r[1645, 1901]])\r1 df_train.iloc[:, -1].compute().values array([False, False, False, ..., False, False, False])\r1 df_train.shape (Delayed('int-94ab9ac8-9432-4a95-b40f-abdaca09c41e'), 3)\r0번째 값은 dask delayed객체로 나오고, 1번째 값은 총 열 개수인 3이 나오는 것이 특징입니다.\n1 df_train.isnull().sum().compute() CRSDepTime 0\rCRSArrTime 0\rCancelled 0\rdtype: int64\r결측치는 없습니다. 아주 간단한 신경망을 정의하고 학습시켜보겠습니다.\n1 2 3 4 5 6 7 8 9 import tensorflow as tf from keras.models import Sequential from keras.layers import Dense model = Sequential() model.add(Dense(20, input_dim=df_train.shape[1]-1, activation=\u0026#39;relu\u0026#39;)) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;sgd\u0026#39;) from_tensor_slices를 사용해 데이터프레임을 변환합니다.\n1 2 3 dataset = tf.data.Dataset.from_tensor_slices( (df_train.iloc[:, :-1].compute().values, df_train.iloc[:, -1].compute().values) ).batch(512) 1 model.fit(dataset, epochs=5) Epoch 1/5\r10203/10203 [==============================] - 38s 4ms/step - loss: 239.6750\rEpoch 2/5\r10203/10203 [==============================] - 38s 4ms/step - loss: 0.1011\rEpoch 3/5\r10203/10203 [==============================] - 38s 4ms/step - loss: 0.1006\rEpoch 4/5\r10203/10203 [==============================] - 39s 4ms/step - loss: 0.1006\rEpoch 5/5\r10203/10203 [==============================] - 39s 4ms/step - loss: 0.1006\r\u0026lt;keras.callbacks.History at 0x298c5f040\u0026gt;\r여기서는 학습을 할 수 있다에 초점을 맞췄기 때문에, 성능 검증은 다루지 않습니다.\n마치며 대용량 데이터에 적합한 라이브러리인 dask에 대해 기초를 다뤄봤습니다. 심화된 기능은 공식 홈페이지에 상세히 나와있습니다.\ndask의 기능을 좀 더 숙지한다면 매우 많은 부분에서 pandas를 대체할 수 있을것이라 기대합니다.\n더 소개할만한 기능을 수집해서 다음 포스팅에 공유하도록 하겠습니다. 감사합니다.\n","date":"2022-01-10T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/dask-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC/","title":"Dask 튜토리얼"},{"content":"엘라스틱 서치를 파이썬으로 쿼리하는 방법에 대해 알아보겠습니다.\n현업에서 엘라스틱 서치를 통해 정보를 받고 Kibana를 통해 시각화를 많이 하고 있는데요, 이러한 로그 정보들을 파이썬을 통해 분석하고 싶은 경우가 있습니다. 예를 든다면 dau(daily activate user)의 정보를 수집하고 있는데 몇시에 가장 많이 접속하는지, 이런 정보를 가공해 다른 곳에 사용한다던지 말이죠.\n이러한 정보를 키바나를 통해 확인할 수 있지만, 별도의 레포트를 만들 경우 seaborn이나 matplotlib을 통해 시각화를 진행할때가 많습니다. 그렇다면 어떻게 엘라스틱 서치로 저장된 정보를 파이썬으로 쿼리할 수 있을 지 알아보겠습니다.\n여기서는 scroll 메서드를 사용해 순차적으로 저장된 모든 정보를 불러오고 json 파일로 저장하는 방법을 살펴봅니다.\n예제 아래 예제에서는 dau 라는 이름을 가진 인덱스에서 product_id=2021 인 정보를 모두 쿼리합니다.\n1 2 3 4 5 6 7 import json from datetime import datetime from elasticsearch import Elasticsearch host = \u0026#34;받아올 엘라스틱 서치 URI\u0026#34; es = Elasticsearch(host) indices = [\u0026#39;dau\u0026#39;] # 인덱스 명 size = 10000 # 한 번에 받아올 데이터 갯수 여기서 host 는 본인이 사용하고 있는 엘라스틱 서치 서비스의 URI입니다. size=10000 은 한 번에 1만개씩 받아오겠다는 의미입니다.\n1 2 3 4 5 6 7 8 doc = { \u0026#39;size\u0026#39;: size, \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;product_id\u0026#39;: 2021, } } } doc에 쿼리 정보를 담아줍니다.\n1 2 3 4 # 이전 스크롤 1초 저장 response = es.search(index=indices, body=doc, scroll=\u0026#39;1s\u0026#39;) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] result = [] response 에 엘라스틱 서치 search 메서드를 정의합니다. old_scroll_id 에 초기 스크롤 id를 정의하고 결과를 받을 빈 리스트를 정의합니다.(result)\n1 2 for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) 가장 처음 쿼리를 저장합니다.\n1 2 3 4 5 6 while len(response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]): response = es.scroll(scroll_id=old_scroll_id, scroll=\u0026#39;1s\u0026#39;) for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] # scroll id 초기화 print(f\u0026#39;Result Length: {len(result)}\u0026#39;) 이후 반복문을 통해 더 이상 받아올 정보가 없을 때 까지 탐색하여 정보를 저장합니다.\n1 2 3 # save file with open(\u0026#39;es_dau.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(result, f) 마지막으로 result 를 json 형식의 파일로 저장하면 이후 자유롭게 어디서나 쿼리한 정보를 받아올 수 있습니다.\n전체코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import json from datetime import datetime from elasticsearch import Elasticsearch host = \u0026#34;사용할 엘라스틱 서치 URL\u0026#34; es = Elasticsearch(host) indices = [\u0026#39;dau\u0026#39;] size = 10000 doc = { \u0026#39;size\u0026#39;: size, \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;product_id\u0026#39;: 2021, } } } # 이전 스크롤 1초 저장 response = es.search(index=indices, body=doc, scroll=\u0026#39;1s\u0026#39;) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] result = [] for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) while len(response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]): response = es.scroll(scroll_id=old_scroll_id, scroll=\u0026#39;1s\u0026#39;) for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] # scroll id 초기화 print(f\u0026#39;Result Length: {len(result)}\u0026#39;) # save file with open(\u0026#39;es_dau.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(result, f) ","date":"2021-12-19T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%EC%97%98%EB%9D%BC%EC%8A%A4%ED%8B%B1-%EC%84%9C%EC%B9%98-%EC%BF%BC%EB%A6%AC%ED%95%98%EA%B8%B0/","title":"파이썬으로 엘라스틱 서치 쿼리하기"},{"content":"지난번 포스팅에서 data analytics learning plan을 수강한다고 언급했었는데요,(참조: [AWS] Data Analytics Learning Plan을 시작하며) 아무래도 영어로 된 강의다보니 첫 수강에 부담이 있었습니다.\n한국어로 된 유사한 강의가 있나 찾아보던 중 발견하게 되어 먼저 수강하기로 결정했습니다. (링크: Data Analytics Fundamentals (Korean), 수강신청을 하지 않았을 경우 접속이 되지 않을 수 있습니다. 먼저 aws skill builder에서 등록을 진행해주세요.)\n강의 구성 강의는 약 3시간 30분으로 이루어져 있습니다. 강의 구성은 5V에 대해 소개하는데요, 볼륨(Volume), 속도(Velocity), 다양성(Variety), 정확성(Veracity), 가치(Value)의 5V입니다.\n데이터 분석 과정에서 직면한 5V의 문제를 어떤식으로 접근해야하는지, AWS의 어떤 서비스를 이용하면 되는지에 대해 소개합니다.\n후기 솔직히 별로..라고 생각했습니다.\n기초강의다보니 실무적으로 사용하는 방법보다는, 이런 문제는 우리의 어떤 서비스를 통해 해결할 수 있어~ 에서 마치는 느낌입니다.\nAWS에 어느정도 관심있으신 분들은 다 아시는 내용일거라고 생각합니다.\n아무튼 수료 ","date":"2021-01-09T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/aws-data-analysis-fundamentals-%ED%9B%84%EA%B8%B0/","title":"AWS Data Analysis Fundamentals 후기"}]