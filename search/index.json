[{"content":"Docker 내용 정리 도커 컨테이너 설계 도커 컨테이너 생성 시 주의해야 할 점 네 가지\n1 컨테이너당 1 프로세스 변경 불가능한 인프라(Immutable Infrastructure) 이미지로 생성 경량의 도커 이미지로 생성 실행 계정은 root 이외의 사용자로 설정 1. 1 컨테이너당 1 프로세스 기존 VM처럼 하나의 이미지 안에 여러 프로세스는 비추 여러 프로세스 기동 시 주변 에코 시스템과 맞지 않거나 관리가 힘들어짐 2. 변경 불가능한 인프라를 구현하는 이미지로 생성 변경 불가능한 인프라: \u0026ldquo;환경 변경 시 오래된 환경은 없애고 새로운 환경 생성\u0026rdquo; 또는 \u0026ldquo;한번 만든 환경은 절대 불변하게\u0026rdquo; 전자의 경우 쿠버네티스는 자동으로 만들어주지만 후자는 컨테이너 이미지 관리자가 고려해야 함 도커 컨테이너는 버전 관리 가능하므로, 컨테이너 이미지 내에 애플리케이션 실행 바이너리 또는 관련 리소스를 가능한 포함시켜야 함 3. 도커 이미지 경량화 컨테이너 실행 시 최초 1회는 이미지를 외부에서 pull해야 함 dnf, yum, apt로 패키지 설치 후 저장소 패키지 목록 등의 캐시파일 삭제 멀티 스테이지 빌드 활용하여 이미지에 필요한 파일만 추가 기본 이미지가 경량인 배포판 이미지 사용 (ex. alpine linux, distorless 등) 도커 파일 최적화에 따라 레이어 줄이기 도커 이미지 생성시 squash 사용 4. 실행 계정 권한 최소화 root 사용자는 최대한 사용하지 않도록 한다. ENTRYPOINT와 CMD 컨테이너가 기동할 때 실행하는 명령어를 지정할 때 사용 아주 간단히 설명하면 $ENTRYOINT $CMD가 실행된다고 볼 수 있음 ENTRYPOINT에 바꿀 필요가 없는 부분을 정의하고 CMD에 기본값 인수 등을 정의하는 것이 일반적 예) ENTRYPOINT에 /bin/sleep 지정, CMD에 sleep 시간 지정 ","date":"2022-06-03T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/docker-%EB%82%B4%EC%9A%A9-%EC%A0%95%EB%A6%AC/","title":"Docker 내용 정리"},{"content":"들어가며 현업에서 간단하게 로컬에서 데이터를 뽑아보려해도 수 GB는 훌쩍 넘어가는 경우가 다반사기 때문에, Pandas로는 한계가 있음을 느꼈습니다.\nDask를 사용하여 기초적인 병렬 계산, 데이터프레임 다루기, 간단한 신경망을 통해 학습하는 과정을 살펴보겠습니다.\nhttps://www.youtube.com/watch?v=Alwgx_1qsj4를 참고했습니다.\n예전에 촬영되어서 그대로 코드를 작성하면 작동하지 않는 코드가 여럿 있습니다. 2022년 1월 10일 기준으로 작동하도록 수정했습니다.\nPre-required dask와 함께 진행에는 영향이 없지만 아래에서 제공하는 시각화를 위해서는 graphviz 라이브러리를 설치해야합니다.\n또한 Machine Learning 파트에서 Tensorflow를 사용합니다. M1 맥북에서 실행했기 때문에 출력문에 약간의 차이가 발생할 수 있습니다.\nBasic 첫번째로 dask가 제공하는 병렬 계산에 대해 살펴보도록 하겠습니다.\n아래와 같이 함수가 작동할 때 마다 1초씩 대기하는 코드가 있습니다.\n1 2 3 4 5 6 7 8 9 from time import sleep def inc(x): sleep(1) return x + 1 def add(x, y): sleep(1) return x + y 한번 실행시켜 보겠습니다. x와 y에 1과 2를 할당하고 x와 y를 더합니다.\n1 2 3 4 5 %%time x = inc(1) y = inc(2) z = add(x, y) CPU times: user 451 µs, sys: 697 µs, total: 1.15 ms\rWall time: 3.01 s\r1초, 1초, 1초 3번을 대기 했기때문에 총 실행시간이 약 3초가 나왔음을 알 수 있습니다.\n이를 Dask를 이용하여 기다리지 않고 계산하게 만들 수 있습니다.\n1 from dask import delayed 이를 위해서 Dask의 delayed 모듈을 임포트합니다. delayed 모듈은 병렬로 계산하고자 하는 것이 있을 때 매우 효과적입니다.\n1 2 3 4 5 6 7 8 9 @delayed def delayed_inc(x): sleep(1) return x + 1 @delayed def delayed_add(x, y): sleep(1) return x + y 위에서 정의했던 inc와 add의 함수와 동일합니다. 단지 함수 위에 @delayed 데코레이터를 붙여주기만 하면 끝입니다.\n한 번 시간을 측정해보겠습니다.\n1 2 3 4 %%time x = delayed(delayed_inc)(1) y = delayed(delayed_inc)(2) z = delayed(delayed_add)(x, y) delayed메서드 안에 위에서 정의한 함수를 넣고 바깥에 함수 값을 할당합니다.\nCPU times: user 116 µs, sys: 20 µs, total: 136 µs\rWall time: 130 µs\r놀랍게도 1초도 안걸려 모든 계산이 끝났습니다. (정확한 결론은 아래를 참조해주세요.)\ndask가 어떤 병렬 계산을 수행했는지 시각적으로 확인할 수 있습니다.\n1 z.visualize() 위에서부터 차례로 코드를 실행하는 것이 아닌 병렬로 계산한다는 것을 알 수 있습니다.\ndask의 메서드로 정의한 값을 알아보려면 평소와는 다른 방법을 써야하는데요, 아래와 같습니다.\nz값 (2+3=5)가 나오길 기대했지만, 엉뚱한 값이 나옵니다.\n1 z Delayed('delayed_add-0e54f9e1-941d-49e9-903f-34e96b0dba54')\r이는 실제 계산이 수행된 것이 아닌 어떤 메타데이터를 가르키고 있다고 볼 수 있습니다.\n우리가 원하는 계산을 수행하려면 compute()를 사용해야 합니다.\n1 2 %%time z.compute() CPU times: user 1.76 ms, sys: 1.56 ms, total: 3.32 ms\rWall time: 2.01 s\r5\rz의 값은 5가 나왔고, 실행 시간은 2초가 나왔습니다. 각 inc(x) inc(y)가 병렬로 수행되는 데 1초, add(x+y)에서 1초가 소요되었기 때문입니다.\nFor loop 조금 더 오래걸리는 예제를 살펴보겠습니다.\n파이썬의 for문은 악명이 자자한데요, 데이터 수를 무자비하게 늘리기보다는 앞에서 사용했던 함수를 사용해 시간을 늘려보겠습니다.\n1부터 8까지 담겨져 있는 파이썬 리스트를 선언합니다.\n1 data = [1, 2, 3, 4, 5, 6, 7, 8] 리스트에서 값을 뽑아 inc함수에 삽입하고 결과를 빈 리스트에 담아 최종 합을 산출하는 코드입니다.\n1 2 3 4 5 6 7 8 %%time results = [] for x in data: y = inc(x) results.append(y) total = sum(results) CPU times: user 1.19 ms, sys: 1.2 ms, total: 2.38 ms\rWall time: 8.03 s\rinc가 총 8번 호출됐기 때문에 실행시간이 8초가 나왔습니다.\n이를 dask의 delayed 메서드로 병렬화시켜보겠습니다. 과연 모든 inc가 병렬로 계산되어 1초 남짓한 시간이 걸릴까요?\n1 2 3 4 5 6 7 8 9 10 11 %%time results = [] for x in data: y = delayed(delayed_inc)(x) results.append(y) total = delayed(sum)(results) total.compute() CPU times: user 3.28 ms, sys: 2.15 ms, total: 5.43 ms\rWall time: 1.01 s\r44\r1부터 8까지 모두 더한 44가 결과값으로 나왔고, 실행시간은 예상한대로 1초가 나왔습니다. 모든 inc 함수가 병렬로 수행됐음이 분명합니다.\n검증을 위해 시각화해보겠습니다.\n1 total.visualize() 처음에 이 그래프를 보고 감탄했던 기억이(\u0026hellip;) 아름답게 병렬 계산을 하는 것을 알 수 있습니다.\nDataFrame dask하면 생각나는 것이 바로 대용량 dataframe입니다. dask를 이용해 빠르게 로드하고 집계하는 방법에 대해 살펴보겠습니다.\n먼저 데이터는 약 200MB의 데이터로 뉴욕 공항의 항공기 이착륙 관련 데이터입니다.\n아래와 같이 데이터를 다운로드 하고 로드합니다.\n1 2 3 4 5 import urllib print(\u0026#34;- Downloading NYC Flights dataset... \u0026#34;, end=\u0026#39;\u0026#39;, flush=True) url = \u0026#34;https://storage.googleapis.com/dask-tutorial-data/nycflights.tar.gz\u0026#34; filename, headers = urllib.request.urlretrieve(url, \u0026#39;nycflights.tar.gz\u0026#39;) - Downloading NYC Flights dataset... 1 2 3 4 import tarfile with tarfile.open(filename, mode=\u0026#39;r:gz\u0026#39;) as flights: flights.extractall(\u0026#39;data/\u0026#39;) 여기까지 했다면 data폴더 안에 10개의 csv파일이 생성된 것을 확인할 수 있습니다.\npandas를 사용할 경우 이를 반복문을 통해 pd.concat으로 순차적으로 데이터프레임을 합치는 방법으로 접근하는데요, dask는 아래와 같이 분할된 파일을 한번에 로드할 수 있는 기능을 제공합니다.\n1 2 3 4 5 6 import os import dask.dataframe as dd df = dd.read_csv(os.path.join(\u0026#39;data\u0026#39;, \u0026#39;nycflights\u0026#39;, \u0026#39;*.csv\u0026#39;), parse_dates={\u0026#39;Date\u0026#39;: [0, 1, 2]}) df .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rpandas와는 다르게 모든 값이 감춰져있습니다. 이를 살펴보는 방법은 조금 후에 살펴보도록 하고, dask 데이터프레임 로드시 주의해야할 점에 대해 먼저 설명하겠습니다.\n1 df.head() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\rhead()메서드는 pandas와 동일한 기능을 제공합니다. tail()도 한 번 살펴볼까요?\n1 df.tail() ---------------------------------------------------------------------------\rValueError Traceback (most recent call last)\r/var/folders/xm/8mvqw44j1md_q70lrkm9_wh00000gn/T/ipykernel_47466/281403043.py in \u0026lt;module\u0026gt;\r----\u0026gt; 1 df.tail()\r/opt/homebrew/Caskroom/miniforge/base/envs/tensorflow/lib/python3.9/site-packages/dask/dataframe/core.py in tail(self, n, compute)\r1143 1144 if compute:\r-\u0026gt; 1145 result = result.compute()\r1146 return result\r1147 ... 중략 ...\rValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\r+----------------+---------+----------+\r| Column | Found | Expected |\r+----------------+---------+----------+\r| CRSElapsedTime | float64 | int64 |\r| TailNum | object | float64 |\r+----------------+---------+----------+\rThe following columns also raised exceptions on conversion:\r- TailNum\rValueError(\u0026quot;could not convert string to float: 'N54711'\u0026quot;)\rUsually this is due to dask's dtype inference failing, and\r*may* be fixed by specifying dtypes manually by adding:\rdtype={'CRSElapsedTime': 'float64',\r'TailNum': 'object'}\rto the call to `read_csv`/`read_table`.\rtail()은 head()와 다르게 오류가 발생합니다. 이는 dask가 dataframe을 생성할 때 데이터타입을 데이터의 초반 행을 통해 추론하기 때문입니다.\n오류문을 살펴보면 CRSElapsedTime은 int64를 기대했는데 float64였고, TailNum은 float64를 기대했는데 object가 나타났다고 합니다.\n이를 해결하기 위해서는 dask dataframe을 정의할 때 데이터타입을 명시해줘야 합니다.\n1 2 3 4 5 6 7 df = dd.read_csv( os.path.join(\u0026#39;data\u0026#39;, \u0026#39;nycflights\u0026#39;, \u0026#39;*.csv\u0026#39;), parse_dates={\u0026#39;Date\u0026#39;: [0, 1, 2]}, dtype={\u0026#39;TailNum\u0026#39;: str, \u0026#39;CRSElapsedTime\u0026#39;: float, \u0026#39;Cancelled\u0026#39;: bool} ) CRSElapsedTime과 TailNum의 데이터 타입을 명시하고, 이따가 사용할 Cancelled열도 미리 데이터 타입을 선언해주겠습니다.\n다시 한 번 마지막 값을 살펴보겠습니다.\n1 df.tail() .dataframe tbody tr th {\rvertical-align: top;\r}\r.dataframe thead th {\rtext-align: right;\r}\r정상적으로 값이 출력됨을 알 수 있습니다.\n이번에는 간단한 집계함수를 사용해보겠습니다.\n1 %time df.DepDelay.max().compute() CPU times: user 3.18 s, sys: 526 ms, total: 3.71 s\rWall time: 1.61 s\r1435.0\r매우 빠르게 최대값을 산출해냄을 알 수 있습니다. dask는 이를 어떻게 계산했을까요?\n시각적으로 살펴보겠습니다.\n1 df.DepDelay.max().visualize(rankdir=\u0026#39;LR\u0026#39;, size=\u0026#39;12, 12!\u0026#39;) 각각의 파티션(총 10개)에서 최대값 후보를 선정한다음에 최종 최대값을 선출해냄을 알 수 있습니다.\n단순하게 생각하면 pandas 집계보다 10배 빠르다고 볼 수도 있겠습니다.\nML with Dask 마지막으로 간단한 신경망을 통해 학습하는 방법을 살펴보고 마치겠습니다.\n먼저 학습에 사용할 데이터를 정의하고 정보를 확인합니다.\n1 df_train = df[[\u0026#39;CRSDepTime\u0026#39;, \u0026#39;CRSArrTime\u0026#39;, \u0026#39;Cancelled\u0026#39;]] 1 df_train.iloc[:, :-1].compute().values array([[1540, 1701],\r[1540, 1701],\r[1540, 1701],\r...,\r[1645, 1901],\r[1645, 1901],\r[1645, 1901]])\r1 df_train.iloc[:, -1].compute().values array([False, False, False, ..., False, False, False])\r1 df_train.shape (Delayed('int-94ab9ac8-9432-4a95-b40f-abdaca09c41e'), 3)\r0번째 값은 dask delayed객체로 나오고, 1번째 값은 총 열 개수인 3이 나오는 것이 특징입니다.\n1 df_train.isnull().sum().compute() CRSDepTime 0\rCRSArrTime 0\rCancelled 0\rdtype: int64\r결측치는 없습니다. 아주 간단한 신경망을 정의하고 학습시켜보겠습니다.\n1 2 3 4 5 6 7 8 9 import tensorflow as tf from keras.models import Sequential from keras.layers import Dense model = Sequential() model.add(Dense(20, input_dim=df_train.shape[1]-1, activation=\u0026#39;relu\u0026#39;)) model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;)) model.compile(loss=\u0026#39;binary_crossentropy\u0026#39;, optimizer=\u0026#39;sgd\u0026#39;) from_tensor_slices를 사용해 데이터프레임을 변환합니다.\n1 2 3 dataset = tf.data.Dataset.from_tensor_slices( (df_train.iloc[:, :-1].compute().values, df_train.iloc[:, -1].compute().values) ).batch(512) 1 model.fit(dataset, epochs=5) Epoch 1/5\r10203/10203 [==============================] - 38s 4ms/step - loss: 239.6750\rEpoch 2/5\r10203/10203 [==============================] - 38s 4ms/step - loss: 0.1011\rEpoch 3/5\r10203/10203 [==============================] - 38s 4ms/step - loss: 0.1006\rEpoch 4/5\r10203/10203 [==============================] - 39s 4ms/step - loss: 0.1006\rEpoch 5/5\r10203/10203 [==============================] - 39s 4ms/step - loss: 0.1006\r\u0026lt;keras.callbacks.History at 0x298c5f040\u0026gt;\r여기서는 학습을 할 수 있다에 초점을 맞췄기 때문에, 성능 검증은 다루지 않습니다.\n마치며 대용량 데이터에 적합한 라이브러리인 dask에 대해 기초를 다뤄봤습니다. 심화된 기능은 공식 홈페이지에 상세히 나와있습니다.\ndask의 기능을 좀 더 숙지한다면 매우 많은 부분에서 pandas를 대체할 수 있을것이라 기대합니다.\n더 소개할만한 기능을 수집해서 다음 포스팅에 공유하도록 하겠습니다. 감사합니다.\n","date":"2022-01-10T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/dask-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC/","title":"Dask 튜토리얼"},{"content":"엘라스틱 서치를 파이썬으로 쿼리하는 방법에 대해 알아보겠습니다.\n현업에서 엘라스틱 서치를 통해 정보를 받고 Kibana를 통해 시각화를 많이 하고 있는데요, 이러한 로그 정보들을 파이썬을 통해 분석하고 싶은 경우가 있습니다. 예를 든다면 dau(daily activate user)의 정보를 수집하고 있는데 몇시에 가장 많이 접속하는지, 이런 정보를 가공해 다른 곳에 사용한다던지 말이죠.\n이러한 정보를 키바나를 통해 확인할 수 있지만, 별도의 레포트를 만들 경우 seaborn이나 matplotlib을 통해 시각화를 진행할때가 많습니다. 그렇다면 어떻게 엘라스틱 서치로 저장된 정보를 파이썬으로 쿼리할 수 있을 지 알아보겠습니다.\n여기서는 scroll 메서드를 사용해 순차적으로 저장된 모든 정보를 불러오고 json 파일로 저장하는 방법을 살펴봅니다.\n예제 아래 예제에서는 dau 라는 이름을 가진 인덱스에서 product_id=2021 인 정보를 모두 쿼리합니다.\n1 2 3 4 5 6 7 import json from datetime import datetime from elasticsearch import Elasticsearch host = \u0026#34;받아올 엘라스틱 서치 URI\u0026#34; es = Elasticsearch(host) indices = [\u0026#39;dau\u0026#39;] # 인덱스 명 size = 10000 # 한 번에 받아올 데이터 갯수 여기서 host 는 본인이 사용하고 있는 엘라스틱 서치 서비스의 URI입니다. size=10000 은 한 번에 1만개씩 받아오겠다는 의미입니다.\n1 2 3 4 5 6 7 8 doc = { \u0026#39;size\u0026#39;: size, \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;product_id\u0026#39;: 2021, } } } doc에 쿼리 정보를 담아줍니다.\n1 2 3 4 # 이전 스크롤 1초 저장 response = es.search(index=indices, body=doc, scroll=\u0026#39;1s\u0026#39;) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] result = [] response 에 엘라스틱 서치 search 메서드를 정의합니다. old_scroll_id 에 초기 스크롤 id를 정의하고 결과를 받을 빈 리스트를 정의합니다.(result)\n1 2 for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) 가장 처음 쿼리를 저장합니다.\n1 2 3 4 5 6 while len(response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]): response = es.scroll(scroll_id=old_scroll_id, scroll=\u0026#39;1s\u0026#39;) for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] # scroll id 초기화 print(f\u0026#39;Result Length: {len(result)}\u0026#39;) 이후 반복문을 통해 더 이상 받아올 정보가 없을 때 까지 탐색하여 정보를 저장합니다.\n1 2 3 # save file with open(\u0026#39;es_dau.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(result, f) 마지막으로 result 를 json 형식의 파일로 저장하면 이후 자유롭게 어디서나 쿼리한 정보를 받아올 수 있습니다.\n전체코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 import json from datetime import datetime from elasticsearch import Elasticsearch host = \u0026#34;사용할 엘라스틱 서치 URL\u0026#34; es = Elasticsearch(host) indices = [\u0026#39;dau\u0026#39;] size = 10000 doc = { \u0026#39;size\u0026#39;: size, \u0026#39;query\u0026#39;: { \u0026#39;match\u0026#39;: { \u0026#39;product_id\u0026#39;: 2021, } } } # 이전 스크롤 1초 저장 response = es.search(index=indices, body=doc, scroll=\u0026#39;1s\u0026#39;) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] result = [] for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) while len(response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]): response = es.scroll(scroll_id=old_scroll_id, scroll=\u0026#39;1s\u0026#39;) for d in response[\u0026#39;hits\u0026#39;][\u0026#39;hits\u0026#39;]: result.append(d[\u0026#39;_source\u0026#39;]) old_scroll_id = response[\u0026#39;_scroll_id\u0026#39;] # scroll id 초기화 print(f\u0026#39;Result Length: {len(result)}\u0026#39;) # save file with open(\u0026#39;es_dau.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(result, f) ","date":"2021-12-19T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%EC%97%98%EB%9D%BC%EC%8A%A4%ED%8B%B1-%EC%84%9C%EC%B9%98-%EC%BF%BC%EB%A6%AC%ED%95%98%EA%B8%B0/","title":"파이썬으로 엘라스틱 서치 쿼리하기"},{"content":"지난번 포스팅에서 data analytics learning plan을 수강한다고 언급했었는데요,(참조: [AWS] Data Analytics Learning Plan을 시작하며) 아무래도 영어로 된 강의다보니 첫 수강에 부담이 있었습니다.\n한국어로 된 유사한 강의가 있나 찾아보던 중 발견하게 되어 먼저 수강하기로 결정했습니다. (링크: Data Analytics Fundamentals (Korean), 수강신청을 하지 않았을 경우 접속이 되지 않을 수 있습니다. 먼저 aws skill builder에서 등록을 진행해주세요.)\n강의 구성 강의는 약 3시간 30분으로 이루어져 있습니다. 강의 구성은 5V에 대해 소개하는데요, 볼륨(Volume), 속도(Velocity), 다양성(Variety), 정확성(Veracity), 가치(Value)의 5V입니다.\n데이터 분석 과정에서 직면한 5V의 문제를 어떤식으로 접근해야하는지, AWS의 어떤 서비스를 이용하면 되는지에 대해 소개합니다.\n후기 솔직히 별로..라고 생각했습니다.\n기초강의다보니 실무적으로 사용하는 방법보다는, 이런 문제는 우리의 어떤 서비스를 통해 해결할 수 있어~ 에서 마치는 느낌입니다.\nAWS에 어느정도 관심있으신 분들은 다 아시는 내용일거라고 생각합니다.\n아무튼 수료 ","date":"2021-01-09T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/aws-data-analysis-fundamentals-%ED%9B%84%EA%B8%B0/","title":"AWS Data Analysis Fundamentals 후기"},{"content":"신년을 맞아 여러가지 공부를 할 것에 대해 찾아보던 도중에, AWS의 자격증 준비를 위해 AWS에서 자체적으로 제공하는 AWS Skill Builder에 대해 알게되었습니다. (aws skill builder 사이트) 요즘 회사에서 ML/DL보다는 거의 AWS 관련 업무만 하고 있기에, 등록하면 업무적으로나 커리어적으로나 매우 도움이 될 것 같았습니다.\nKorean으로 검색하면 한국어로 된 강의도 매우 많음을 알 수 있습니다. 하나의 아이템(강의)별로 올라와있습니다. 길이는 긴 경우에 3시간 짜리도 있습니다.\n이러한 강의들을 묶어서 사용자의 니즈에 맞춘 커리큘럼처럼 제공하는 플랜도 있습니다. 아쉽게도 이러한 플랜의 경우 모두 영어로 되어있습니다. 저는 그중에서도 data analytics learning plan을 선택하였습니다. 해당 강의에서는 kinesis부터 시작하여 EMR, Redshift등 현재 사내에서 사용/사용을 고려한 서비스에 대해 다루고 있었기 때문이었습니다. 총 강의는 17시간입니다.\n먼저 overview강의를 들었는데, 듣고나니 certificate를 발급해주었습니다. aws는 별도의 자격증이 있기 때문에 수강 자격증이 의미가 있을진 모르겠습니다만, 앞으로 수강한 내용을 정리하여 업로드할 예정입니다.\n","date":"2021-01-01T17:12:39+09:00","permalink":"https://Haebuk.github.io/p/aws-data-analysis-learning-plan/","title":"AWS Data Analysis Learning Plan"}]